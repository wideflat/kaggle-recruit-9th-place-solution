{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohei\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime, timedelta, date\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from keras.models import Model\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, Merge, LSTM\n",
    "from keras.layers import Input, BatchNormalization, GlobalMaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "from keras import backend\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/Kohei/Documents/Kaggle/Recruit/02_data/02_19a/'\n",
    "path = 'C:/Users/Kohei/Documents/Kaggle/Recruit/06_keras/06_19ac/'\n",
    "ver = '06_19ac_keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID = ['air_store_id','visit_date','flag']\n",
    "TARGET='visitors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return mean_squared_error(y, pred) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_path, 'train2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable Selection\n",
    "cols2 = ['holiday_flg','holiday_flg+1d','holiday_flg-1d','holiday_flg_rev',\\\n",
    "        'dow',\\\n",
    "#          'month',\\\n",
    "#             'dow_0',\n",
    "        'dow_1','dow_2','dow_3','dow_4','dow_5','dow_6',\\\n",
    "#             'month_1',\n",
    "        'month_2','month_3','month_4','month_5','month_6','month_7','month_8','month_9','month_10','month_11','month_12',\\\n",
    "        'air_genre',\\\n",
    "        'latitude','longitude',\\\n",
    "#             'var_max_lat','var_max_long',\\\n",
    "#             'km_latlong',\\\n",
    "        'air_areaL1_lbl',\\\n",
    "        'air_areaL2_lbl',\\\n",
    "#         'air_areaL3_lbl',\\\n",
    "#             'km_latlong_0',\\\n",
    "        'km_latlong_1','km_latlong_2','km_latlong_3','km_latlong_4','km_latlong_5',\\\n",
    "        'km_latlong_6','km_latlong_7','km_latlong_8','km_latlong_9','km_latlong_10','km_latlong_11',\\\n",
    "#             'air_genre_0','air_genre_1','air_genre_2','air_genre_3','air_genre_4','air_genre_5','air_genre_6','air_genre_7',\\\n",
    "#             'air_genre_8','air_genre_9','air_genre_10','air_genre_11','air_genre_12','air_genre_13',\\\n",
    "#             'km_hpg_latlong',\\\n",
    "        'days_from_first_date',\\\n",
    "        'dow_all_wmean','dow_all_max','dow_all_min','dow_all_med',\\\n",
    "#         'dow_all_cnt',\\\n",
    "        'dowhol_all_wmean','dowhol_all_max','dowhol_all_min','dowhol_all_med',\\\n",
    "        'dowhol+1d_all_wmean','dowhol+1d_all_max','dowhol+1d_all_min','dowhol+1d_all_med',\\\n",
    "        'dowhol-1d_all_wmean','dowhol-1d_all_max','dowhol-1d_all_min','dowhol-1d_all_med',\\\n",
    "#             'dowhol_rev_all_wmean','dowhol_rev_all_max','dowhol_rev_all_min','dowhol_rev_all_med','dowhol_rev_all_cnt',\\\n",
    "#             'dowhol_all_med',\\\n",
    "#             'dowhol+1d_all_med',\\\n",
    "#             'dowhol-1d_all_med',\\\n",
    "#         'dowhol_all_cnt',\\\n",
    "#             'dowhol+1d_all_cnt',\\\n",
    "#             'dowhol-1d_all_cnt',\\\n",
    "#             'km_latlong_mean','km_latlong_dow_mean',\\\n",
    "        'genre_mean',\\\n",
    "#          'genre_dow_mean',\\\n",
    "#         'areaL1_mean',\\\n",
    "#         'areaL1_dow_mean',\\\n",
    "#         'areaL2_mean',\\\n",
    "#         'areaL2_dow_mean',\\\n",
    "        'areaL3_mean',\\\n",
    "#         'areaL3_dow_mean',\\\n",
    "        'res_ttl','res_cnt','res_mean',\\\n",
    "        'res_hr_std',\\\n",
    "#         'res_std',\\\n",
    "#         'res_hr_dif_sum',\\\n",
    "#         'res_hr_dif_mean',\\\n",
    "        'res_ttl_dow_mean','res_cnt_dow_mean',\\\n",
    "#         'lag_1d','lag_2d','lag_3d','lag_4d','lag_5d','lag_6d',\\\n",
    "        'lag_10d','lag_15d','lag_20d','lag_25d','lag_30d','lag_35d','lag_40d','lag_45d','lag_50d',\\\n",
    "         \n",
    "        'lag_res_1d','lag_res_2d','lag_res_3d','lag_res_4d','lag_res_5d','lag_res_6d',\\\n",
    "#         'lag_res_10d','lag_res_15d','lag_res_20d',\\\n",
    "#         'lag_res_25d','lag_res_30d','lag_res_35d','lag_res_40d','lag_res_45d','lag_res_50d',\\\n",
    "        'lag_1w',\\\n",
    "        'lag_2w',\\\n",
    "        'lag_3w',\\\n",
    "        'lag_4w',\\\n",
    "        'lag_5w',\\\n",
    "        'lag_6w',\\\n",
    "        'lag_7w',\\\n",
    "        'lag_8w',\\\n",
    "        'lag_9w',\\\n",
    "        'lag_10w',\\\n",
    "        'lag_11w',\\\n",
    "        'lag_12w',\\\n",
    "        'lag_13w',\\\n",
    "        'lag_14w',\\\n",
    "        'lag_15w',\\\n",
    "        'lag_16w',\\\n",
    "        'lag_17w',\\\n",
    "        'lag_18w',\\\n",
    "        'lag_19w',\\\n",
    "        'lag_20w',\\\n",
    "        'lag_res_1w','lag_res_2w','lag_res_3w','lag_res_4w','lag_res_5w',\\\n",
    "        'lag_res_6w','lag_res_7w','lag_res_8w','lag_res_9w','lag_res_10w',\\\n",
    "#         'lag_res_11w','lag_res_12w','lag_res_13w','lag_res_14w','lag_res_15w',\\\n",
    "#         'lag_res_16w','lag_res_17w','lag_res_18w','lag_res_19w','lag_res_20w',\\\n",
    "        'mean_3d','max_3d','min_3d','std_3d',\\\n",
    "        'mean_7d','max_7d','min_7d','std_7d',\\\n",
    "        'mean_14d','max_14d','min_14d','std_14d',\\\n",
    "#         'mean_21d','max_21d','min_21d','std_21d',\\\n",
    "        'mean_28d','max_28d','min_28d','std_28d',\\\n",
    "#         'mean_35d','max_35d','min_35d','std_35d',\\\n",
    "        'mean_42d','max_42d','min_42d','std_42d',\\\n",
    "#             'mean_49d','max_49d','min_49d','std_49d',\\\n",
    "        'mean_56d','max_56d','min_56d','std_56d',\\\n",
    "#             'mean_63d','max_63d','min_63d','std_63d',\\\n",
    "#         'mean_70d','max_70d','min_70d','std_70d',\\\n",
    "#             'mean_77d','max_77d','min_77d','std_77d',\\\n",
    "        'mean_84d','max_84d','min_84d','std_84d',\\\n",
    "#             'scale_to_maxmin_3d','scale_to_std_3d',\\\n",
    "        'scale_to_maxmin_7d','scale_to_std_7d',\\\n",
    "        'scale_to_maxmin_14d','scale_to_std_14d',\\\n",
    "#             'scale_to_maxmin_21d','scale_to_std_21d',\\\n",
    "        'scale_to_maxmin_28d','scale_to_std_28d',\\\n",
    "#             'scale_to_maxmin_35d','scale_to_std_35d',\\\n",
    "#             'scale_to_maxmin_42d','scale_to_std_42d',\\\n",
    "#             'scale_to_maxmin_49d','scale_to_std_49d',\\\n",
    "        'scale_to_maxmin_56d','scale_to_std_56d',\\\n",
    "#             'scale_to_maxmin_63d','scale_to_std_63d',\\\n",
    "#             'scale_to_maxmin_70d','scale_to_std_70d',\\\n",
    "#             'scale_to_maxmin_77d','scale_to_std_77d',\\\n",
    "        'scale_to_maxmin_84d','scale_to_std_84d',\\\n",
    "#             'mean_7d-14d','mean_7d-28d','mean_7d-56d','mean_7d-84d','mean_14d-28d','mean_14d-56d','mean_14d-84d',\\\n",
    "#             'mean_28d-56d','mean_28d-84d','mean_56d-84d',\\\n",
    "        'mean_dow_2w','max_dow_2w','min_dow_2w','std_dow_2w',\\\n",
    "#         'mean_dow_3w','max_dow_3w','min_dow_3w','std_dow_3w',\\\n",
    "        'mean_dow_4w','max_dow_4w','min_dow_4w','std_dow_4w',\\\n",
    "#             'mean_dow_5w','max_dow_5w','min_dow_5w','std_dow_5w',\\\n",
    "        'mean_dow_6w','max_dow_6w','min_dow_6w','std_dow_6w',\\\n",
    "#             'mean_dow_7w','max_dow_7w','min_dow_7w','std_dow_7w',\\\n",
    "#         'mean_dow_8w','max_dow_8w','min_dow_8w','std_dow_8w',\\\n",
    "#             'mean_dow_9w','max_dow_9w','min_dow_9w','std_dow_9w',\\\n",
    "#             'mean_dow_10w','max_dow_10w','min_dow_10w','std_dow_10w',\\\n",
    "#             'mean_dow_2w-4w','mean_dow_2w-6w','mean_dow_2w-8w','mean_dow_2w-10w',\\\n",
    "#         'mean_dow_4w-6w','mean_dow_4w-8w','mean_dow_4w-10w',\\\n",
    "#         'mean_dow_6w-8w','mean_dow_6w-10w','mean_dow_8w-10w',\\\n",
    "        'rainfall','snowfall','wind_max',\\\n",
    "        'weather_daytime',\\\n",
    "        'weather_nighttime',\\\n",
    "#             'weather_daytime2','weather_nighttime2',\\\n",
    "        'weather_daytime_0',\\\n",
    "        'weather_daytime_1',\\\n",
    "#         'weather_daytime_2',\\\n",
    "        'weather_daytime_3',\\\n",
    "        'weather_nighttime_0',\\\n",
    "        'weather_nighttime_1',\\\n",
    "        'weather_nighttime_2',\\\n",
    "        'weather_nighttime_3',\\\n",
    "#         'wind_max_inst',\\\n",
    "#         'rainfall_max1h',\\\n",
    "#         'wind_avg',\\\n",
    "        'temperature_high','temperature_low','temperature_avg',\\\n",
    "#         'snowfall_max',\\\n",
    "#         'humidity_avg',\\\n",
    "#         'daylight_hr',\\\n",
    "#         'na_cnt'\n",
    "        ]\n",
    "# add = [c for c in train.columns if \"air_areaL1_lbl_\" in c]\n",
    "# cols.extend(add)\n",
    "# add = [c for c in train.columns if \"air_areaL2_lbl_\" in c]\n",
    "# cols.extend(add)\n",
    "add = [c for c in train.columns if \"air_areaL3_lbl_\" in c]\n",
    "cols2.extend(add)\n",
    "\n",
    "cols1 = cols2.copy()\n",
    "lag = ['lag_1d','lag_2d','lag_3d','lag_4d','lag_5d','lag_6d']\n",
    "cols1.extend(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_adj = ['dow_all_wmean','dow_all_max','dow_all_min','dow_all_med',\\\n",
    "            'dowhol_all_wmean','dowhol_all_max','dowhol_all_min','dowhol_all_med',\\\n",
    "            'dowhol+1d_all_wmean','dowhol+1d_all_max','dowhol+1d_all_min','dowhol+1d_all_med',\\\n",
    "            'dowhol-1d_all_wmean','dowhol-1d_all_max','dowhol-1d_all_min','dowhol-1d_all_med',\\\n",
    "            'dowhol_rev_all_wmean','dowhol_rev_all_max','dowhol_rev_all_min','dowhol_rev_all_med',\\\n",
    "            'km_latlong_mean','km_latlong_dow_mean',\\\n",
    "            'genre_mean','genre_dow_mean',\\\n",
    "            'areaL1_mean','areaL1_dow_mean',\\\n",
    "            'areaL2_mean','areaL2_dow_mean',\\\n",
    "            'areaL3_mean','areaL3_dow_mean',\\\n",
    "            'mean_3d','max_3d','min_3d','std_3d','scale_to_maxmin_3d','scale_to_std_3d',\\\n",
    "            'mean_7d','max_7d','min_7d','std_7d','scale_to_maxmin_7d','scale_to_std_7d',\\\n",
    "            'mean_14d','max_14d','min_14d','std_14d','scale_to_maxmin_14d','scale_to_std_14d',\\\n",
    "            'mean_21d','max_21d','min_21d','std_21d','scale_to_maxmin_21d','scale_to_std_21d',\\\n",
    "            'mean_28d','max_28d','min_28d','std_28d','scale_to_maxmin_28d','scale_to_std_28d',\\\n",
    "            'mean_35d','max_35d','min_35d','std_35d','scale_to_maxmin_35d','scale_to_std_35d',\\\n",
    "            'mean_42d','max_42d','min_42d','std_42d','scale_to_maxmin_42d','scale_to_std_42d',\\\n",
    "            'mean_49d','max_49d','min_49d','std_49d','scale_to_maxmin_49d','scale_to_std_49d',\\\n",
    "            'mean_56d','max_56d','min_56d','std_56d','scale_to_maxmin_56d','scale_to_std_56d',\\\n",
    "            'mean_63d','max_63d','min_63d','std_63d','scale_to_maxmin_63d','scale_to_std_63d',\\\n",
    "            'mean_70d','max_70d','min_70d','std_70d','scale_to_maxmin_70d','scale_to_std_70d',\\\n",
    "            'mean_77d','max_77d','min_77d','std_77d','scale_to_maxmin_77d','scale_to_std_77d',\\\n",
    "            'mean_84d','max_84d','min_84d','std_84d','scale_to_maxmin_84d','scale_to_std_84d',\\\n",
    "            'mean_7d-14d','mean_7d-28d','mean_7d-56d','mean_7d-84d','mean_14d-28d','mean_14d-56d','mean_14d-84d',\\\n",
    "            'mean_28d-56d','mean_28d-84d','mean_56d-84d']\n",
    "cols_remove = ['dow_all_cnt','dowhol_all_cnt','dowhol+1d_all_cnt','dowhol-1d_all_cnt','dowhol_rev_all_cnt',\n",
    "               'lag_1d','lag_2d','lag_3d','lag_4d','lag_5d','lag_6d']\n",
    "cols_inf = ['scale_to_maxmin_3d','scale_to_std_3d',\\\n",
    "            'scale_to_maxmin_7d','scale_to_std_7d',\\\n",
    "            'scale_to_maxmin_14d','scale_to_std_14d',\\\n",
    "            'scale_to_maxmin_21d','scale_to_std_21d',\\\n",
    "            'scale_to_maxmin_28d','scale_to_std_28d',\\\n",
    "            'scale_to_maxmin_35d','scale_to_std_35d',\\\n",
    "            'scale_to_maxmin_42d','scale_to_std_42d',\\\n",
    "            'scale_to_maxmin_49d','scale_to_std_49d',\\\n",
    "            'scale_to_maxmin_56d','scale_to_std_56d',\\\n",
    "            'scale_to_maxmin_63d','scale_to_std_63d',\\\n",
    "            'scale_to_maxmin_70d','scale_to_std_70d',\\\n",
    "            'scale_to_maxmin_77d','scale_to_std_77d',\\\n",
    "            'scale_to_maxmin_84d','scale_to_std_84d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_run(df):\n",
    "    train = pd.read_csv(os.path.join(data_path, df+'.csv'))\n",
    "    test = pd.read_csv(os.path.join(data_path, 'test_all.csv'))\n",
    "    if df=='train1':\n",
    "        test = test.loc[test.flag==1,:].reset_index(drop=True)\n",
    "    elif df=='train2':\n",
    "        test = test.loc[test.flag==2,:].reset_index(drop=True)\n",
    "        \n",
    "    y_train = np.log1p(train['visitors'])\n",
    "    id_train = train[ID]\n",
    "    id_test  = test[ID]\n",
    "    x_train = train\n",
    "    x_test = test\n",
    "    x_train.drop(ID, axis=1, inplace=True)\n",
    "    x_train.drop(TARGET, axis=1, inplace=True)\n",
    "    x_test.drop(ID, axis=1, inplace=True)\n",
    "    del train, test\n",
    "    \n",
    "    tr_te = pd.concat([x_train,x_test])\n",
    "    for x in cols_inf:\n",
    "        tr_te[x] = tr_te[x].replace(-np.inf,tr_te[x].median())\n",
    "        tr_te[x] = tr_te[x].replace(np.inf,tr_te[x].median())\n",
    "    for x in cols_adj:\n",
    "        tr_te[x] = tr_te[x].replace(-1,tr_te[x].median())\n",
    "        \n",
    "    ntrain = x_train.shape[0]\n",
    "    x_train = tr_te[:ntrain]\n",
    "    x_test  = tr_te[ntrain:]\n",
    "\n",
    "    if df=='train1':\n",
    "        x_train = x_train[cols1]\n",
    "        x_test = x_test[cols1]\n",
    "    elif df=='train2':\n",
    "        x_train = x_train[cols2]\n",
    "        x_test = x_test[cols2]\n",
    "    \n",
    "    x_train['wind_max_x_rainfall'] = x_train['wind_max']*x_train['rainfall'] \n",
    "    x_test['wind_max_x_rainfall']  = x_test['wind_max']*x_test['rainfall']\n",
    "    \n",
    "    x_train = x_train.replace(-1,0) \n",
    "    x_test  = x_test.replace(-1,0)\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_test  = np.array(x_test)\n",
    "    \n",
    "    x_test  = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "    n_folds = 5\n",
    "    bagging = 1\n",
    "    divisor = 1\n",
    "    cv_sum = 0\n",
    "    pred_tr_all = []\n",
    "    pred_te_all = []\n",
    "\n",
    "    kf = KFold(x_train.shape[0], n_folds=n_folds, random_state=1234, shuffle=True)\n",
    "\n",
    "    # parameters\n",
    "    batch_size = 20000\n",
    "    epochs = 500\n",
    "\n",
    "    for i, (tr_index,vl_index) in enumerate(kf):\n",
    "        print('\\nFold %d / %d' % (i+1, n_folds))\n",
    "        pred = []\n",
    "        pred_vl = []\n",
    "\n",
    "        x_tr, x_vl = x_train[tr_index], x_train[vl_index]\n",
    "        y_tr, y_vl = y_train.iloc[tr_index], y_train.iloc[vl_index]\n",
    "        id_vl      = id_train.iloc[vl_index,:]\n",
    "        \n",
    "        x_tr = x_tr.reshape((x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "        x_vl = x_vl.reshape((x_vl.shape[0], 1, x_vl.shape[1]))\n",
    "        \n",
    "\n",
    "        for j in range(1,bagging+1):\n",
    "            print('\\nBagging %d / %d' % (j, bagging))\n",
    "\n",
    "            # sampling\n",
    "#             np.random.seed(j)\n",
    "#             sample = np.random.choice(len(x_tr), size=int(len(x_tr)/divisor))\n",
    "#             x_tr_bag = x_tr.iloc[sample]\n",
    "#             y_tr_bag = y_tr.iloc[sample]\n",
    "            x_tr_bag = x_tr\n",
    "            y_tr_bag = y_tr\n",
    "            y_mean   = y_tr_bag.mean() \n",
    "\n",
    "            # model architect\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(128, input_shape=(x_tr.shape[1],x_tr.shape[2])))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(.2))\n",
    "\n",
    "            model.add(Dense(64))\n",
    "            model.add(PReLU())\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(.05))\n",
    "\n",
    "            model.add(Dense(32))\n",
    "            model.add(PReLU())\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(.05))\n",
    "\n",
    "            model.add(Dense(16))\n",
    "            model.add(PReLU())\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(.05))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            opt = optimizers.Adam(lr=0.001)\n",
    "            model.compile(loss='mse', optimizer=opt)\n",
    "            \n",
    "            \n",
    "            MODEL_FILE = 'keras_k{}_wght.hdf5'.format(i+1)\n",
    "            callbacks = [\n",
    "                        ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_loss', mode='min'),\n",
    "                        EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "                        ]\n",
    "            \n",
    "            model.fit(x_tr, y_tr-y_mean,\n",
    "#             model.fit(x_tr, y_tr,\n",
    "                      batch_size = batch_size,\n",
    "                      epochs = epochs,\n",
    "                      verbose = 1,\n",
    "                      validation_data=(x_vl, y_vl-y_mean),callbacks = callbacks)\n",
    "#                       validation_data=(x_vl, y_vl),callbacks = callbacks)\n",
    "\n",
    "            if j==1:\n",
    "                pred_v = model.predict(x_vl, verbose=0)[:,0]+y_mean\n",
    "                pred_t = model.predict(x_test, verbose=0)[:,0]+y_mean\n",
    "#                 pred_v = model.predict(x_vl, verbose=0)[:,0]\n",
    "#                 pred_t = model.predict(x_test, verbose=0)[:,0]\n",
    "            else:\n",
    "                pred_v += model.predict(x_vl, verbose=0)[:,0]+y_mean\n",
    "                pred_t += model.predict(x_test, verbose=0)[:,0]+y_mean \n",
    "#                 pred_v += model.predict(x_vl, verbose=0)[:,0]\n",
    "#                 pred_t += model.predict(x_test, verbose=0)[:,0] \n",
    "            pred_vl = (pred_v/j)\n",
    "            pred_te = (pred_t/j)\n",
    "\n",
    "            cv_score = RMSLE(y_vl, pred_vl)\n",
    "            print('Fold RMSLE : %.6f' % cv_score)\n",
    "\n",
    "        # end of bagging\n",
    "        pred = id_vl\n",
    "        pred['pred'] = pred_vl\n",
    "\n",
    "        if i==0:\n",
    "            pred_tr_all = pred\n",
    "            pred_te_all = pred_te\n",
    "        else:\n",
    "            pred_tr_all = pd.concat([pred_tr_all,pred])\n",
    "            pred_te_all += pred_te\n",
    "\n",
    "        cv_sum = cv_sum + cv_score\n",
    "\n",
    "    pred_te_all /= n_folds\n",
    "    pred_te_all = pd.DataFrame({'pred':pred_te_all})\n",
    "    pred_te_all = pd.concat([id_test,pred_te_all],axis=1)\n",
    "\n",
    "    print('CV RMSLE : %.6f' % (cv_sum / n_folds))\n",
    "\n",
    "    return pred_tr_all, pred_te_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 6s - loss: 1.6251 - val_loss: 0.6566\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.7876 - val_loss: 0.5949\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.6119 - val_loss: 0.5647\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.5403 - val_loss: 0.5426\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5058 - val_loss: 0.5177\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4810 - val_loss: 0.5039\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4570 - val_loss: 0.4884\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4454 - val_loss: 0.4801\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4311 - val_loss: 0.4618\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4224 - val_loss: 0.4472\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4106 - val_loss: 0.4402\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4086 - val_loss: 0.4313\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3995 - val_loss: 0.4148\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3935 - val_loss: 0.4035\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3888 - val_loss: 0.4034\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3865 - val_loss: 0.3853\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3808 - val_loss: 0.3840\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3775 - val_loss: 0.3835\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3718 - val_loss: 0.3692\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3674 - val_loss: 0.3680\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3620 - val_loss: 0.3619\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3593 - val_loss: 0.3468\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3580 - val_loss: 0.3470\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3512 - val_loss: 0.3407\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3510 - val_loss: 0.3409\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3482 - val_loss: 0.3284\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3456 - val_loss: 0.3238\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3426 - val_loss: 0.3145\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3401 - val_loss: 0.3147\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3372 - val_loss: 0.3119\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3347 - val_loss: 0.3070\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3313 - val_loss: 0.3031\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3322 - val_loss: 0.2982\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3283 - val_loss: 0.2937\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3306 - val_loss: 0.2987\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3280 - val_loss: 0.2909\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3245 - val_loss: 0.2894\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3236 - val_loss: 0.2875\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3220 - val_loss: 0.2826\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3207 - val_loss: 0.2854\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3180 - val_loss: 0.2871\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3180 - val_loss: 0.2833\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3144 - val_loss: 0.2783\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3124 - val_loss: 0.2789\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3136 - val_loss: 0.2780\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3123 - val_loss: 0.2751\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3104 - val_loss: 0.2719\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3099 - val_loss: 0.2726\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3078 - val_loss: 0.2713\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3067 - val_loss: 0.2724\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3075 - val_loss: 0.2717\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3044 - val_loss: 0.2686\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3038 - val_loss: 0.2686\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3015 - val_loss: 0.2701\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3016 - val_loss: 0.2687\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3019 - val_loss: 0.2713\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2985 - val_loss: 0.2680\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2692\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2991 - val_loss: 0.2670\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2954 - val_loss: 0.2674\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2964 - val_loss: 0.2676\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2948 - val_loss: 0.2650\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2950 - val_loss: 0.2665\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2929 - val_loss: 0.2638\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2922 - val_loss: 0.2651\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2906 - val_loss: 0.2655\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2919 - val_loss: 0.2635\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2911 - val_loss: 0.2668\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2900 - val_loss: 0.2620\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2892 - val_loss: 0.2624\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2874 - val_loss: 0.2625\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2871 - val_loss: 0.2622\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2872 - val_loss: 0.2633\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2619\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2856 - val_loss: 0.2612\n",
      "Epoch 76/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2848 - val_loss: 0.2625\n",
      "Epoch 77/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2836 - val_loss: 0.2612\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2830 - val_loss: 0.2611\n",
      "Epoch 79/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2829 - val_loss: 0.2617\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2825 - val_loss: 0.2631\n",
      "Epoch 81/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2818\n",
      "Epoch 00080: reducing learning rate to 0.00010000000474974513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.2816 - val_loss: 0.2621\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2807 - val_loss: 0.2613\n",
      "Epoch 83/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2803 - val_loss: 0.2611\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2806 - val_loss: 0.2613\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2819 - val_loss: 0.2613\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2798 - val_loss: 0.2609\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2611\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2806 - val_loss: 0.2607\n",
      "Epoch 89/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2803 - val_loss: 0.2603\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2793 - val_loss: 0.2606\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2802 - val_loss: 0.2611\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2788 - val_loss: 0.2607\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2805 - val_loss: 0.2603\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2804 - val_loss: 0.2603\n",
      "Epoch 95/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2800\n",
      "Epoch 00094: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2799 - val_loss: 0.2604\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2792 - val_loss: 0.2603\n",
      "Epoch 97/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2803 - val_loss: 0.2603\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2792 - val_loss: 0.2602\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2602\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2788 - val_loss: 0.2601\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2601\n",
      "Epoch 102/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2802 - val_loss: 0.2601\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2808 - val_loss: 0.2601\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2601\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2788 - val_loss: 0.2601\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2600\n",
      "Epoch 107/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2808 - val_loss: 0.2600\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2804 - val_loss: 0.2600\n",
      "Epoch 109/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2795 - val_loss: 0.2600\n",
      "Epoch 110/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2788 - val_loss: 0.2600\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2600\n",
      "Epoch 112/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2783 - val_loss: 0.2600\n",
      "Epoch 113/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2797\n",
      "Epoch 00112: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2797 - val_loss: 0.2600\n",
      "Epoch 114/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2600\n",
      "Epoch 115/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2803 - val_loss: 0.2600\n",
      "Epoch 116/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2797 - val_loss: 0.2600\n",
      "Epoch 117/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2600\n",
      "Epoch 118/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2806\n",
      "Epoch 00117: reducing learning rate to 1.0000001111620805e-07.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2807 - val_loss: 0.2600\n",
      "Epoch 119/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2802 - val_loss: 0.2600\n",
      "Epoch 120/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2789 - val_loss: 0.2600\n",
      "Epoch 121/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2792 - val_loss: 0.2600\n",
      "Epoch 122/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2790 - val_loss: 0.2599\n",
      "Epoch 123/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2795\n",
      "Epoch 00122: reducing learning rate to 1.000000082740371e-08.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2795 - val_loss: 0.2599\n",
      "Epoch 124/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2795 - val_loss: 0.2599\n",
      "Epoch 125/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2799 - val_loss: 0.2599\n",
      "Epoch 126/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2801 - val_loss: 0.2599\n",
      "Epoch 127/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2793 - val_loss: 0.2599\n",
      "Epoch 128/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2786\n",
      "Epoch 00127: reducing learning rate to 1.000000082740371e-09.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2790 - val_loss: 0.2599\n",
      "Epoch 129/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2795 - val_loss: 0.2599\n",
      "Epoch 130/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2802 - val_loss: 0.2599\n",
      "Epoch 131/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2786 - val_loss: 0.2599\n",
      "Epoch 132/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2790 - val_loss: 0.2599\n",
      "Epoch 133/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2790\n",
      "Epoch 00132: reducing learning rate to 1.000000082740371e-10.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2790 - val_loss: 0.2599\n",
      "Epoch 134/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2797 - val_loss: 0.2599\n",
      "Epoch 135/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2799 - val_loss: 0.2599\n",
      "Epoch 136/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2791 - val_loss: 0.2599\n",
      "Epoch 137/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2599\n",
      "Epoch 138/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2793\n",
      "Epoch 00137: reducing learning rate to 1.000000082740371e-11.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2795 - val_loss: 0.2599\n",
      "Epoch 139/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2599\n",
      "Epoch 140/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2794 - val_loss: 0.2599\n",
      "Epoch 141/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2799 - val_loss: 0.2599\n",
      "Epoch 142/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2797 - val_loss: 0.2599\n",
      "Epoch 143/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2792\n",
      "Epoch 00142: reducing learning rate to 1.000000082740371e-12.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2789 - val_loss: 0.2599\n",
      "Epoch 144/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2791 - val_loss: 0.2599\n",
      "Fold RMSLE : 0.509821\n",
      "\n",
      "Fold 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 6s - loss: 1.4302 - val_loss: 0.6186\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.7501 - val_loss: 0.6151\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.5964 - val_loss: 0.5878\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5268 - val_loss: 0.5626\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4907 - val_loss: 0.5420\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4661 - val_loss: 0.5278\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4468 - val_loss: 0.5101\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4376 - val_loss: 0.4919\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4203 - val_loss: 0.4727\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4106 - val_loss: 0.4544\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4042 - val_loss: 0.4428\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3959 - val_loss: 0.4348\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3891 - val_loss: 0.4259\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3811 - val_loss: 0.4108\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3761 - val_loss: 0.4024\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3761 - val_loss: 0.3894\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3725 - val_loss: 0.3842\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3644 - val_loss: 0.3729\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3598 - val_loss: 0.3654\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3587 - val_loss: 0.3585\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3521 - val_loss: 0.3483\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3513 - val_loss: 0.3462\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3484 - val_loss: 0.3425\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3456 - val_loss: 0.3374\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3417 - val_loss: 0.3291\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3386 - val_loss: 0.3250\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3346 - val_loss: 0.3237\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3325 - val_loss: 0.3178\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3320 - val_loss: 0.3161\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3303 - val_loss: 0.3096\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3284 - val_loss: 0.3087\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3256 - val_loss: 0.3044\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3224 - val_loss: 0.3046\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3223 - val_loss: 0.2987\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3216 - val_loss: 0.2943\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3176 - val_loss: 0.2944\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3163 - val_loss: 0.2925\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3137 - val_loss: 0.2893\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3143 - val_loss: 0.2885\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3114 - val_loss: 0.2877\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3122 - val_loss: 0.2849\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3081 - val_loss: 0.2850\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3097 - val_loss: 0.2854\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3051 - val_loss: 0.2821\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3067 - val_loss: 0.2802\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3042 - val_loss: 0.2809\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3034 - val_loss: 0.2779\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3012 - val_loss: 0.2783\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2993 - val_loss: 0.2751\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2783\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2975 - val_loss: 0.2759\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2763\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2952 - val_loss: 0.2752\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2956 - val_loss: 0.2734\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2938 - val_loss: 0.2765\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2941 - val_loss: 0.2742\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2928 - val_loss: 0.2758\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2920 - val_loss: 0.2725\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2896 - val_loss: 0.2725\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2900 - val_loss: 0.2717\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2886 - val_loss: 0.2733\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2876 - val_loss: 0.2707\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2890 - val_loss: 0.2715\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2691\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2855 - val_loss: 0.2698\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2693\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2849 - val_loss: 0.2690\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2831 - val_loss: 0.2694\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2838 - val_loss: 0.2689\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2823 - val_loss: 0.2683\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2820 - val_loss: 0.2687\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2808 - val_loss: 0.2683\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2807 - val_loss: 0.2694\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2805 - val_loss: 0.2698\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2801 - val_loss: 0.2690\n",
      "Epoch 76/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2793\n",
      "Epoch 00075: reducing learning rate to 0.00010000000474974513.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2794 - val_loss: 0.2689\n",
      "Epoch 77/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2779 - val_loss: 0.2678\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2674\n",
      "Epoch 79/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2783 - val_loss: 0.2672\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2672\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.2777 - val_loss: 0.2674\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2782 - val_loss: 0.2672\n",
      "Epoch 83/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2769 - val_loss: 0.2670\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2673\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2766 - val_loss: 0.2672\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2778 - val_loss: 0.2670\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2775 - val_loss: 0.2671\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2782 - val_loss: 0.2671\n",
      "Epoch 89/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2773\n",
      "Epoch 00088: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2769 - val_loss: 0.2672\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2778 - val_loss: 0.2672\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2769 - val_loss: 0.2671\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2670\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2766 - val_loss: 0.2669\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2668\n",
      "Epoch 95/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2668\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2667\n",
      "Epoch 97/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2667\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2772 - val_loss: 0.2667\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2667\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2667\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2778 - val_loss: 0.2667\n",
      "Epoch 102/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2780\n",
      "Epoch 00101: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2777 - val_loss: 0.2667\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2666\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2770 - val_loss: 0.2666\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2666\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2774 - val_loss: 0.2666\n",
      "Epoch 107/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2769 - val_loss: 0.2666\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2775 - val_loss: 0.2666\n",
      "Epoch 109/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2772 - val_loss: 0.2666\n",
      "Epoch 110/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2760\n",
      "Epoch 00109: reducing learning rate to 1.0000001111620805e-07.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2760 - val_loss: 0.2666\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2764 - val_loss: 0.2666\n",
      "Epoch 112/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2766 - val_loss: 0.2666\n",
      "Epoch 113/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2779 - val_loss: 0.2666\n",
      "Epoch 114/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2770 - val_loss: 0.2666\n",
      "Epoch 115/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2781\n",
      "Epoch 00114: reducing learning rate to 1.000000082740371e-08.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2779 - val_loss: 0.2666\n",
      "Epoch 116/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2770 - val_loss: 0.2666\n",
      "Epoch 117/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2768 - val_loss: 0.2666\n",
      "Epoch 118/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2771 - val_loss: 0.2666\n",
      "Epoch 119/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2772 - val_loss: 0.2666\n",
      "Epoch 120/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2768\n",
      "Epoch 00119: reducing learning rate to 1.000000082740371e-09.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2766 - val_loss: 0.2666\n",
      "Epoch 121/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2666\n",
      "Epoch 122/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2666\n",
      "Epoch 123/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2666\n",
      "Epoch 124/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2772 - val_loss: 0.2666\n",
      "Epoch 125/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2774\n",
      "Epoch 00124: reducing learning rate to 1.000000082740371e-10.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2777 - val_loss: 0.2666\n",
      "Epoch 126/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2666\n",
      "Epoch 127/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2666\n",
      "Epoch 128/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2777 - val_loss: 0.2666\n",
      "Epoch 129/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2666\n",
      "Epoch 130/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2761\n",
      "Epoch 00129: reducing learning rate to 1.000000082740371e-11.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2763 - val_loss: 0.2666\n",
      "Epoch 131/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2775 - val_loss: 0.2666\n",
      "Epoch 132/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2761 - val_loss: 0.2666\n",
      "Epoch 133/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2770 - val_loss: 0.2666\n",
      "Epoch 134/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2766 - val_loss: 0.2666\n",
      "Epoch 135/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2777\n",
      "Epoch 00134: reducing learning rate to 1.000000082740371e-12.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2776 - val_loss: 0.2666\n",
      "Epoch 136/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2767 - val_loss: 0.2666\n",
      "Epoch 137/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2774 - val_loss: 0.2666\n",
      "Epoch 138/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2778 - val_loss: 0.2666\n",
      "Epoch 139/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2781 - val_loss: 0.2666\n",
      "Epoch 140/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2772\n",
      "Epoch 00139: reducing learning rate to 1.0000001044244145e-13.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2772 - val_loss: 0.2666\n",
      "Epoch 141/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2779 - val_loss: 0.2666\n",
      "Epoch 142/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2771 - val_loss: 0.2666\n",
      "Epoch 143/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2666\n",
      "Epoch 144/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2666\n",
      "Epoch 145/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2771\n",
      "Epoch 00144: reducing learning rate to 1.0000001179769417e-14.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2771 - val_loss: 0.2666\n",
      "Epoch 146/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2762 - val_loss: 0.2666\n",
      "Epoch 147/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2666\n",
      "Epoch 148/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2763 - val_loss: 0.2666\n",
      "Epoch 149/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.2777 - val_loss: 0.2666\n",
      "Epoch 150/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2778\n",
      "Epoch 00149: reducing learning rate to 1.0000001518582595e-15.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2777 - val_loss: 0.2666\n",
      "Epoch 151/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2769 - val_loss: 0.2666\n",
      "Epoch 152/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2762 - val_loss: 0.2666\n",
      "Epoch 153/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2774 - val_loss: 0.2666\n",
      "Fold RMSLE : 0.516327\n",
      "\n",
      "Fold 3 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 6s - loss: 1.6214 - val_loss: 0.6188\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.8231 - val_loss: 0.6421\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.6424 - val_loss: 0.5936\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5582 - val_loss: 0.5437\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5111 - val_loss: 0.5165\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4961 - val_loss: 0.4931\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4700 - val_loss: 0.4727\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4464 - val_loss: 0.4556\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4338 - val_loss: 0.4385\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4278 - val_loss: 0.4252\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4195 - val_loss: 0.4150\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4089 - val_loss: 0.4047\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4042 - val_loss: 0.3956\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3973 - val_loss: 0.3883\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3925 - val_loss: 0.3813\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3845 - val_loss: 0.3760\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3824 - val_loss: 0.3702\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3774 - val_loss: 0.3642\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3790 - val_loss: 0.3578\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3675 - val_loss: 0.3516\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3624 - val_loss: 0.3445\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3620 - val_loss: 0.3424\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3550 - val_loss: 0.3412\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3549 - val_loss: 0.3306\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3507 - val_loss: 0.3278\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3484 - val_loss: 0.3286\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3472 - val_loss: 0.3236\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3425 - val_loss: 0.3191\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3448 - val_loss: 0.3148\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3388 - val_loss: 0.3120\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3358 - val_loss: 0.3085\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3362 - val_loss: 0.3079\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3332 - val_loss: 0.3008\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3304 - val_loss: 0.2954\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3287 - val_loss: 0.2960\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3291 - val_loss: 0.2909\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3264 - val_loss: 0.2907\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3217 - val_loss: 0.2905\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3228 - val_loss: 0.2880\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3194 - val_loss: 0.2858\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3181 - val_loss: 0.2830\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3159 - val_loss: 0.2842\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3145 - val_loss: 0.2815\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3136 - val_loss: 0.2772\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3115 - val_loss: 0.2752\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3133 - val_loss: 0.2760\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3098 - val_loss: 0.2723\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3092 - val_loss: 0.2737\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3073 - val_loss: 0.2725\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3087 - val_loss: 0.2747\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3051 - val_loss: 0.2750\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3069 - val_loss: 0.2709\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3040 - val_loss: 0.2708\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3032 - val_loss: 0.2673\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2674\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2997 - val_loss: 0.2674\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2663\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2660\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2975 - val_loss: 0.2689\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2955 - val_loss: 0.2646\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2944 - val_loss: 0.2662\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2949 - val_loss: 0.2680\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2929 - val_loss: 0.2652\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2922 - val_loss: 0.2639\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2920 - val_loss: 0.2651\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2902 - val_loss: 0.2665\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2910 - val_loss: 0.2648\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2895 - val_loss: 0.2640\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2887 - val_loss: 0.2635\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2881 - val_loss: 0.2637\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2879 - val_loss: 0.2654\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2873 - val_loss: 0.2635\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2852 - val_loss: 0.2624\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2654\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2847 - val_loss: 0.2622\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.2843 - val_loss: 0.2625\n",
      "Epoch 77/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2828 - val_loss: 0.2628\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2847 - val_loss: 0.2620\n",
      "Epoch 79/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2822 - val_loss: 0.2629\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2814 - val_loss: 0.2618\n",
      "Epoch 81/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2811 - val_loss: 0.2616\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2808 - val_loss: 0.2616\n",
      "Epoch 83/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2802 - val_loss: 0.2609\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2791 - val_loss: 0.2635\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2796 - val_loss: 0.2609\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2789 - val_loss: 0.2615\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2781 - val_loss: 0.2611\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2785 - val_loss: 0.2608\n",
      "Epoch 89/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2777 - val_loss: 0.2607\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2766 - val_loss: 0.2607\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2763 - val_loss: 0.2629\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2756 - val_loss: 0.2610\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2760 - val_loss: 0.2610\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2751 - val_loss: 0.2629\n",
      "Epoch 95/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2757\n",
      "Epoch 00094: reducing learning rate to 0.00010000000474974513.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2755 - val_loss: 0.2614\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2740 - val_loss: 0.2622\n",
      "Epoch 97/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2744 - val_loss: 0.2622\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2745 - val_loss: 0.2612\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2741 - val_loss: 0.2603\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2736 - val_loss: 0.2602\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2600\n",
      "Epoch 102/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2743 - val_loss: 0.2600\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2602\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2727 - val_loss: 0.2601\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2741 - val_loss: 0.2604\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2602\n",
      "Epoch 107/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2736\n",
      "Epoch 00106: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2604\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2603\n",
      "Epoch 109/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2602\n",
      "Epoch 110/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2740 - val_loss: 0.2601\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2730 - val_loss: 0.2601\n",
      "Epoch 112/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2731\n",
      "Epoch 00111: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2731 - val_loss: 0.2600\n",
      "Epoch 113/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2600\n",
      "Fold RMSLE : 0.509900\n",
      "\n",
      "Fold 4 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185083 samples, validate on 46270 samples\n",
      "Epoch 1/500\n",
      "185083/185083 [==============================] - 7s - loss: 1.1619 - val_loss: 0.6350\n",
      "Epoch 2/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.6264 - val_loss: 0.5443\n",
      "Epoch 3/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5213 - val_loss: 0.5019\n",
      "Epoch 4/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4725 - val_loss: 0.4615\n",
      "Epoch 5/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4464 - val_loss: 0.4293\n",
      "Epoch 6/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4285 - val_loss: 0.4107\n",
      "Epoch 7/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4167 - val_loss: 0.3986\n",
      "Epoch 8/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4071 - val_loss: 0.3866\n",
      "Epoch 9/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3972 - val_loss: 0.3788\n",
      "Epoch 10/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3895 - val_loss: 0.3723\n",
      "Epoch 11/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3811 - val_loss: 0.3606\n",
      "Epoch 12/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3755 - val_loss: 0.3616\n",
      "Epoch 13/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3733 - val_loss: 0.3541\n",
      "Epoch 14/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3670 - val_loss: 0.3497\n",
      "Epoch 15/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3609 - val_loss: 0.3447\n",
      "Epoch 16/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3562 - val_loss: 0.3430\n",
      "Epoch 17/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3534 - val_loss: 0.3350\n",
      "Epoch 18/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3510 - val_loss: 0.3293\n",
      "Epoch 19/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3479 - val_loss: 0.3253\n",
      "Epoch 20/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3433 - val_loss: 0.3200\n",
      "Epoch 21/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3420 - val_loss: 0.3172\n",
      "Epoch 22/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3395 - val_loss: 0.3114\n",
      "Epoch 23/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3372 - val_loss: 0.3133\n",
      "Epoch 24/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3324 - val_loss: 0.3101\n",
      "Epoch 25/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3304 - val_loss: 0.3073\n",
      "Epoch 26/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3291 - val_loss: 0.3024\n",
      "Epoch 27/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3280 - val_loss: 0.2972\n",
      "Epoch 28/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3237 - val_loss: 0.2949\n",
      "Epoch 29/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3211 - val_loss: 0.2923\n",
      "Epoch 30/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3220 - val_loss: 0.2898\n",
      "Epoch 31/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3197 - val_loss: 0.2880\n",
      "Epoch 32/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3173 - val_loss: 0.2866\n",
      "Epoch 33/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3135 - val_loss: 0.2810\n",
      "Epoch 34/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3133 - val_loss: 0.2791\n",
      "Epoch 35/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3114 - val_loss: 0.2796\n",
      "Epoch 36/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3100 - val_loss: 0.2785\n",
      "Epoch 37/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3093 - val_loss: 0.2780\n",
      "Epoch 38/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3067 - val_loss: 0.2749\n",
      "Epoch 39/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3056 - val_loss: 0.2743\n",
      "Epoch 40/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 5s - loss: 0.3057 - val_loss: 0.2737\n",
      "Epoch 41/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3026 - val_loss: 0.2741\n",
      "Epoch 42/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3023 - val_loss: 0.2712\n",
      "Epoch 43/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3018 - val_loss: 0.2714\n",
      "Epoch 44/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3004 - val_loss: 0.2697\n",
      "Epoch 45/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2995 - val_loss: 0.2678\n",
      "Epoch 46/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2983 - val_loss: 0.2665\n",
      "Epoch 47/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2668\n",
      "Epoch 48/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2946 - val_loss: 0.2642\n",
      "Epoch 49/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2947 - val_loss: 0.2648\n",
      "Epoch 50/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2935 - val_loss: 0.2642\n",
      "Epoch 51/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2935 - val_loss: 0.2648\n",
      "Epoch 52/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2924 - val_loss: 0.2643\n",
      "Epoch 53/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2911 - val_loss: 0.2638\n",
      "Epoch 54/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2901 - val_loss: 0.2616\n",
      "Epoch 55/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2892 - val_loss: 0.2618\n",
      "Epoch 56/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2890 - val_loss: 0.2631\n",
      "Epoch 57/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2883 - val_loss: 0.2606\n",
      "Epoch 58/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2872 - val_loss: 0.2614\n",
      "Epoch 59/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2873 - val_loss: 0.2619\n",
      "Epoch 60/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2599\n",
      "Epoch 61/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2854 - val_loss: 0.2634\n",
      "Epoch 62/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2842 - val_loss: 0.2614\n",
      "Epoch 63/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2830 - val_loss: 0.2605\n",
      "Epoch 64/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2830 - val_loss: 0.2595\n",
      "Epoch 65/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2818 - val_loss: 0.2609\n",
      "Epoch 66/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2814 - val_loss: 0.2655\n",
      "Epoch 67/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2806 - val_loss: 0.2586\n",
      "Epoch 68/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2798 - val_loss: 0.2602\n",
      "Epoch 69/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2801 - val_loss: 0.2600\n",
      "Epoch 70/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2781 - val_loss: 0.2586\n",
      "Epoch 71/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2791 - val_loss: 0.2592\n",
      "Epoch 72/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2786 - val_loss: 0.2621\n",
      "Epoch 73/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 00072: reducing learning rate to 0.00010000000474974513.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2783 - val_loss: 0.2607\n",
      "Epoch 74/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2779 - val_loss: 0.2597\n",
      "Epoch 75/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2767 - val_loss: 0.2586\n",
      "Epoch 76/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2760 - val_loss: 0.2582\n",
      "Epoch 77/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2761 - val_loss: 0.2578\n",
      "Epoch 78/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2577\n",
      "Epoch 79/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2767 - val_loss: 0.2580\n",
      "Epoch 80/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2761 - val_loss: 0.2578\n",
      "Epoch 81/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2760 - val_loss: 0.2577\n",
      "Epoch 82/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2577\n",
      "Epoch 83/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2756 - val_loss: 0.2578\n",
      "Epoch 84/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2575\n",
      "Epoch 85/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2576\n",
      "Epoch 86/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2760 - val_loss: 0.2575\n",
      "Epoch 87/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2763 - val_loss: 0.2574\n",
      "Epoch 88/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2577\n",
      "Epoch 89/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2761 - val_loss: 0.2577\n",
      "Epoch 90/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2751 - val_loss: 0.2574\n",
      "Epoch 91/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2758 - val_loss: 0.2573\n",
      "Epoch 92/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2757 - val_loss: 0.2576\n",
      "Epoch 93/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2755\n",
      "Epoch 00092: reducing learning rate to 1.0000000474974514e-05.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2754 - val_loss: 0.2578\n",
      "Epoch 94/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2756 - val_loss: 0.2577\n",
      "Epoch 95/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2748 - val_loss: 0.2575\n",
      "Epoch 96/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2749 - val_loss: 0.2574\n",
      "Epoch 97/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2758 - val_loss: 0.2573\n",
      "Epoch 98/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2750 - val_loss: 0.2573\n",
      "Epoch 99/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2761 - val_loss: 0.2572\n",
      "Epoch 100/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2750 - val_loss: 0.2572\n",
      "Epoch 101/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2757 - val_loss: 0.2572\n",
      "Epoch 102/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2751 - val_loss: 0.2571\n",
      "Epoch 103/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2760 - val_loss: 0.2571\n",
      "Epoch 104/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2753 - val_loss: 0.2571\n",
      "Epoch 105/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2571\n",
      "Epoch 106/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2759 - val_loss: 0.2571\n",
      "Epoch 107/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2755 - val_loss: 0.2570\n",
      "Epoch 108/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2750 - val_loss: 0.2570\n",
      "Epoch 109/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2756 - val_loss: 0.2570\n",
      "Epoch 110/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2756 - val_loss: 0.2570\n",
      "Epoch 111/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2758 - val_loss: 0.2570\n",
      "Epoch 112/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2758 - val_loss: 0.2570\n",
      "Epoch 113/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2752\n",
      "Epoch 00112: reducing learning rate to 1.0000000656873453e-06.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2752 - val_loss: 0.2570\n",
      "Epoch 114/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2754 - val_loss: 0.2570\n",
      "Epoch 115/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2751 - val_loss: 0.2570\n",
      "Epoch 116/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2754 - val_loss: 0.2570\n",
      "Epoch 117/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2756 - val_loss: 0.2570\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2754\n",
      "Epoch 00117: reducing learning rate to 1.0000001111620805e-07.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2570\n",
      "Epoch 119/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2753 - val_loss: 0.2570\n",
      "Epoch 120/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2755 - val_loss: 0.2570\n",
      "Epoch 121/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2751 - val_loss: 0.2570\n",
      "Epoch 122/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2757 - val_loss: 0.2570\n",
      "Epoch 123/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2753\n",
      "Epoch 00122: reducing learning rate to 1.000000082740371e-08.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2570\n",
      "Epoch 124/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2755 - val_loss: 0.2570\n",
      "Epoch 125/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2753 - val_loss: 0.2570\n",
      "Epoch 126/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2750 - val_loss: 0.2570\n",
      "Epoch 127/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2751 - val_loss: 0.2570\n",
      "Epoch 128/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2749\n",
      "Epoch 00127: reducing learning rate to 1.000000082740371e-09.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2749 - val_loss: 0.2570\n",
      "Epoch 129/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2759 - val_loss: 0.2570\n",
      "Epoch 130/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2746 - val_loss: 0.2570\n",
      "Epoch 131/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2751 - val_loss: 0.2570\n",
      "Epoch 132/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2761 - val_loss: 0.2570\n",
      "Epoch 133/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2752\n",
      "Epoch 00132: reducing learning rate to 1.000000082740371e-10.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2756 - val_loss: 0.2570\n",
      "Epoch 134/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2748 - val_loss: 0.2570\n",
      "Epoch 135/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2755 - val_loss: 0.2570\n",
      "Epoch 136/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2751 - val_loss: 0.2570\n",
      "Epoch 137/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2746 - val_loss: 0.2570\n",
      "Epoch 138/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2756\n",
      "Epoch 00137: reducing learning rate to 1.000000082740371e-11.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2570\n",
      "Fold RMSLE : 0.506932\n",
      "\n",
      "Fold 5 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185083 samples, validate on 46270 samples\n",
      "Epoch 1/500\n",
      "185083/185083 [==============================] - 7s - loss: 1.3653 - val_loss: 0.6239\n",
      "Epoch 2/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.7467 - val_loss: 0.6198\n",
      "Epoch 3/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5914 - val_loss: 0.5937\n",
      "Epoch 4/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5240 - val_loss: 0.5543\n",
      "Epoch 5/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4893 - val_loss: 0.5147\n",
      "Epoch 6/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4626 - val_loss: 0.4915\n",
      "Epoch 7/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4445 - val_loss: 0.4688\n",
      "Epoch 8/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4296 - val_loss: 0.4466\n",
      "Epoch 9/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4182 - val_loss: 0.4354\n",
      "Epoch 10/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4096 - val_loss: 0.4217\n",
      "Epoch 11/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4015 - val_loss: 0.4149\n",
      "Epoch 12/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3924 - val_loss: 0.4073\n",
      "Epoch 13/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3873 - val_loss: 0.3877\n",
      "Epoch 14/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3818 - val_loss: 0.3798\n",
      "Epoch 15/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3759 - val_loss: 0.3711\n",
      "Epoch 16/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3703 - val_loss: 0.3676\n",
      "Epoch 17/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3667 - val_loss: 0.3587\n",
      "Epoch 18/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3628 - val_loss: 0.3503\n",
      "Epoch 19/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3580 - val_loss: 0.3461\n",
      "Epoch 20/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3539 - val_loss: 0.3439\n",
      "Epoch 21/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3508 - val_loss: 0.3356\n",
      "Epoch 22/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3483 - val_loss: 0.3316\n",
      "Epoch 23/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3437 - val_loss: 0.3275\n",
      "Epoch 24/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3414 - val_loss: 0.3256\n",
      "Epoch 25/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3375 - val_loss: 0.3229\n",
      "Epoch 26/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3368 - val_loss: 0.3176\n",
      "Epoch 27/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3339 - val_loss: 0.3156\n",
      "Epoch 28/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3323 - val_loss: 0.3109\n",
      "Epoch 29/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3307 - val_loss: 0.3065\n",
      "Epoch 30/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3271 - val_loss: 0.3044\n",
      "Epoch 31/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3281 - val_loss: 0.3045\n",
      "Epoch 32/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3251 - val_loss: 0.2993\n",
      "Epoch 33/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3232 - val_loss: 0.2960\n",
      "Epoch 34/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3204 - val_loss: 0.2913\n",
      "Epoch 35/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3190 - val_loss: 0.2916\n",
      "Epoch 36/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3176 - val_loss: 0.2917\n",
      "Epoch 37/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3159 - val_loss: 0.2830\n",
      "Epoch 38/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3138 - val_loss: 0.2883\n",
      "Epoch 39/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3113 - val_loss: 0.2824\n",
      "Epoch 40/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3109 - val_loss: 0.2875\n",
      "Epoch 41/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3090 - val_loss: 0.2807\n",
      "Epoch 42/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3090 - val_loss: 0.2796\n",
      "Epoch 43/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3058 - val_loss: 0.2807\n",
      "Epoch 44/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3048 - val_loss: 0.2798\n",
      "Epoch 45/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3036 - val_loss: 0.2757\n",
      "Epoch 46/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3034 - val_loss: 0.2751\n",
      "Epoch 47/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3011 - val_loss: 0.2744\n",
      "Epoch 48/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2742\n",
      "Epoch 49/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2722\n",
      "Epoch 50/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2984 - val_loss: 0.2719\n",
      "Epoch 51/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2979 - val_loss: 0.2694\n",
      "Epoch 52/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2969 - val_loss: 0.2718\n",
      "Epoch 53/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2960 - val_loss: 0.2676\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 4s - loss: 0.2953 - val_loss: 0.2672\n",
      "Epoch 55/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2935 - val_loss: 0.2673\n",
      "Epoch 56/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2938 - val_loss: 0.2669\n",
      "Epoch 57/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2921 - val_loss: 0.2686\n",
      "Epoch 58/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2906 - val_loss: 0.2659\n",
      "Epoch 59/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2897 - val_loss: 0.2688\n",
      "Epoch 60/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2886 - val_loss: 0.2674\n",
      "Epoch 61/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2891 - val_loss: 0.2650\n",
      "Epoch 62/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2881 - val_loss: 0.2667\n",
      "Epoch 63/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2873 - val_loss: 0.2647\n",
      "Epoch 64/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2875 - val_loss: 0.2649\n",
      "Epoch 65/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2857 - val_loss: 0.2663\n",
      "Epoch 66/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2662\n",
      "Epoch 67/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2844 - val_loss: 0.2652\n",
      "Epoch 68/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2843 - val_loss: 0.2642\n",
      "Epoch 69/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2844 - val_loss: 0.2638\n",
      "Epoch 70/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2827 - val_loss: 0.2649\n",
      "Epoch 71/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2820 - val_loss: 0.2639\n",
      "Epoch 72/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2808 - val_loss: 0.2644\n",
      "Epoch 73/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2808 - val_loss: 0.2628\n",
      "Epoch 74/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2805 - val_loss: 0.2634\n",
      "Epoch 75/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2800 - val_loss: 0.2644\n",
      "Epoch 76/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2789 - val_loss: 0.2676\n",
      "Epoch 77/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2782 - val_loss: 0.2629\n",
      "Epoch 78/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2780 - val_loss: 0.2632\n",
      "Epoch 79/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2768 - val_loss: 0.2613\n",
      "Epoch 80/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2773 - val_loss: 0.2633\n",
      "Epoch 81/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2765 - val_loss: 0.2661\n",
      "Epoch 82/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2763 - val_loss: 0.2633\n",
      "Epoch 83/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2754 - val_loss: 0.2630\n",
      "Epoch 84/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2749 - val_loss: 0.2655\n",
      "Epoch 85/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2743\n",
      "Epoch 00084: reducing learning rate to 0.00010000000474974513.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2744 - val_loss: 0.2625\n",
      "Epoch 86/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2747 - val_loss: 0.2614\n",
      "Epoch 87/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2736 - val_loss: 0.2613\n",
      "Epoch 88/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2616\n",
      "Epoch 89/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2616\n",
      "Epoch 90/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2729\n",
      "Epoch 00089: reducing learning rate to 1.0000000474974514e-05.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2613\n",
      "Epoch 91/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2739 - val_loss: 0.2612\n",
      "Epoch 92/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2732 - val_loss: 0.2611\n",
      "Epoch 93/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2740 - val_loss: 0.2611\n",
      "Epoch 94/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2611\n",
      "Epoch 95/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2736 - val_loss: 0.2610\n",
      "Epoch 96/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2727 - val_loss: 0.2611\n",
      "Epoch 97/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2739 - val_loss: 0.2610\n",
      "Epoch 98/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2731 - val_loss: 0.2610\n",
      "Epoch 99/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2730 - val_loss: 0.2609\n",
      "Epoch 100/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2737 - val_loss: 0.2608\n",
      "Epoch 101/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2735 - val_loss: 0.2608\n",
      "Epoch 102/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2608\n",
      "Epoch 103/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2731 - val_loss: 0.2608\n",
      "Epoch 104/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2728 - val_loss: 0.2608\n",
      "Epoch 105/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2736 - val_loss: 0.2607\n",
      "Epoch 106/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2731 - val_loss: 0.2607\n",
      "Epoch 107/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2742 - val_loss: 0.2607\n",
      "Epoch 108/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2607\n",
      "Epoch 109/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2606\n",
      "Epoch 110/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2732 - val_loss: 0.2606\n",
      "Epoch 111/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2606\n",
      "Epoch 112/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2605\n",
      "Epoch 113/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2730 - val_loss: 0.2605\n",
      "Epoch 114/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2728 - val_loss: 0.2605\n",
      "Epoch 115/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2605\n",
      "Epoch 116/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2742 - val_loss: 0.2605\n",
      "Epoch 117/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2730\n",
      "Epoch 00116: reducing learning rate to 1.0000000656873453e-06.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2731 - val_loss: 0.2605\n",
      "Epoch 118/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2605\n",
      "Epoch 119/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2739 - val_loss: 0.2605\n",
      "Epoch 120/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2730 - val_loss: 0.2605\n",
      "Epoch 121/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2739 - val_loss: 0.2605\n",
      "Epoch 122/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2734\n",
      "Epoch 00121: reducing learning rate to 1.0000001111620805e-07.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2731 - val_loss: 0.2605\n",
      "Epoch 123/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2737 - val_loss: 0.2605\n",
      "Epoch 124/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2739 - val_loss: 0.2605\n",
      "Epoch 125/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2740 - val_loss: 0.2605\n",
      "Epoch 126/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 127/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2724\n",
      "Epoch 00126: reducing learning rate to 1.000000082740371e-08.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2726 - val_loss: 0.2605\n",
      "Epoch 128/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2737 - val_loss: 0.2605\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 5s - loss: 0.2732 - val_loss: 0.2605\n",
      "Epoch 130/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 131/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 132/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2730 - val_loss: 0.2605\n",
      "Epoch 133/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 134/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 135/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2605\n",
      "Epoch 136/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2740 - val_loss: 0.2605\n",
      "Epoch 137/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2605\n",
      "Epoch 138/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2732\n",
      "Epoch 00137: reducing learning rate to 1.000000082740371e-09.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2605\n",
      "Epoch 139/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 140/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2729 - val_loss: 0.2605\n",
      "Epoch 141/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2605\n",
      "Epoch 142/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2739 - val_loss: 0.2605\n",
      "Epoch 143/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2737\n",
      "Epoch 00142: reducing learning rate to 1.000000082740371e-10.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2737 - val_loss: 0.2605\n",
      "Epoch 144/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2605\n",
      "Epoch 145/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2728 - val_loss: 0.2605\n",
      "Epoch 146/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2605\n",
      "Epoch 147/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2737 - val_loss: 0.2605\n",
      "Epoch 148/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2740\n",
      "Epoch 00147: reducing learning rate to 1.000000082740371e-11.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2738 - val_loss: 0.2605\n",
      "Epoch 149/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2605\n",
      "Epoch 150/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2727 - val_loss: 0.2605\n",
      "Epoch 151/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2737 - val_loss: 0.2605\n",
      "Epoch 152/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2734 - val_loss: 0.2605\n",
      "Epoch 153/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2730\n",
      "Epoch 00152: reducing learning rate to 1.000000082740371e-12.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2731 - val_loss: 0.2605\n",
      "Epoch 154/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2728 - val_loss: 0.2605\n",
      "Epoch 155/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2736 - val_loss: 0.2605\n",
      "Epoch 156/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2735 - val_loss: 0.2604\n",
      "Epoch 157/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2736 - val_loss: 0.2604\n",
      "Epoch 158/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2733\n",
      "Epoch 00157: reducing learning rate to 1.0000001044244145e-13.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2730 - val_loss: 0.2604\n",
      "Epoch 159/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2733 - val_loss: 0.2604\n",
      "Epoch 160/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2739 - val_loss: 0.2604\n",
      "Epoch 161/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2734 - val_loss: 0.2604\n",
      "Epoch 162/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2604\n",
      "Epoch 163/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2732\n",
      "Epoch 00162: reducing learning rate to 1.0000001179769417e-14.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2733 - val_loss: 0.2604\n",
      "Epoch 164/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2604\n",
      "Epoch 165/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2735 - val_loss: 0.2604\n",
      "Epoch 166/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2604\n",
      "Epoch 167/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2731 - val_loss: 0.2604\n",
      "Epoch 168/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2734\n",
      "Epoch 00167: reducing learning rate to 1.0000001518582595e-15.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2604\n",
      "Epoch 169/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2604\n",
      "Epoch 170/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2732 - val_loss: 0.2604\n",
      "Epoch 171/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2739 - val_loss: 0.2604\n",
      "Epoch 172/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2734 - val_loss: 0.2604\n",
      "Fold RMSLE : 0.510342\n",
      "CV RMSLE : 0.510665\n"
     ]
    }
   ],
   "source": [
    "pred_tr1, pred_te1 = model_run('train1')\n",
    "pred_tr1.to_csv(path+'/'+ver+'_pred_tr1.csv', index=False)\n",
    "pred_te1.to_csv(path+'/'+ver+'_pred_te1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 7s - loss: 1.3266 - val_loss: 0.6152\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.7332 - val_loss: 0.5914\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.6023 - val_loss: 0.5642\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.5465 - val_loss: 0.5357\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5109 - val_loss: 0.5043\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4883 - val_loss: 0.4837\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4709 - val_loss: 0.4670\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4549 - val_loss: 0.4536\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4462 - val_loss: 0.4400\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4360 - val_loss: 0.4299\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4284 - val_loss: 0.4202\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4225 - val_loss: 0.4077\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4174 - val_loss: 0.4003\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4101 - val_loss: 0.3932\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4049 - val_loss: 0.3830\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4036 - val_loss: 0.3774\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3947 - val_loss: 0.3713\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3919 - val_loss: 0.3673\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3892 - val_loss: 0.3667\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3841 - val_loss: 0.3581\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3809 - val_loss: 0.3520\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3788 - val_loss: 0.3540\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3735 - val_loss: 0.3481\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3720 - val_loss: 0.3468\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3722 - val_loss: 0.3391\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3678 - val_loss: 0.3338\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3649 - val_loss: 0.3330\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3643 - val_loss: 0.3309\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3608 - val_loss: 0.3274\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3585 - val_loss: 0.3249\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3559 - val_loss: 0.3248\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3551 - val_loss: 0.3197\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3545 - val_loss: 0.3190\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3519 - val_loss: 0.3177\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3519 - val_loss: 0.3161\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3482 - val_loss: 0.3156\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3466 - val_loss: 0.3130\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3447 - val_loss: 0.3120\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3430 - val_loss: 0.3101\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3415 - val_loss: 0.3096\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3418 - val_loss: 0.3078\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3386 - val_loss: 0.3075\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3381 - val_loss: 0.3056\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3371 - val_loss: 0.3066\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3362 - val_loss: 0.3024\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3344 - val_loss: 0.3048\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3341 - val_loss: 0.3031\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3322 - val_loss: 0.3027\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3305 - val_loss: 0.3017\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3302 - val_loss: 0.3020\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3278 - val_loss: 0.3013\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3284 - val_loss: 0.3011\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3269 - val_loss: 0.2995\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3252 - val_loss: 0.2993\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3246 - val_loss: 0.2990\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3230 - val_loss: 0.3019\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3232 - val_loss: 0.2987\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3218 - val_loss: 0.3008\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3213 - val_loss: 0.2977\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3196 - val_loss: 0.2966\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3188 - val_loss: 0.2965\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3175 - val_loss: 0.2957\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3175 - val_loss: 0.2971\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3164 - val_loss: 0.2950\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3163 - val_loss: 0.2948\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3151 - val_loss: 0.2937\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3146 - val_loss: 0.2949\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3146 - val_loss: 0.2995\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3128 - val_loss: 0.2960\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3128 - val_loss: 0.2928\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3107 - val_loss: 0.2925\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3113 - val_loss: 0.2911\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3101 - val_loss: 0.2922\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3103 - val_loss: 0.2918\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3089 - val_loss: 0.2942\n",
      "Epoch 76/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3091 - val_loss: 0.2909\n",
      "Epoch 77/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3089 - val_loss: 0.2909\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3090 - val_loss: 0.2988\n",
      "Epoch 79/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3078 - val_loss: 0.2900\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3069 - val_loss: 0.2960\n",
      "Epoch 81/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3059 - val_loss: 0.2903\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3053 - val_loss: 0.2898\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.3056 - val_loss: 0.2911\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3050 - val_loss: 0.2974\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3040 - val_loss: 0.2890\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3037 - val_loss: 0.2890\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3036 - val_loss: 0.2949\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3035 - val_loss: 0.2894\n",
      "Epoch 89/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3029 - val_loss: 0.2886\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3024 - val_loss: 0.2889\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3012 - val_loss: 0.2881\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3021 - val_loss: 0.2886\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3011 - val_loss: 0.2889\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3004 - val_loss: 0.2890\n",
      "Epoch 95/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3006 - val_loss: 0.2894\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3008 - val_loss: 0.2911\n",
      "Epoch 97/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2994\n",
      "Epoch 00096: reducing learning rate to 0.00010000000474974513.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2995 - val_loss: 0.2881\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2991 - val_loss: 0.2871\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2886\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2877\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2987 - val_loss: 0.2869\n",
      "Epoch 102/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2984 - val_loss: 0.2867\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2987 - val_loss: 0.2866\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2868\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2864\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2865\n",
      "Epoch 107/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2864\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2868\n",
      "Epoch 109/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2867\n",
      "Epoch 110/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2981 - val_loss: 0.2863\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2867\n",
      "Epoch 112/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2863\n",
      "Epoch 113/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2867\n",
      "Epoch 114/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2864\n",
      "Epoch 115/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2863\n",
      "Epoch 116/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2985\n",
      "Epoch 00115: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2988 - val_loss: 0.2862\n",
      "Epoch 117/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2862\n",
      "Epoch 118/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2975 - val_loss: 0.2862\n",
      "Epoch 119/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2862\n",
      "Epoch 120/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2862\n",
      "Epoch 121/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2862\n",
      "Epoch 122/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2982 - val_loss: 0.2861\n",
      "Epoch 123/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2861\n",
      "Epoch 124/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2861\n",
      "Epoch 125/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2973 - val_loss: 0.2861\n",
      "Epoch 126/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00125: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2861\n",
      "Epoch 127/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2982 - val_loss: 0.2861\n",
      "Epoch 128/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2982 - val_loss: 0.2861\n",
      "Epoch 129/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2973 - val_loss: 0.2861\n",
      "Epoch 130/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2861\n",
      "Epoch 131/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00130: reducing learning rate to 1.0000001111620805e-07.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2861\n",
      "Epoch 132/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2861\n",
      "Epoch 133/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2861\n",
      "Epoch 134/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2861\n",
      "Epoch 135/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2861\n",
      "Epoch 136/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00135: reducing learning rate to 1.000000082740371e-08.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2861\n",
      "Epoch 137/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2861\n",
      "Epoch 138/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2971 - val_loss: 0.2861\n",
      "Epoch 139/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2861\n",
      "Epoch 140/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2971 - val_loss: 0.2861\n",
      "Epoch 141/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00140: reducing learning rate to 1.000000082740371e-09.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2861\n",
      "Epoch 142/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2861\n",
      "Epoch 143/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2861\n",
      "Epoch 144/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2975 - val_loss: 0.2861\n",
      "Epoch 145/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2978 - val_loss: 0.2861\n",
      "Epoch 146/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00145: reducing learning rate to 1.000000082740371e-10.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2861\n",
      "Epoch 147/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2861\n",
      "Epoch 148/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2861\n",
      "Epoch 149/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2967 - val_loss: 0.2861\n",
      "Epoch 150/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2971 - val_loss: 0.2861\n",
      "Epoch 151/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00150: reducing learning rate to 1.000000082740371e-11.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2975 - val_loss: 0.2861\n",
      "Fold RMSLE : 0.534873\n",
      "\n",
      "Fold 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kohei\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 7s - loss: 1.3800 - val_loss: 0.6610\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.7441 - val_loss: 0.6332\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.6093 - val_loss: 0.5981\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5444 - val_loss: 0.5657\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5186 - val_loss: 0.5354\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4919 - val_loss: 0.5108\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4759 - val_loss: 0.4940\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4599 - val_loss: 0.4719\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4508 - val_loss: 0.4597\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4397 - val_loss: 0.4487\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4297 - val_loss: 0.4314\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4215 - val_loss: 0.4233\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4212 - val_loss: 0.4082\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4132 - val_loss: 0.4052\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4083 - val_loss: 0.3980\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4045 - val_loss: 0.3920\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3985 - val_loss: 0.3840\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3944 - val_loss: 0.3784\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3897 - val_loss: 0.3751\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3866 - val_loss: 0.3720\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3858 - val_loss: 0.3631\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3786 - val_loss: 0.3593\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3784 - val_loss: 0.3554\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3754 - val_loss: 0.3532\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3731 - val_loss: 0.3474\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3723 - val_loss: 0.3441\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3696 - val_loss: 0.3442\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3664 - val_loss: 0.3417\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3652 - val_loss: 0.3310\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3635 - val_loss: 0.3374\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3595 - val_loss: 0.3314\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3575 - val_loss: 0.3274\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3577 - val_loss: 0.3217\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3540 - val_loss: 0.3200\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3529 - val_loss: 0.3298\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3502 - val_loss: 0.3189\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3500 - val_loss: 0.3204\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3461 - val_loss: 0.3170\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3446 - val_loss: 0.3187\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3459 - val_loss: 0.3172\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3445 - val_loss: 0.3128\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3414 - val_loss: 0.3132\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3396 - val_loss: 0.3099\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3396 - val_loss: 0.3113\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3383 - val_loss: 0.3067\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3355 - val_loss: 0.3099\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3359 - val_loss: 0.3084\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3350 - val_loss: 0.3101\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3335 - val_loss: 0.3048\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3311 - val_loss: 0.3049\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3322 - val_loss: 0.3087\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3288 - val_loss: 0.3031\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3270 - val_loss: 0.3001\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3268 - val_loss: 0.2989\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3244 - val_loss: 0.2979\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3248 - val_loss: 0.2978\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3236 - val_loss: 0.2971\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3236 - val_loss: 0.3006\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3218 - val_loss: 0.2986\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3203 - val_loss: 0.2967\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3203 - val_loss: 0.2968\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3197 - val_loss: 0.2991\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3187 - val_loss: 0.2995\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3177 - val_loss: 0.2942\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3172 - val_loss: 0.2981\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3148 - val_loss: 0.2924\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3144 - val_loss: 0.3022\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3150 - val_loss: 0.2919\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3138 - val_loss: 0.2917\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3121 - val_loss: 0.2930\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3123 - val_loss: 0.2899\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3123 - val_loss: 0.2906\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3110 - val_loss: 0.2899\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3101 - val_loss: 0.2900\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3109 - val_loss: 0.2908\n",
      "Epoch 76/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3096 - val_loss: 0.2909\n",
      "Epoch 77/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3084\n",
      "Epoch 00076: reducing learning rate to 0.00010000000474974513.\n",
      "185082/185082 [==============================] - 5s - loss: 0.3082 - val_loss: 0.2901\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3076 - val_loss: 0.2885\n",
      "Epoch 79/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3074 - val_loss: 0.2889\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3073 - val_loss: 0.2887\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 5s - loss: 0.3065 - val_loss: 0.2884\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3067 - val_loss: 0.2879\n",
      "Epoch 83/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3067 - val_loss: 0.2880\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3064 - val_loss: 0.2876\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3056 - val_loss: 0.2876\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3059 - val_loss: 0.2880\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3069 - val_loss: 0.2882\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3068 - val_loss: 0.2880\n",
      "Epoch 89/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3060 - val_loss: 0.2878\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3056 - val_loss: 0.2874\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3060 - val_loss: 0.2873\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3064 - val_loss: 0.2873\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3058 - val_loss: 0.2871\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3060 - val_loss: 0.2870\n",
      "Epoch 95/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3056 - val_loss: 0.2870\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3060 - val_loss: 0.2868\n",
      "Epoch 97/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3062 - val_loss: 0.2870\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3054 - val_loss: 0.2874\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3063 - val_loss: 0.2869\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3055 - val_loss: 0.2869\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3056 - val_loss: 0.2869\n",
      "Epoch 102/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3050\n",
      "Epoch 00101: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 5s - loss: 0.3051 - val_loss: 0.2867\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2867\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3059 - val_loss: 0.2867\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2867\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3051 - val_loss: 0.2867\n",
      "Epoch 107/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3042 - val_loss: 0.2867\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3051 - val_loss: 0.2867\n",
      "Epoch 109/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3052 - val_loss: 0.2867\n",
      "Epoch 110/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3048 - val_loss: 0.2866\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 112/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3049\n",
      "Epoch 00111: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 4s - loss: 0.3047 - val_loss: 0.2867\n",
      "Epoch 113/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3061 - val_loss: 0.2867\n",
      "Epoch 114/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2866\n",
      "Epoch 115/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 116/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3051 - val_loss: 0.2866\n",
      "Epoch 117/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3048\n",
      "Epoch 00116: reducing learning rate to 1.0000001111620805e-07.\n",
      "185082/185082 [==============================] - 4s - loss: 0.3050 - val_loss: 0.2866\n",
      "Epoch 118/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3048 - val_loss: 0.2866\n",
      "Epoch 119/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3053 - val_loss: 0.2866\n",
      "Epoch 120/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3048 - val_loss: 0.2866\n",
      "Epoch 121/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 122/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3060 - val_loss: 0.2866\n",
      "Epoch 123/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3054 - val_loss: 0.2866\n",
      "Epoch 124/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 125/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3055 - val_loss: 0.2866\n",
      "Epoch 126/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3050 - val_loss: 0.2866\n",
      "Epoch 127/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3053\n",
      "Epoch 00126: reducing learning rate to 1.000000082740371e-08.\n",
      "185082/185082 [==============================] - 5s - loss: 0.3052 - val_loss: 0.2866\n",
      "Epoch 128/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3055 - val_loss: 0.2866\n",
      "Epoch 129/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3063 - val_loss: 0.2866\n",
      "Epoch 130/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3047 - val_loss: 0.2866\n",
      "Epoch 131/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3054 - val_loss: 0.2866\n",
      "Epoch 132/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3048\n",
      "Epoch 00131: reducing learning rate to 1.000000082740371e-09.\n",
      "185082/185082 [==============================] - 4s - loss: 0.3048 - val_loss: 0.2866\n",
      "Epoch 133/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 134/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3050 - val_loss: 0.2866\n",
      "Epoch 135/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3047 - val_loss: 0.2866\n",
      "Epoch 136/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 137/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3052\n",
      "Epoch 00136: reducing learning rate to 1.000000082740371e-10.\n",
      "185082/185082 [==============================] - 5s - loss: 0.3054 - val_loss: 0.2866\n",
      "Epoch 138/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3047 - val_loss: 0.2866\n",
      "Epoch 139/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2866\n",
      "Epoch 140/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3048 - val_loss: 0.2866\n",
      "Epoch 141/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3046 - val_loss: 0.2866\n",
      "Epoch 142/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3051\n",
      "Epoch 00141: reducing learning rate to 1.000000082740371e-11.\n",
      "185082/185082 [==============================] - 5s - loss: 0.3051 - val_loss: 0.2866\n",
      "Epoch 143/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 144/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3049 - val_loss: 0.2866\n",
      "Epoch 145/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3056 - val_loss: 0.2866\n",
      "Epoch 146/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2866\n",
      "Epoch 147/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3047\n",
      "Epoch 00146: reducing learning rate to 1.000000082740371e-12.\n",
      "185082/185082 [==============================] - 4s - loss: 0.3047 - val_loss: 0.2866\n",
      "Epoch 148/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3050 - val_loss: 0.2866\n",
      "Epoch 149/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3057 - val_loss: 0.2866\n",
      "Epoch 150/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2866\n",
      "Epoch 151/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3046 - val_loss: 0.2866\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.3057\n",
      "Epoch 00151: reducing learning rate to 1.0000001044244145e-13.\n",
      "185082/185082 [==============================] - 4s - loss: 0.3057 - val_loss: 0.2866\n",
      "Epoch 153/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3042 - val_loss: 0.2866\n",
      "Fold RMSLE : 0.535318\n",
      "\n",
      "Fold 3 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185082 samples, validate on 46271 samples\n",
      "Epoch 1/500\n",
      "185082/185082 [==============================] - 7s - loss: 1.2292 - val_loss: 0.7096\n",
      "Epoch 2/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.6801 - val_loss: 0.7080\n",
      "Epoch 3/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5711 - val_loss: 0.6812\n",
      "Epoch 4/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.5208 - val_loss: 0.6500\n",
      "Epoch 5/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4872 - val_loss: 0.6150\n",
      "Epoch 6/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.4666 - val_loss: 0.5832\n",
      "Epoch 7/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4489 - val_loss: 0.5484\n",
      "Epoch 8/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4354 - val_loss: 0.5212\n",
      "Epoch 9/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4245 - val_loss: 0.4981\n",
      "Epoch 10/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4156 - val_loss: 0.4781\n",
      "Epoch 11/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4082 - val_loss: 0.4632\n",
      "Epoch 12/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.4044 - val_loss: 0.4469\n",
      "Epoch 13/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3974 - val_loss: 0.4353\n",
      "Epoch 14/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3927 - val_loss: 0.4242\n",
      "Epoch 15/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3866 - val_loss: 0.4143\n",
      "Epoch 16/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3794 - val_loss: 0.4074\n",
      "Epoch 17/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3789 - val_loss: 0.3996\n",
      "Epoch 18/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3746 - val_loss: 0.3923\n",
      "Epoch 19/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3712 - val_loss: 0.3889\n",
      "Epoch 20/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3685 - val_loss: 0.3831\n",
      "Epoch 21/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3670 - val_loss: 0.3713\n",
      "Epoch 22/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3626 - val_loss: 0.3716\n",
      "Epoch 23/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3591 - val_loss: 0.3635\n",
      "Epoch 24/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3565 - val_loss: 0.3641\n",
      "Epoch 25/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3548 - val_loss: 0.3614\n",
      "Epoch 26/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3525 - val_loss: 0.3571\n",
      "Epoch 27/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3511 - val_loss: 0.3612\n",
      "Epoch 28/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3476 - val_loss: 0.3494\n",
      "Epoch 29/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3453 - val_loss: 0.3513\n",
      "Epoch 30/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3428 - val_loss: 0.3428\n",
      "Epoch 31/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3429 - val_loss: 0.3487\n",
      "Epoch 32/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3388 - val_loss: 0.3417\n",
      "Epoch 33/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3394 - val_loss: 0.3370\n",
      "Epoch 34/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3370 - val_loss: 0.3417\n",
      "Epoch 35/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3356 - val_loss: 0.3350\n",
      "Epoch 36/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3357 - val_loss: 0.3359\n",
      "Epoch 37/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3323 - val_loss: 0.3362\n",
      "Epoch 38/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3313 - val_loss: 0.3256\n",
      "Epoch 39/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3299 - val_loss: 0.3284\n",
      "Epoch 40/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3285 - val_loss: 0.3248\n",
      "Epoch 41/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3282 - val_loss: 0.3230\n",
      "Epoch 42/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3268 - val_loss: 0.3221\n",
      "Epoch 43/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3246 - val_loss: 0.3175\n",
      "Epoch 44/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3246 - val_loss: 0.3153\n",
      "Epoch 45/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3223 - val_loss: 0.3240\n",
      "Epoch 46/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3225 - val_loss: 0.3150\n",
      "Epoch 47/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3209 - val_loss: 0.3186\n",
      "Epoch 48/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3196 - val_loss: 0.3222\n",
      "Epoch 49/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3193 - val_loss: 0.3117\n",
      "Epoch 50/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3179 - val_loss: 0.3128\n",
      "Epoch 51/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3161 - val_loss: 0.3131\n",
      "Epoch 52/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3143 - val_loss: 0.3092\n",
      "Epoch 53/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3144 - val_loss: 0.3114\n",
      "Epoch 54/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3146 - val_loss: 0.3155\n",
      "Epoch 55/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3134 - val_loss: 0.3087\n",
      "Epoch 56/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3127 - val_loss: 0.3093\n",
      "Epoch 57/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3105 - val_loss: 0.3150\n",
      "Epoch 58/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3103 - val_loss: 0.3080\n",
      "Epoch 59/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3104 - val_loss: 0.3076\n",
      "Epoch 60/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3090 - val_loss: 0.3034\n",
      "Epoch 61/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3083 - val_loss: 0.3081\n",
      "Epoch 62/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3071 - val_loss: 0.3066\n",
      "Epoch 63/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3070 - val_loss: 0.3008\n",
      "Epoch 64/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3054 - val_loss: 0.3020\n",
      "Epoch 65/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3053 - val_loss: 0.2999\n",
      "Epoch 66/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3045 - val_loss: 0.3013\n",
      "Epoch 67/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3051 - val_loss: 0.3022\n",
      "Epoch 68/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3043 - val_loss: 0.3011\n",
      "Epoch 69/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3029 - val_loss: 0.3008\n",
      "Epoch 70/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3035 - val_loss: 0.2975\n",
      "Epoch 71/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3017 - val_loss: 0.2989\n",
      "Epoch 72/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3010 - val_loss: 0.2991\n",
      "Epoch 73/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3014 - val_loss: 0.2967\n",
      "Epoch 74/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2973\n",
      "Epoch 75/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.3000 - val_loss: 0.2995\n",
      "Epoch 76/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2998 - val_loss: 0.2965\n",
      "Epoch 77/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2988 - val_loss: 0.2992\n",
      "Epoch 78/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2990 - val_loss: 0.2963\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2978\n",
      "Epoch 80/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2971\n",
      "Epoch 81/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2973 - val_loss: 0.2954\n",
      "Epoch 82/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2963 - val_loss: 0.2997\n",
      "Epoch 83/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2961 - val_loss: 0.2958\n",
      "Epoch 84/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2957 - val_loss: 0.2955\n",
      "Epoch 85/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2962 - val_loss: 0.2952\n",
      "Epoch 86/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2952 - val_loss: 0.3007\n",
      "Epoch 87/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2947 - val_loss: 0.2952\n",
      "Epoch 88/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2943 - val_loss: 0.2953\n",
      "Epoch 89/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2950 - val_loss: 0.2956\n",
      "Epoch 90/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2940 - val_loss: 0.2941\n",
      "Epoch 91/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2935 - val_loss: 0.2947\n",
      "Epoch 92/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2926 - val_loss: 0.2943\n",
      "Epoch 93/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2930 - val_loss: 0.2944\n",
      "Epoch 94/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2929 - val_loss: 0.2945\n",
      "Epoch 95/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2927 - val_loss: 0.2979\n",
      "Epoch 96/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2925 - val_loss: 0.2938\n",
      "Epoch 97/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2919 - val_loss: 0.2966\n",
      "Epoch 98/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2928 - val_loss: 0.2941\n",
      "Epoch 99/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2910 - val_loss: 0.2952\n",
      "Epoch 100/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2903 - val_loss: 0.2936\n",
      "Epoch 101/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2904 - val_loss: 0.2931\n",
      "Epoch 102/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2898 - val_loss: 0.2932\n",
      "Epoch 103/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2902 - val_loss: 0.2925\n",
      "Epoch 104/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2897 - val_loss: 0.2952\n",
      "Epoch 105/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2896 - val_loss: 0.2943\n",
      "Epoch 106/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2890 - val_loss: 0.2944\n",
      "Epoch 107/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2894 - val_loss: 0.2932\n",
      "Epoch 108/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2885 - val_loss: 0.2928\n",
      "Epoch 109/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2886\n",
      "Epoch 00108: reducing learning rate to 0.00010000000474974513.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2886 - val_loss: 0.2928\n",
      "Epoch 110/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2877 - val_loss: 0.2926\n",
      "Epoch 111/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2872 - val_loss: 0.2918\n",
      "Epoch 112/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2872 - val_loss: 0.2918\n",
      "Epoch 113/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2866 - val_loss: 0.2917\n",
      "Epoch 114/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2867 - val_loss: 0.2918\n",
      "Epoch 115/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2917\n",
      "Epoch 116/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2866 - val_loss: 0.2916\n",
      "Epoch 117/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2867 - val_loss: 0.2916\n",
      "Epoch 118/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2867 - val_loss: 0.2916\n",
      "Epoch 119/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2866 - val_loss: 0.2919\n",
      "Epoch 120/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2866 - val_loss: 0.2916\n",
      "Epoch 121/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2869 - val_loss: 0.2919\n",
      "Epoch 122/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2860\n",
      "Epoch 00121: reducing learning rate to 1.0000000474974514e-05.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2918\n",
      "Epoch 123/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2865 - val_loss: 0.2918\n",
      "Epoch 124/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2859 - val_loss: 0.2916\n",
      "Epoch 125/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2916\n",
      "Epoch 126/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2915\n",
      "Epoch 127/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2870\n",
      "Epoch 00126: reducing learning rate to 1.0000000656873453e-06.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2868 - val_loss: 0.2915\n",
      "Epoch 128/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2915\n",
      "Epoch 129/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2914\n",
      "Epoch 130/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2858 - val_loss: 0.2914\n",
      "Epoch 131/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2914\n",
      "Epoch 132/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2914\n",
      "Epoch 133/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2865 - val_loss: 0.2913\n",
      "Epoch 134/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2865 - val_loss: 0.2913\n",
      "Epoch 135/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2863 - val_loss: 0.2913\n",
      "Epoch 136/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2913\n",
      "Epoch 137/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2856 - val_loss: 0.2913\n",
      "Epoch 138/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2863 - val_loss: 0.2913\n",
      "Epoch 139/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2862\n",
      "Epoch 00138: reducing learning rate to 1.0000001111620805e-07.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2913\n",
      "Epoch 140/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2913\n",
      "Epoch 141/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2913\n",
      "Epoch 142/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2866 - val_loss: 0.2913\n",
      "Epoch 143/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2855 - val_loss: 0.2912\n",
      "Epoch 144/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 145/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2857 - val_loss: 0.2912\n",
      "Epoch 146/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 147/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 148/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2858 - val_loss: 0.2912\n",
      "Epoch 149/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 150/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2864\n",
      "Epoch 00149: reducing learning rate to 1.000000082740371e-08.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2865 - val_loss: 0.2912\n",
      "Epoch 151/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 152/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 153/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2865 - val_loss: 0.2912\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185082/185082 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 155/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2856\n",
      "Epoch 00154: reducing learning rate to 1.000000082740371e-09.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 156/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2912\n",
      "Epoch 157/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 158/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2867 - val_loss: 0.2912\n",
      "Epoch 159/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 160/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00159: reducing learning rate to 1.000000082740371e-10.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 161/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 162/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2865 - val_loss: 0.2912\n",
      "Epoch 163/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2858 - val_loss: 0.2912\n",
      "Epoch 164/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2861 - val_loss: 0.2912\n",
      "Epoch 165/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2862\n",
      "Epoch 00164: reducing learning rate to 1.000000082740371e-11.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 166/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 167/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2865 - val_loss: 0.2912\n",
      "Epoch 168/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2869 - val_loss: 0.2912\n",
      "Epoch 169/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 170/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2861\n",
      "Epoch 00169: reducing learning rate to 1.000000082740371e-12.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2857 - val_loss: 0.2912\n",
      "Epoch 171/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2858 - val_loss: 0.2912\n",
      "Epoch 172/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 173/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 174/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2865 - val_loss: 0.2912\n",
      "Epoch 175/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2858\n",
      "Epoch 00174: reducing learning rate to 1.0000001044244145e-13.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 176/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2861 - val_loss: 0.2912\n",
      "Epoch 177/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 178/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 179/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 180/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2864\n",
      "Epoch 00179: reducing learning rate to 1.0000001179769417e-14.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 181/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2867 - val_loss: 0.2912\n",
      "Epoch 182/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 183/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 184/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2861 - val_loss: 0.2912\n",
      "Epoch 185/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2866\n",
      "Epoch 00184: reducing learning rate to 1.0000001518582595e-15.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2867 - val_loss: 0.2912\n",
      "Epoch 186/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 187/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 188/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2859 - val_loss: 0.2912\n",
      "Epoch 189/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 190/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2859\n",
      "Epoch 00189: reducing learning rate to 1.0000001095066122e-16.\n",
      "185082/185082 [==============================] - 5s - loss: 0.2860 - val_loss: 0.2912\n",
      "Epoch 191/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2862 - val_loss: 0.2912\n",
      "Epoch 192/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2870 - val_loss: 0.2912\n",
      "Epoch 193/500\n",
      "185082/185082 [==============================] - 5s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 194/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 195/500\n",
      "180000/185082 [============================>.] - ETA: 0s - loss: 0.2860\n",
      "Epoch 00194: reducing learning rate to 1.0000000830368326e-17.\n",
      "185082/185082 [==============================] - 4s - loss: 0.2856 - val_loss: 0.2912\n",
      "Epoch 196/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2868 - val_loss: 0.2912\n",
      "Epoch 197/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2864 - val_loss: 0.2912\n",
      "Epoch 198/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2863 - val_loss: 0.2912\n",
      "Epoch 199/500\n",
      "185082/185082 [==============================] - 4s - loss: 0.2862 - val_loss: 0.2912\n",
      "Fold RMSLE : 0.539630\n",
      "\n",
      "Fold 4 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185083 samples, validate on 46270 samples\n",
      "Epoch 1/500\n",
      "185083/185083 [==============================] - 9s - loss: 1.3966 - val_loss: 0.6409\n",
      "Epoch 2/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.7670 - val_loss: 0.6249\n",
      "Epoch 3/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.6217 - val_loss: 0.6242\n",
      "Epoch 4/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5532 - val_loss: 0.6165\n",
      "Epoch 5/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5167 - val_loss: 0.6052\n",
      "Epoch 6/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4911 - val_loss: 0.5768\n",
      "Epoch 7/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4738 - val_loss: 0.5537\n",
      "Epoch 8/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4604 - val_loss: 0.5349\n",
      "Epoch 9/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4505 - val_loss: 0.5167\n",
      "Epoch 10/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4382 - val_loss: 0.4998\n",
      "Epoch 11/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4301 - val_loss: 0.4849\n",
      "Epoch 12/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4269 - val_loss: 0.4651\n",
      "Epoch 13/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4191 - val_loss: 0.4544\n",
      "Epoch 14/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4108 - val_loss: 0.4372\n",
      "Epoch 15/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4073 - val_loss: 0.4299\n",
      "Epoch 16/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4045 - val_loss: 0.4155\n",
      "Epoch 17/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3973 - val_loss: 0.4053\n",
      "Epoch 18/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3952 - val_loss: 0.3966\n",
      "Epoch 19/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3914 - val_loss: 0.3917\n",
      "Epoch 20/500\n",
      "185083/185083 [==============================] - 7s - loss: 0.3850 - val_loss: 0.3856\n",
      "Epoch 21/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3817 - val_loss: 0.3853\n",
      "Epoch 22/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3795 - val_loss: 0.3736\n",
      "Epoch 23/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3787 - val_loss: 0.3685\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 5s - loss: 0.3747 - val_loss: 0.3669\n",
      "Epoch 25/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3721 - val_loss: 0.3540\n",
      "Epoch 26/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3696 - val_loss: 0.3570\n",
      "Epoch 27/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3668 - val_loss: 0.3511\n",
      "Epoch 28/500\n",
      "185083/185083 [==============================] - 9s - loss: 0.3637 - val_loss: 0.3510\n",
      "Epoch 29/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3636 - val_loss: 0.3404\n",
      "Epoch 30/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3602 - val_loss: 0.3409\n",
      "Epoch 31/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3587 - val_loss: 0.3362\n",
      "Epoch 32/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3560 - val_loss: 0.3386\n",
      "Epoch 33/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3541 - val_loss: 0.3321\n",
      "Epoch 34/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3529 - val_loss: 0.3338\n",
      "Epoch 35/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3516 - val_loss: 0.3320\n",
      "Epoch 36/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3484 - val_loss: 0.3256\n",
      "Epoch 37/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3484 - val_loss: 0.3244\n",
      "Epoch 38/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3452 - val_loss: 0.3245\n",
      "Epoch 39/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3447 - val_loss: 0.3233\n",
      "Epoch 40/500\n",
      "185083/185083 [==============================] - 8s - loss: 0.3433 - val_loss: 0.3186\n",
      "Epoch 41/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3404 - val_loss: 0.3221\n",
      "Epoch 42/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3412 - val_loss: 0.3217\n",
      "Epoch 43/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3377 - val_loss: 0.3202\n",
      "Epoch 44/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3372 - val_loss: 0.3187\n",
      "Epoch 45/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3361 - val_loss: 0.3132\n",
      "Epoch 46/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3354 - val_loss: 0.3137\n",
      "Epoch 47/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3344 - val_loss: 0.3174\n",
      "Epoch 48/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3316 - val_loss: 0.3136\n",
      "Epoch 49/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3320 - val_loss: 0.3131\n",
      "Epoch 50/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3305 - val_loss: 0.3127\n",
      "Epoch 51/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3282 - val_loss: 0.3103\n",
      "Epoch 52/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3278 - val_loss: 0.3086\n",
      "Epoch 53/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3276 - val_loss: 0.3086\n",
      "Epoch 54/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3266 - val_loss: 0.3069\n",
      "Epoch 55/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3255 - val_loss: 0.3053\n",
      "Epoch 56/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3247 - val_loss: 0.3053\n",
      "Epoch 57/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3234 - val_loss: 0.3047\n",
      "Epoch 58/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3232 - val_loss: 0.3030\n",
      "Epoch 59/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3219 - val_loss: 0.3030\n",
      "Epoch 60/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3213 - val_loss: 0.3066\n",
      "Epoch 61/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3214 - val_loss: 0.3001\n",
      "Epoch 62/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3190 - val_loss: 0.3005\n",
      "Epoch 63/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3186 - val_loss: 0.2995\n",
      "Epoch 64/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3180 - val_loss: 0.3009\n",
      "Epoch 65/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3167 - val_loss: 0.3050\n",
      "Epoch 66/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3161 - val_loss: 0.2999\n",
      "Epoch 67/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3156 - val_loss: 0.2991\n",
      "Epoch 68/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3146 - val_loss: 0.2983\n",
      "Epoch 69/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3155 - val_loss: 0.2971\n",
      "Epoch 70/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3143 - val_loss: 0.2988\n",
      "Epoch 71/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3134 - val_loss: 0.2984\n",
      "Epoch 72/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3128 - val_loss: 0.2977\n",
      "Epoch 73/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3123 - val_loss: 0.2961\n",
      "Epoch 74/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3113 - val_loss: 0.2999\n",
      "Epoch 75/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3109 - val_loss: 0.2954\n",
      "Epoch 76/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3102 - val_loss: 0.2972\n",
      "Epoch 77/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3087 - val_loss: 0.2949\n",
      "Epoch 78/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3096 - val_loss: 0.2950\n",
      "Epoch 79/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3084 - val_loss: 0.2956\n",
      "Epoch 80/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3089 - val_loss: 0.2949\n",
      "Epoch 81/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3076 - val_loss: 0.2947\n",
      "Epoch 82/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3063 - val_loss: 0.2954\n",
      "Epoch 83/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3062 - val_loss: 0.2940\n",
      "Epoch 84/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3061 - val_loss: 0.2935\n",
      "Epoch 85/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3053 - val_loss: 0.2938\n",
      "Epoch 86/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3051 - val_loss: 0.2929\n",
      "Epoch 87/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3052 - val_loss: 0.2932\n",
      "Epoch 88/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3037 - val_loss: 0.2935\n",
      "Epoch 89/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3037 - val_loss: 0.2924\n",
      "Epoch 90/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3041 - val_loss: 0.2934\n",
      "Epoch 91/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3032 - val_loss: 0.2917\n",
      "Epoch 92/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3022 - val_loss: 0.2928\n",
      "Epoch 93/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3019 - val_loss: 0.2970\n",
      "Epoch 94/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3011 - val_loss: 0.2916\n",
      "Epoch 95/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3010 - val_loss: 0.2945\n",
      "Epoch 96/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3006 - val_loss: 0.2921\n",
      "Epoch 97/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2999\n",
      "Epoch 00096: reducing learning rate to 0.00010000000474974513.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2998 - val_loss: 0.2930\n",
      "Epoch 98/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2996 - val_loss: 0.2929\n",
      "Epoch 99/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2990 - val_loss: 0.2912\n",
      "Epoch 100/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2988 - val_loss: 0.2909\n",
      "Epoch 101/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2988 - val_loss: 0.2908\n",
      "Epoch 102/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2990 - val_loss: 0.2908\n",
      "Epoch 103/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2990 - val_loss: 0.2905\n",
      "Epoch 104/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2990 - val_loss: 0.2904\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 5s - loss: 0.2988 - val_loss: 0.2903\n",
      "Epoch 106/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2905\n",
      "Epoch 107/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2908\n",
      "Epoch 108/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2986 - val_loss: 0.2904\n",
      "Epoch 109/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2906\n",
      "Epoch 110/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2985 - val_loss: 0.2905\n",
      "Epoch 111/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2979 - val_loss: 0.2899\n",
      "Epoch 112/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2902\n",
      "Epoch 113/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2901\n",
      "Epoch 114/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2982 - val_loss: 0.2898\n",
      "Epoch 115/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2986 - val_loss: 0.2901\n",
      "Epoch 116/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2989 - val_loss: 0.2901\n",
      "Epoch 117/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2984 - val_loss: 0.2898\n",
      "Epoch 118/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2898\n",
      "Epoch 119/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2898\n",
      "Epoch 120/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2987\n",
      "Epoch 00119: reducing learning rate to 1.0000000474974514e-05.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2985 - val_loss: 0.2900\n",
      "Epoch 121/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2899\n",
      "Epoch 122/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2898\n",
      "Epoch 123/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2898\n",
      "Epoch 124/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2977 - val_loss: 0.2897\n",
      "Epoch 125/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2978\n",
      "Epoch 00124: reducing learning rate to 1.0000000656873453e-06.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2897\n",
      "Epoch 126/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2986 - val_loss: 0.2897\n",
      "Epoch 127/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2977 - val_loss: 0.2897\n",
      "Epoch 128/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2897\n",
      "Epoch 129/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2897\n",
      "Epoch 130/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2986 - val_loss: 0.2896\n",
      "Epoch 131/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 132/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 133/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2971\n",
      "Epoch 00132: reducing learning rate to 1.0000001111620805e-07.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2973 - val_loss: 0.2896\n",
      "Epoch 134/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2896\n",
      "Epoch 135/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2982 - val_loss: 0.2896\n",
      "Epoch 136/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 137/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 138/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2979\n",
      "Epoch 00137: reducing learning rate to 1.000000082740371e-08.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 139/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 140/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 141/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2977 - val_loss: 0.2896\n",
      "Epoch 142/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2973 - val_loss: 0.2896\n",
      "Epoch 143/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2985\n",
      "Epoch 00142: reducing learning rate to 1.000000082740371e-09.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2982 - val_loss: 0.2896\n",
      "Epoch 144/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 145/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2977 - val_loss: 0.2896\n",
      "Epoch 146/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 147/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 148/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00147: reducing learning rate to 1.000000082740371e-10.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 149/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 150/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2970 - val_loss: 0.2896\n",
      "Epoch 151/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2896\n",
      "Epoch 152/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 153/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00152: reducing learning rate to 1.000000082740371e-11.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2973 - val_loss: 0.2896\n",
      "Epoch 154/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 155/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2985 - val_loss: 0.2896\n",
      "Epoch 156/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2975 - val_loss: 0.2896\n",
      "Epoch 157/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2896\n",
      "Epoch 158/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2978\n",
      "Epoch 00157: reducing learning rate to 1.000000082740371e-12.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 159/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 160/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2986 - val_loss: 0.2896\n",
      "Epoch 161/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2985 - val_loss: 0.2896\n",
      "Epoch 162/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2981 - val_loss: 0.2896\n",
      "Epoch 163/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2983 - val_loss: 0.2896\n",
      "Epoch 164/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 165/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2970 - val_loss: 0.2896\n",
      "Epoch 166/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2972\n",
      "Epoch 00165: reducing learning rate to 1.0000001044244145e-13.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2896\n",
      "Epoch 167/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2985 - val_loss: 0.2896\n",
      "Epoch 168/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2971 - val_loss: 0.2896\n",
      "Epoch 169/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 170/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 171/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2976\n",
      "Epoch 00170: reducing learning rate to 1.0000001179769417e-14.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2980 - val_loss: 0.2896\n",
      "Epoch 172/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2979 - val_loss: 0.2896\n",
      "Epoch 173/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2896\n",
      "Epoch 174/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 175/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 176/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2973\n",
      "Epoch 00175: reducing learning rate to 1.0000001518582595e-15.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2972 - val_loss: 0.2896\n",
      "Epoch 177/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2981 - val_loss: 0.2896\n",
      "Epoch 178/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2979 - val_loss: 0.2896\n",
      "Epoch 179/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 180/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2974 - val_loss: 0.2896\n",
      "Epoch 181/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2977\n",
      "Epoch 00180: reducing learning rate to 1.0000001095066122e-16.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 182/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2896\n",
      "Epoch 183/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2977 - val_loss: 0.2896\n",
      "Epoch 184/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2976 - val_loss: 0.2896\n",
      "Epoch 185/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2984 - val_loss: 0.2896\n",
      "Epoch 186/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.2981\n",
      "Epoch 00185: reducing learning rate to 1.0000000830368326e-17.\n",
      "185083/185083 [==============================] - 4s - loss: 0.2978 - val_loss: 0.2896\n",
      "Epoch 187/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2896\n",
      "Epoch 188/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2896\n",
      "Epoch 189/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2983 - val_loss: 0.2896\n",
      "Epoch 190/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2981 - val_loss: 0.2896\n",
      "Fold RMSLE : 0.538119\n",
      "\n",
      "Fold 5 / 5\n",
      "\n",
      "Bagging 1 / 1\n",
      "Train on 185083 samples, validate on 46270 samples\n",
      "Epoch 1/500\n",
      "185083/185083 [==============================] - 10s - loss: 1.0518 - val_loss: 0.6603\n",
      "Epoch 2/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.6314 - val_loss: 0.6338\n",
      "Epoch 3/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.5337 - val_loss: 0.6121\n",
      "Epoch 4/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4811 - val_loss: 0.5980\n",
      "Epoch 5/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.4526 - val_loss: 0.5721\n",
      "Epoch 6/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4331 - val_loss: 0.5486\n",
      "Epoch 7/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4181 - val_loss: 0.5248\n",
      "Epoch 8/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.4073 - val_loss: 0.5059\n",
      "Epoch 9/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3976 - val_loss: 0.4847\n",
      "Epoch 10/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3935 - val_loss: 0.4643\n",
      "Epoch 11/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3875 - val_loss: 0.4549\n",
      "Epoch 12/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3807 - val_loss: 0.4415\n",
      "Epoch 13/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3770 - val_loss: 0.4296\n",
      "Epoch 14/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3717 - val_loss: 0.4259\n",
      "Epoch 15/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3697 - val_loss: 0.4202\n",
      "Epoch 16/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3662 - val_loss: 0.4090\n",
      "Epoch 17/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3631 - val_loss: 0.3986\n",
      "Epoch 18/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3589 - val_loss: 0.3949\n",
      "Epoch 19/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3563 - val_loss: 0.3929\n",
      "Epoch 20/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3542 - val_loss: 0.3869\n",
      "Epoch 21/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3513 - val_loss: 0.3833\n",
      "Epoch 22/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3484 - val_loss: 0.3774\n",
      "Epoch 23/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3477 - val_loss: 0.3734\n",
      "Epoch 24/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3453 - val_loss: 0.3641\n",
      "Epoch 25/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3432 - val_loss: 0.3634\n",
      "Epoch 26/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3405 - val_loss: 0.3625\n",
      "Epoch 27/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3384 - val_loss: 0.3580\n",
      "Epoch 28/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3380 - val_loss: 0.3574\n",
      "Epoch 29/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3363 - val_loss: 0.3509\n",
      "Epoch 30/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3343 - val_loss: 0.3477\n",
      "Epoch 31/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3333 - val_loss: 0.3413\n",
      "Epoch 32/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3307 - val_loss: 0.3491\n",
      "Epoch 33/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3294 - val_loss: 0.3372\n",
      "Epoch 34/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3287 - val_loss: 0.3369\n",
      "Epoch 35/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3274 - val_loss: 0.3356\n",
      "Epoch 36/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3254 - val_loss: 0.3385\n",
      "Epoch 37/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3248 - val_loss: 0.3302\n",
      "Epoch 38/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3235 - val_loss: 0.3262\n",
      "Epoch 39/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3213 - val_loss: 0.3296\n",
      "Epoch 40/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3206 - val_loss: 0.3186\n",
      "Epoch 41/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3207 - val_loss: 0.3262\n",
      "Epoch 42/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3192 - val_loss: 0.3229\n",
      "Epoch 43/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3183 - val_loss: 0.3223\n",
      "Epoch 44/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3173 - val_loss: 0.3138\n",
      "Epoch 45/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3167 - val_loss: 0.3161\n",
      "Epoch 46/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3156 - val_loss: 0.3140\n",
      "Epoch 47/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3144 - val_loss: 0.3124\n",
      "Epoch 48/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3142 - val_loss: 0.3102\n",
      "Epoch 49/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3126 - val_loss: 0.3087\n",
      "Epoch 50/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3125 - val_loss: 0.3103\n",
      "Epoch 51/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3115 - val_loss: 0.3070\n",
      "Epoch 52/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3103 - val_loss: 0.3074\n",
      "Epoch 53/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3096 - val_loss: 0.3067\n",
      "Epoch 54/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3084 - val_loss: 0.3049\n",
      "Epoch 55/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3092 - val_loss: 0.3042\n",
      "Epoch 56/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3080 - val_loss: 0.3035\n",
      "Epoch 57/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3078 - val_loss: 0.3058\n",
      "Epoch 58/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3065 - val_loss: 0.3020\n",
      "Epoch 59/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3060 - val_loss: 0.3037\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 4s - loss: 0.3059 - val_loss: 0.3022\n",
      "Epoch 61/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3053 - val_loss: 0.3047\n",
      "Epoch 62/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3053 - val_loss: 0.2992\n",
      "Epoch 63/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3048 - val_loss: 0.3009\n",
      "Epoch 64/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3042 - val_loss: 0.2999\n",
      "Epoch 65/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3029 - val_loss: 0.2997\n",
      "Epoch 66/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3034 - val_loss: 0.3005\n",
      "Epoch 67/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3027 - val_loss: 0.2995\n",
      "Epoch 68/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3019\n",
      "Epoch 00067: reducing learning rate to 0.00010000000474974513.\n",
      "185083/185083 [==============================] - 5s - loss: 0.3019 - val_loss: 0.3003\n",
      "Epoch 69/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3021 - val_loss: 0.2989\n",
      "Epoch 70/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3022 - val_loss: 0.2988\n",
      "Epoch 71/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3011 - val_loss: 0.2981\n",
      "Epoch 72/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3009 - val_loss: 0.2976\n",
      "Epoch 73/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3009 - val_loss: 0.2976\n",
      "Epoch 74/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3012 - val_loss: 0.2972\n",
      "Epoch 75/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3010 - val_loss: 0.2969\n",
      "Epoch 76/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3008 - val_loss: 0.2965\n",
      "Epoch 77/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3011 - val_loss: 0.2967\n",
      "Epoch 78/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2965\n",
      "Epoch 79/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2961\n",
      "Epoch 80/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2959\n",
      "Epoch 81/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3006 - val_loss: 0.2960\n",
      "Epoch 82/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3000 - val_loss: 0.2962\n",
      "Epoch 83/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3008 - val_loss: 0.2959\n",
      "Epoch 84/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3008 - val_loss: 0.2960\n",
      "Epoch 85/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3001 - val_loss: 0.2959\n",
      "Epoch 86/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3002\n",
      "Epoch 00085: reducing learning rate to 1.0000000474974514e-05.\n",
      "185083/185083 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2959\n",
      "Epoch 87/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3006 - val_loss: 0.2958\n",
      "Epoch 88/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3000 - val_loss: 0.2956\n",
      "Epoch 89/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2955\n",
      "Epoch 90/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3004 - val_loss: 0.2954\n",
      "Epoch 91/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2954\n",
      "Epoch 92/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2953\n",
      "Epoch 93/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2953\n",
      "Epoch 94/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3000 - val_loss: 0.2952\n",
      "Epoch 95/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2952\n",
      "Epoch 96/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2952\n",
      "Epoch 97/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3007 - val_loss: 0.2952\n",
      "Epoch 98/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3008 - val_loss: 0.2952\n",
      "Epoch 99/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3005 - val_loss: 0.2952\n",
      "Epoch 100/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3001\n",
      "Epoch 00099: reducing learning rate to 1.0000000656873453e-06.\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 101/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 102/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2951\n",
      "Epoch 103/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 104/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2951\n",
      "Epoch 105/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 106/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3005 - val_loss: 0.2951\n",
      "Epoch 107/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 108/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3000\n",
      "Epoch 00107: reducing learning rate to 1.0000001111620805e-07.\n",
      "185083/185083 [==============================] - 5s - loss: 0.2994 - val_loss: 0.2951\n",
      "Epoch 109/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3007 - val_loss: 0.2951\n",
      "Epoch 110/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 111/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2995 - val_loss: 0.2951\n",
      "Epoch 112/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3002 - val_loss: 0.2951\n",
      "Epoch 113/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3003\n",
      "Epoch 00112: reducing learning rate to 1.000000082740371e-08.\n",
      "185083/185083 [==============================] - 4s - loss: 0.3000 - val_loss: 0.2951\n",
      "Epoch 114/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3007 - val_loss: 0.2951\n",
      "Epoch 115/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2996 - val_loss: 0.2951\n",
      "Epoch 116/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 117/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.2997 - val_loss: 0.2951\n",
      "Epoch 118/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3000\n",
      "Epoch 00117: reducing learning rate to 1.000000082740371e-09.\n",
      "185083/185083 [==============================] - 5s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 119/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2998 - val_loss: 0.2951\n",
      "Epoch 120/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2993 - val_loss: 0.2951\n",
      "Epoch 121/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3004 - val_loss: 0.2951\n",
      "Epoch 122/500\n",
      "185083/185083 [==============================] - 5s - loss: 0.3004 - val_loss: 0.2951\n",
      "Epoch 123/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3006\n",
      "Epoch 00122: reducing learning rate to 1.000000082740371e-10.\n",
      "185083/185083 [==============================] - 5s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 124/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 125/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 126/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2998 - val_loss: 0.2951\n",
      "Epoch 127/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 128/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3001\n",
      "Epoch 00127: reducing learning rate to 1.000000082740371e-11.\n",
      "185083/185083 [==============================] - 4s - loss: 0.3002 - val_loss: 0.2951\n",
      "Epoch 129/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3005 - val_loss: 0.2951\n",
      "Epoch 130/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3002 - val_loss: 0.2951\n",
      "Epoch 131/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185083/185083 [==============================] - 4s - loss: 0.3001 - val_loss: 0.2951\n",
      "Epoch 132/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 133/500\n",
      "180000/185083 [============================>.] - ETA: 0s - loss: 0.3003\n",
      "Epoch 00132: reducing learning rate to 1.000000082740371e-12.\n",
      "185083/185083 [==============================] - 4s - loss: 0.3003 - val_loss: 0.2951\n",
      "Epoch 134/500\n",
      "185083/185083 [==============================] - 4s - loss: 0.2997 - val_loss: 0.2951\n",
      "Fold RMSLE : 0.543213\n",
      "CV RMSLE : 0.538231\n"
     ]
    }
   ],
   "source": [
    "pred_tr2, pred_te2 = model_run('train2')\n",
    "pred_tr2.to_csv(path+'/'+ver+'_pred_tr2.csv', index=False)\n",
    "pred_te2.to_csv(path+'/'+ver+'_pred_te2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('C:/Users/Kohei/Documents/Kaggle/Recruit/00_input/sample_submission.csv')\n",
    "sub['air_store_id'] = sub.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sub['visit_date']   = sub.id.map(lambda x: x.split('_')[2])\n",
    "sub['visit_date'] = pd.to_datetime(sub['visit_date'])\n",
    "\n",
    "pred_te = pd.concat([pred_te1,pred_te2])\n",
    "pred_te['visit_date'] = pd.to_datetime(pred_te['visit_date'])\n",
    "\n",
    "# submission\n",
    "sub.loc[:,'visitors'] = sub.merge(pred_te, on=['air_store_id','visit_date'], how='left')['pred'].values\n",
    "sub['visitors'] = np.expm1(sub['visitors'])\n",
    "sub_file = path+'/'+ver+'_sub.csv'\n",
    "sub[['id','visitors']].to_csv(sub_file, index=False)\n",
    "\n",
    "# adjusted subumission\n",
    "sub['pred'] = sub['visitors']\n",
    "sub.loc[sub.visit_date==date(2017,4,25),'visitors']*=1.10 # Tuesday-judgmental\n",
    "\n",
    "sub.loc[sub.visit_date==date(2017,4,27),'visitors']*=1.10 # Thursday-judgmental\n",
    "\n",
    "sub.loc[sub.visit_date==date(2017,4,30),'visitors']*=1.35 # Sunday-judgmental-\n",
    "sub.loc[sub.visit_date==date(2017,5,1),'visitors'] *=1.20 # Monday-judgmental-should be high because in-between\n",
    "sub.loc[sub.visit_date==date(2017,5,2),'visitors'] *=1.10 # Tuesday-judgmental-should be high because in-between\n",
    "sub.loc[sub.visit_date==date(2017,5,3),'visitors'] *=1.10 # Day1\n",
    "sub.loc[sub.visit_date==date(2017,5,4),'visitors'] *=1.10 # Day2\n",
    "sub.loc[sub.visit_date==date(2017,5,5),'visitors'] *=1.05 # Day3\n",
    "sub.loc[sub.visit_date==date(2017,5,6),'visitors'] *=1.00 # Saturday\n",
    "sub.loc[sub.visit_date==date(2017,5,7),'visitors'] *=0.95 # Sunday\n",
    "sub.loc[sub.visit_date==date(2017,5,8),'visitors'] *=0.90 # Monday\n",
    "sub.loc[sub.visit_date==date(2017,5,9),'visitors'] *=0.90 # Tuesday\n",
    "sub_file = path+'/'+ver+'_sub_adjusted.csv'\n",
    "sub[['id','visitors']].to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAEzCAYAAACIWRrLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4VGX2wPHvTSgJHUKXqnQChCIt9A6KotJce1mxrZR1\nXfipu9jZtaG7uhZEsaKkgAoGaREBEUgIvQpIb5Hek7y/P06GhBDITObO3JnkfJ4nT8hk7r1vksvM\nPfc97zmWMQallFJKKaWUUgVLiNMDUEoppZRSSillPw32lFJKKaWUUqoA0mBPKaWUUkoppQogDfaU\nUkoppZRSqgDSYE8ppZRSSimlCiAN9pRSSimllFKqAMoz2LMsK8yyrGWWZa2yLGudZVnPZT5ewbKs\nOZZlbcn8XP4K2/ezLGuTZVlbLcsaa/cPoJRSSimllFLqclZeffYsy7KAksaYk5ZlFQUWASOBW4E/\njDETMoO48saYv+fYNhTYDPQGdgPLgduNMevt/1GUUkoppZRSSrnkObNnxMnML4tmfhjgZmBK5uNT\ngEG5bN4W2GqM2WaMOQ9MzdxOKaWUUkoppZQPubVmz7KsUMuyUoCDwBxjzK9AFWPMvsyn7Aeq5LLp\nNcCubF/vznxMKaWUUkoppZQPFXHnScaYdCDKsqxyQLxlWZE5vm8sy7p6PmgeLMt6CHgIIDw8vHXN\nmjW92Z0KQBkZGYSEaE0g5R49X5Qn9HxRntDzRXlCzxflKbvOmc2bNx82xlTyZh9uBXsuxpijlmUt\nAPoBByzLqmaM2WdZVjVk1i+nPUD2qK1G5mO57fsD4AOANm3amBUrVngyNBUEEhMT6datm9PDUEFC\nzxflCT1flCf0fFGe0PNFecquc8ayrN+93Yc71TgrZc7oYVlWOFJsZSPwLXBP5tPuAWbksvlyoL5l\nWXUtyyoGDM/cTimllFJKKaWUD7kzs1cNmJJZWTME+MYY871lWb8A31iW9QDwOzAUwLKs6sAkY8wA\nY0yaZVmPA7OBUGCyMWadT34SpZRSSimllFIX5RnsGWNWAy1zeTwV6JnL43uBAdm+ngXM8m6YSiml\nlFJKKaU84dGaPSdduHCB3bt3c/bsWaeHonIICwujRo0aFC1a1OmhKKWUUkoppTIFTbC3e/duSpcu\nTZ06dZA+7yoQGGNITU1l9+7d1K1b1+nhKKWUUkoppTIFTR3Zs2fPEhERoYFegLEsi4iICJ1xVUop\npZRSKsAETbAHaKAXoPTvopRSSimlVOAJqmDPSbt27aJ79+40adKEpk2b8tZbbwHwxx9/0Lt3b+rX\nr0/v3r05cuQIAKmpqXTv3p1SpUrx+OOPX7Kv8+fP89BDD9GgQQMaNWpEbGys338epZRSSimlVMGm\nwZ6bihQpwuuvv8769etZunQp77zzDuvXr2fChAn07NmTLVu20LNnTyZMmABI0ZIXXniB11577bJ9\nvfTSS1SuXJnNmzezfv16unbt6u8fRymllFIBaNYsuOEGSEtzeiRKqYJAgz03VatWjVatWgFQunRp\nGjduzJ49e5gxYwb33CO95e+55x6mT58OQMmSJenUqRNhYWGX7Wvy5MmMGzcOgJCQECpWrOinn0Ip\n+23cCM2awQsvOD0SpZQKfu+/LwHfsmVOj0QpVRBosJcPO3bsYOXKlbRr144DBw5QrVo1AKpWrcqB\nAweuuu3Ro0cBePbZZ2nVqhVDhgzJcxulAlViInToAGvXwjvvQHq60yNSSqngdf48zJsn//7hB2fH\nopQqGIKm9UJ2o0ZBSoq9+4yKgokT837eyZMnue2225g4cSJlypS55HuWZeVZrCQtLY3du3fTsWNH\n3njjDd544w2efPJJPvvsM2+Gr5TfffYZPPAA1K8PTz8Nf/sb/PQT9Ojh9MiUUio4LV4Mp05BeLgE\ne5oxoZTyls7seeDChQvcdttt3HHHHdx6660AVKlShX379gGwb98+KleufNV9REREUKJEiYvbDxky\nhOTkZN8OXCkbGQPjx8Pdd0PnznJx8uijULIkfP2106NTSqnglZAARYvCyJGQlAT79zs9IqVUsAvK\nmT13ZuDsZozhgQceoHHjxowZM+bi4zfddBNTpkxh7NixTJkyhZtvvvmq+7Esi4EDB5KYmEiPHj2Y\nN28eTZo08fXwlbLFuXPw5z/LrN6998rakmLF5Hs33QQxMfDf/8rFilJKKc8kJECnTjB0KEyYALNn\nQ2ZZAKWUyhed2XPT4sWL+eyzz5g/fz5RUVFERUUxa9Ysxo4dy5w5c6hfvz5z585l7NixF7epU6cO\nY8aM4ZNPPqFGjRqsX78egH/961+MHz+e5s2b89lnn/H666879WMp5bYjR6BvXwn0XnwRJk/OCvQA\nhg2DP/7IWm+ilFLKfXv3wurV0K+fLC2pWlXX7SmlvBeUM3tO6NSpE8aYXL837wpXtzt27Mj18dq1\na7Nw4UK7hqaUz23bBgMGwPbt8MUX8Kc/Xf6cfv2gbFmYOlX+rZRSyn2zZ8vnfv3AsuTzjBnSgqGI\nXq0ppfJJZ/aUUle1dCm0bw+HDsHcubkHegDFi8OgQRAfL+meSiml3JeQANWqSSsbgP79JaNCWzAo\npbyhwZ5S6opiYqB7dyhTBn75RQqyXM3w4XD8uFy0KKWUck9aGsyZkzWrB9C7N4SEaCqnUso7Guwp\npS5jDLz6KgwZAq1aSaDXoEHe2/XsCRERWpVTKaU8sXy5zOJlT4EvX176mM6a5dy4lFLBT4M9pdQl\n0tLgkUfgqaekIty8eVCpknvbFi0Kt90G334Lp0/7dpxKKVVQJCTILF6vXpc+PmAAJCdrCwalVP5p\nsKeUuujECRg4UFoqjB0LX30FYWGe7WPYMGkKPHOmb8aolFIFTUICtGsHFSpc+nj//vLZVbxFKaU8\npcGeUgqA3bulv9OcOfDBB/DKK3Kn2VNdu0KVKlKVUyml1NUdPixpnLlVMdYWDCovq1bJebJhg9Mj\nUYFKgz03pKamXuytV7VqVa655pqLX58/f97t/UyePJn9XuZipKen0zmvKhnAfffdx6ZNm/J1jEmT\nJjFq1Kh8bauCU0qK3FXevl3Wh/z5z/nfV2iorPWbNUuKtSillLqyOXNknXRuwZ6rBcOPP0qKvVI5\nTZ4sAd+QIbp8QuVOgz03REREkJKSQkpKCg8//DCjR4+++HWx7F2l8+BtsJeWlkZoaCg///xzns/9\n+OOPadiwYb6PpQqPWbNkRi80FBYvhj59vN/n8OFw9qys3VNKKXVlCQlS2Kp169y/ry0Y1JUYA9On\nQ/36sH49/OUvTo9IBSIN9rw0ZcoU2rZtS1RUFI8++igZGRmkpaVx11130axZMyIjI3n77bf5+uuv\nSUlJYdiwYbnOCGZkZDBmzBgiIyNp1qwZMTExAMydO5du3bpx44030qxZM9LS0ihXrhwgs3wPP/ww\njRo1ok+fPvTr14/p06cD0gQ+JSXl4vPHjh1LixYt6NChAwcPHgRgxowZtGvXjpYtW9KnT5+Lj6vC\n4913ZY1egwbST8/V38lbHTpAjRpalVMppa4mI0PW4/XpIzfccuNqwaBVOVVOycmwcyf83//Jx+TJ\n8OmnTo9KBRoN9rywdu1a4uPjWbJkycXAaurUqSQlJXH48GHWrFnD2rVrufvuuy8Gea6gL+eM4LRp\n09iwYQOrVq1izpw5jB49+mLwtWLFCt5991025EjInjZtGnv27GH9+vV88skn/PLLL7mO89ixY3Tt\n2pVVq1bRoUMHJk+eDECXLl1YunQpK1eu5NZbb+X111/3wW/JfmfPyl2sQYNg1y6nRxOcMjLgr3+F\nxx6Tam8LF0L16vbtPyRECrXMni13pJVSSl1u1So4cCD3FE6X8uWhY0ddt6cuFx8vNwkGDoTx46FL\nF6mmrev3VHZFnB5AviSNgiMp9u6zfBS0nujRJnPnzmX58uW0adMGgDNnzlCzZk369u3Lpk2beOKJ\nJ7jhhhvo40Ze3KJFi7j99tsJDQ2latWqdOrUiRUrVlCsWDE6dOhArVq1ct1m6NChhISEUL16dbp2\n7ZrrvsPDw+mfWdKrdevWF9NAd+7cydChQ9m/fz/nzp2jgTuN1ALAnDmwdausL5s3TwqJPPLIle+K\n+psxEjw1a3Z5ZbVAce+98Nln8PjjMHGib353w4bB66/Lm9H999u/f6WCxa5dctOjeXOnR6ICTUKC\nfM7rMqF/f3j6aWnBULWq78elgkN8vAR4ERHy9VdfSbGWIUMk7bdECWfHpwKDzux5wRjD/ffff3H9\n3qZNm3j22WeJiIhg9erVdO7cmXfeeYcRI0Zctu2SJUsuFnmZlUduRsmSJb0aZ/ZZxNDQUNIyV3k/\n9thjjB49mjVr1vDuu+9y9uxZr47jL3FxUK6c5Kd37Cg56p06wZo1To8MliyB6Gjo1g2efNLp0eTu\n998l0Bs9Gt5+23dBcps2cO21WpVTqTvvhM6dITXV6ZGoQJOQAC1b5h3AaQsGldOmTXIddMstWY9V\nrw6ff67r99Sl8pzZsyyrJvApUAUwwAfGmLcsy/oacFUAKQccNcZE5bL9DuAEkA6kGWPaeD1qD2fg\nfKVXr14MHjyYkSNHUrFiRVJTUzl16hTh4eGEhYUxZMgQ6tevz4MPPghA6dKlOXHiBAAdO3YkJSVr\ndvLkyZN88skn3HnnnRw6dIjFixfz1ltvsXr16isePzo6mqlTp3LnnXeyf/9+Fi5cyP0eTKEcO3aM\na665BmMMU6ZMyedvwb8uXJCiH651ZgkJ8OWXMGoUtGoFf/87PPOM573hvLV1q/Sli42FatVkoX18\nPLz3HnhQw8cvYmPl86OPSqU3X7Esmd3797/h4EGoXNl3x1IqUG3eLDP9ABMmwKuvOjseFTiOHZMb\nhH/7W97Pzd6C4Z57fD82Ffji4+XzoEGXPt6nj6zfe+klufF8111+H5oKMO7M7KUBfzXGNAHaA49Z\nltXEGDPMGBOVGeDFAnFX2Uf3zOd6H+gFkGbNmvHPf/6TXr160bx5c/r06cOBAwfYtWsXXbp0ISoq\nivvuu4+XX34ZkHYIDz74YK4FWgYPHkyjRo1o3rw5vXr14o033qByHlfHQ4cOpXLlyjRu3Jh7772X\nli1bUrZsWbfHP378eG655Rauv/56qlSp4vkvwAELF8Iff8Ctt8rXlgV33CH56XfcIS9uzZtDYqJ/\nxnP4MIwcCU2aSOD5/POwZQv8859w9KikmQaamBi5cKhXz/fHGj4c0tOzAkylCpuPPpLZ8xtugP/8\nR/pZKgUwf760U7jaej0XbcGgcoqPlwyamjUv/55r/d7DD+v6PYWkInryAcwAemf72gJ2AfWv8Pwd\nQEVPjtG6dWuT0/r16y97TBlz4sQJY4wxBw8eNHXr1jUHDx50ZBzu/H0WLFjg9XEefdSYEiWMOX06\n9+/PmWPMtdcaA8Y88IAxqaleHzJXZ84Y869/GVO2rDEhIcaMGGHMvn1Z3z971pgyZYy57z7fHD+/\ndu6U381LL/nneBkZxjRqZEzXrp5va8f5ogqPQDxfzp83pkoVY266yZgdO4wpVsyYBx90elTKmMA4\nXx56yJjSpeU8ccc338jr96JFvh2XulwgnC/Z7d6d93v57t3GVKxoTGSkMadO+W9sSth1zgArjIex\nWs4Pj9bsWZZVB2gJ/Jrt4c7AAWPMlivFk8Bcy7KSLMt6yJPjqbz179+fFi1a0LVrV5577jkqVark\n9JB8JiND7mQNGADh4bk/p1cvWbv397/DJ59A48ZS/l/uO9gzhs8/h4YN5RidO8vx3nvv0jUXxYvD\nTTdJ/5sLF+w5th3iMuffBw/2z/EsS2b3Fi6EvXv9c0ylAsXMmVJp8cEHoXZtSZ2ePBk2bnR6ZMpp\nxkg2SK9eULSoe9v07i2zxFqVU2V22bpkvV5O11wj1ytr18ITT/hnXCowuV2N07KsUki65ihjzPFs\n37od+Ooqm3YyxuyxLKsyMMeyrI3GmIW57P8h4CGAKlWqkJgjD69s2bIX17upLDmLuzj1Ozp79uxl\nf7OcTp48medzrmbdujLs29eKRo3Wk5h49Z6A/fpBvXolef31hgwfXoY330xl1KjNVK16Lt/HX7my\nHP/733Vs2VKa+vVP8MYbv9Gy5VEOHpQ1aTk1ahTB55834803V9O27R/5Pq6dJk1qybXXhrJ37wq/\nBV9165bAmLa8/PIWBg/e4/Z23p4vqnAJxPPl3/+OJCKiNCVKLCUx0dC1a1Hef78dI0Yc4bnn1jk9\nvELN6fPl999LsHNnWwYP3kRi4j63t2vSJIpvvgmlV68kH45O5eT0+ZLT5MktqFWrGAcOLOfAgSs/\nr3hxuOOOunz0UW2qVNlA795XebKyVUCdM+5M/wFFgdnAmByPFwEOADXc3M944Mm8nqdpnMHHH2mc\nTz4paVDHjrm/TVqaMW++aUzJkvLx5pvymCfWrTPmhhskZaJWLWM+/9yY9PS8tztzRlJ07r/fs+P5\niivt44UX/H/sFi2Mad/es20CLW1GBbZAO19275YU77FjL318/Hj5f7hsmTPjUsLp8+WNN+Q82LHD\ns+1eekm2y75sQPme0+dLdqmpxoSGGjNunHvPv3DBmM6d5Rpowwbfjk1lCao0TsuyLOAjYIMx5o0c\n3+4FbDTG5Lrk3LKskpZllXb9G+gDrPU4IlWFnjGSgtirF5Qp4/52oaFSqXPdOujaVdoNtG8vjWzz\nsn8/jBgh/fIWLZKqkps2SSGYEDcSoMPCAiuV098pnNkNGwZLl8KOHf4/tlJOmDJF0r5zFkgeMwYq\nVoRx45wZlwoMCQmyzKB2bc+20xYM6vvvpfDZ1VI4sytSRPrvhYdL/73Tp307PhV43FmzFw3cBfSw\nLCsl82NA5veGkyOF07Ks6pZluXILqwCLLMtaBSwDZhpjEmwauypEVq+GbduyqnB6qnZteYGcOhV2\n7pTWCGPHwpkzlz/31Cl47jmpVjl5svSq2bpVymN72tJhyBCpHjp/fv7Gbadp0yAyEho18v+xhw2T\nz9984/9jK+VvGRny2tG1K9Svf+n3SpeW9jDz5sHcuc6MTznr9Gn46Sf3qnDmlL0Fgyqc4uOhRg2p\nxOmu7Ov3Ro703dhUYMoz2DPGLDLGWMaY5iaz1YIxZlbm9+41xryX4/l7jTEDMv+9zRjTIvOjqTHm\nJd/8GKqgi4uT2bSbbsr/Plx93zZsgHvvhX/9S2btXBdc6ekwaZJcnI0fL3dQN2yAiRPlTnx+9O0r\nF3fTpuV/3HbYt09mJ52Y1QNprt62rRTLUaqgW7gQfvsNHngg9+8//DDUqiU3nOwqHqWCx08/wblz\n+Qv2LEvem2bP1hYMhdGpUzIrPGiQ531y+/aVjIJJkyTwU4WHR9U4C6vU1FSioqKIioqiatWqXHPN\nNRe/ztkv72omT57M/v37vRrLM888w8SJ0lT+6aefZsGCBR7vY9u2bUydOtXj7e68806mu0pA+Vlc\nnPSMsaPYaIUK8mK3YIEEkL17w+23yx3TP/8Z6tSBxYslQPO2F11YmDSAj493NpUzLk4uKocMcW4M\nw4ZBcrL0IVSqIJs0CcqWhdtuy/37xYtLT86kJO1BWRglJEhKXZcu+du+f3/p4/rrr3k/VxUss2fD\n2bPup3Dm9Pzz0KmT3HDSqsCFhwZ7boiIiCAlJYWUlBQefvhhRo8effHrYsWKub0fO4K97F566SW6\nd+/u8Xb5DfacsnmzpB7kN4XzSrp1k/TQp5+WRuNnz8rnxYuhY0f7jjN0qKRy5iMut01MjKwPadLE\nuTEMHSqfdXZPFWRHjkgA96c/QYkSV37enXfK/8enn9YZmsImIUHefzxdFuCiLRgKr/h4uWGd3xsF\n2dfvDR2a+1IWVfBosOelKVOm0LZtW6Kionj00UfJyMggLS2Nu+66i2bNmhEZGcnbb7/N119/TUpK\nCsOGDct1RvD48eP06NGDVq1a0bx5c77//vuL33v++edp0KABnTp1Yku2aZHsM201atTg6NGjACxd\nupRevXoBMH/+fFq0aEFUVBStWrXi1KlTjB07lgULFhAVFcXbb79NWloaY8aMoW3btjRv3pxJkyYB\nkJGRwaOPPkqjRo3o3bs3hw8f9unv8kpchUUGDbJ/32Fh8OKLEoytXy934j1NjciL06mcBw5IWpmT\ns3ogaww6dZJ1k0oVVF9+KTeOHnzw6s8LDYWXX5abWZ984pehqQCwbZv8zfOTwulSrhx06KDBXmFz\n4YLUHhg4UIK2/KpRAz77THoE6/q9wkGDPS+sXbuW+Ph4lixZQkpKCmlpaUydOpWkpCQOHz7MmjVr\nWLt2LXfffffFIM8V9OWcEQwPD2f69OkkJyczd+5cRo8eDcCyZcuIjY1l1apVzJw5k2XLlnk0xldf\nfZUPPviAlJQUFi5cSFhYGBMmTKB79+6kpKTwxBNP8MEHH1C5cmWWLVvG8uXLeeedd9i5cycxMTFs\n376d9evX8/HHH7NkyRLbfneeiIuT9V41a/ruGKVLu9/Y1lNOp3LGx0vBCKfW62U3fLhURl2rNXlV\nAfXRR5IS3qpV3s+96SapDjx+vN5hLyxcVTS9CfZAUjmTk6VqtCocEhMlfdeOLKd+/WT93ocfwhdf\neL8/FdiCN9jr1i3rduiFC/K1a8Xp6dPytStf7Ngx+do1RXT4sHz93XfydT5fLefOncvy5ctp06YN\nUVFR/PTTT/z222/Uq1ePTZs28cQTTzB79mzKli2b576MMYwdO5bmzZvTp08fdu3axeHDh1m4cCG3\n3XYb4eHhlC1bloEDB3o0xujoaEaOHMl//vMfjh8/Tmho6GXP+fHHH/n444+JioqiXbt2HD16lC1b\ntrBw4UJuv/12QkJCqFGjBt26dfPo2HbYuROWL7c/hdPfhgyB1FR5sfa3adOgYUOpxOm0wYNlnaSm\ncqqCKDkZVq68cmGWnCwLJkyAPXvgnXd8OzYVGBISZF14ziqtntIWDIVPfDyULClpvHZwrd8bMULa\nSqmCK3iDvQBgjOH++++/uH5v06ZNPPvss0RERLB69Wo6d+7MO++8w4gRIy7bdsmSJReLvMyaNYtP\nP/2UY8eOkZycTEpKChUrVuTs2bNuj6VIkSJkZGQAXLLdM888wwcffMDJkydp3779JWmg2X+Od999\n9+LPsX37dnr27JmP34j9XPVg8rsYOVD07QulSvk/lfPgQQkwhwyxPz01P6pUge7dJdjTKoSqoPno\nIym+cscd7m/TtavcZX/5Zblrrwqu8+el5Ua/ft6/HkdFQbVqMGtW3s9VwS8jQ66H+vWT9XZ2cK3f\nCwuTawTNLii4gjfYS0yU+vkg+XeJibLiHWRVfGJiVnOvsmXla9f0UMWK8rVrlqxq1XwNoVevXnzz\nzTcX17Klpqayc+dODh06hDGGIUOG8Pzzz5OcnAxA6dKlOXHiBAAdO3a8GFwNGDCAY8eOUblyZYoU\nKcKcOXPYs2cPAF26dCE+Pp6zZ89y/PjxS9byZVenTh2SkpIAiM1W3u23336jefPmjBs3jlatWrFp\n06ZLxgHQt29f3n33XdIyqwRs2rSJM2fO0KVLF77++msyMjLYs2cPP/30U75+T96Ii5MZqQYN/H5o\nW4WHZ6Vy+rMYw/TpgZPC6TJsmFTkXLnS6ZEoZZ8zZyQd6rbboHx5z7Z9+WUp7PLaa74ZmwoMixdL\n6XxvUzhBgsV+/eDHH7XAT2GwbJm0ULL7xreu3yscgjfYCwDNmjXjn//8J7169bqYfnngwAF27dpF\nly5diIqK4r777uPll18G4L777uPBBx/MtUDLXXfdxZIlS2jWrBlTp06lfmaOR9u2bbnlllto3rw5\nN9xwA23btr1kOyvz9uD48eN59NFHuf766y9ZD/jaa68RGRlJ8+bNKVWqFH369KFly5akp6fTokUL\n3n77bUaMGEH9+vWJiooiMjKSRx55hLS0NAYPHkytWrVo0qQJ9913Hx06dPDlr/MyBw/Czz8Hfwqn\ny5AhkkHsz1TOadMkXah5c/8dMy+33ip3FDWVUxUksbGyYiCvwiy5adlS1rO++aauwSrIEhLkta9H\nD3v2py0YCo+4ODl3brjB/n337y89Pz/8UApMqQLIGBNwH61btzY5rV+//rLHCrt+/fqZhQsXOj0M\nY4x7f58FCxZ4tM8PPzQGjElJyeegAszp08aUKmXMn//sn+MdOmRMaKgx48b553ie6N/fmNq1jcnI\nuPJzPD1fVOHm9PnSrZsx115rTHp6/rbfssWYIkWMeewxe8elcufE+dK8uZwndjlyRF7jn37avn2q\n3Dn5+pKRYUy9esb06eO7Y1y4YEx0tFyjbNzou+MUJnadM8AK42VcpTN7Qeqee+4hLS3N77Nt/hQX\nB9deG1izUt4ID4cbb/RfKuf06ZCe7nzLhdwMHw6//653pFXBsHWrzNjff78UIMqPevVkVvD996U8\nvypY9u6Vvq52pHC6aAuGwmHdOnmN8WXtgiJFpC1S8eLaf68g0mAvSE2ZMoU5c+ZQxJtmKwHs2DGY\nO1dS/gKhsIhd/JnKGRMjwXJUlO+P5ambb4ZixbTnnioYJk+WIM+1jDy/nn1WlqD/4x+2DEsFELta\nLuSkLRgKvvh4uQ66+WbfHse1fm/1ahg1yrfHUv6lwZ4KSDNnSkeNgrJez6V/fymd7OuqnH/8IVXf\nAqUKZ05ly8KAAfDNNzL7qFSwSkuTLkD9+8M113i3r+rVpUjCl1/CqlW2DE8FiIQEqQVnd6bKgAFZ\n+1cFU3y89OOsVs33x+rfH556Cj74QII+VTAEVbBntFZ7QPLF3yUuTl7Y2rWzfdeOcqVyxsX5NpVz\n+nTZfyCmcLoMGybVxRYtcnokSuVfQoKcx+721svLU0/JzZCnn7Znf8p5aWkwZ449LRdyatFC3is1\nlbNg2rFDKlf788a3q8hUZiF5VQAETbAXFhZGamqqBnwBxhhDamoqYWFhtu3z9Gl547rllvyvfwlk\nrlROX3ayiImRxr2tWvnuGN4aOFC6pGhVThXMJk2CypXlJo4dypeXyngzZ0o1YhX8li+X1hp2p3CC\ntmAo6Jwte3VTAAAgAElEQVToNXzttdJ7b+1a/x1T+VbQLPiqUaMGu3fv5tChQ04PReUQFhZGjRo1\nbNvfjz9KwHfbbbbtMqD07y9BzrRp4Ive9UeOyHrHUaMCM4XTpWRJuUCOiYG335YF4koFk/374fvv\nYcwYWWtnl7/8Bd56C8aNk4AvkP8fq7wlJMiNy169fLP//v3h44+l4FV0tG+OoZwRHw/NmsF11/nv\nmKGh0KSJBnsFSdBcXhUtWpS6des6PQzlB3FxUKECdOni9Eh8o0SJrFTO//7X/iDn229lvWMgNVK/\nkuHDZd3eggXQu7fTo1HKM1OmyJpTu1I4XUqUgH/+Ex5+WGb47Jo1VM5ISIC2bSEiwjf7791bLtB/\n+EGDvYLk4EFZ5vDMM/4/dtOmMH++/4+rfKMAJsmpYHb+PHz3nVSdKsgzPUOGwKFDsHCh/fueNg1q\n1YLrr7d/33br3x9Kl9aqnCr4GCNVODt1goYN7d///fdLO4Zx47SIUTA7fFjSOH2RwuniasEwa5bv\njqH879tvISPDvymcLpGRsGcPHD3q/2Mr+2mwpwJKYqK8uBS0Kpw5DRiQlcppp2PHJA128ODgSP0K\nC4NBg2SW8/x5p0ejlPsWLYLNm+2f1XMpWhRefFFSqb76yjfHUL43Z47cGPBlsAfynrJypbZgKEji\n42XtfYsW/j92ZKR8XrfO/8dW9tNgTwWUuDgoVcp3axsCRYkScMMN8vPaedfelcIZyFU4cxo2TAL8\nH390eiRKuW/SJJmV9uX/tSFDoGVL6b937pzvjqN8JyFBliW0aePb4/Tvn3U8FfyOH5e197fc4syN\nW1ewp+v2CgYN9lTASE+XylM33CAzPgXdkCGSk29nKmdMjDRGbdvWvn36Wu/eUoFQq3KqYHHsmMzK\n3367FBrylZAQeOUVKb/+wQe+O47yjYwMaabep4+sqfMlbcFQsPzwg2S7OJHCCVCzptzM0mCvYNBg\nTwWMX36BAwcKfgqni92pnMePy4XF4MHB1bKiWDH5m0+fDmfOOD0apfL21Vdyrrr6UflSnz7QrRu8\n8AKcOOH74yn7rFol72m+TuEEbcFQ0MTHQ6VK0LGjM8e3LCnSosFewRBEl4SqoIuLg+LFs9JRCrqS\nJe1N5fzuO0n1CqYUTpfhw+HkSb0rrYLDRx9JOXRfp+aBXHS98ooUdJo40ffHU/ZxpVT26eOf4/Xv\nLynxv/7qn+Mp3zh3TortDBrk+xnhq4mM1GCvoNBgTwUEYyTo6dNHUgcKiyFD5M6vHc2TY2Lgmmug\nfXvv9+Vv3brJXUytyqkC3apVsGKFFGbx11qa9u0lnevVV6W6owoOCQkQFSXplf7gasGgVTmD27x5\nMovvVAqnS2SkvN4cPOjsOJT3NNhTASE5GX7/vfCkcLoMGADh4d6ncp44IbNit90WXCmcLkWKSOD7\n/fcyw6dUoProI0k9vvNO/x73xRfh1CmZ5VOB79gxWLLEPymcLuXKSdqfZkgEt/h4uendo4ez49Ai\nLQVHEF4WqoIoLk7uSA4c6PRI/MuVyhkb610q58yZkvoRDI3Ur2TYMFkH9d13To9EqdydPQuffy53\n3H3VIPtKmjSBe+6Bd96BnTv9e2zlufnzZe2cP4M9kFRObcEQvNLTYcYMuS4oXtzZsWiwV3DkGexZ\nllXTsqwFlmWttyxrnWVZIzMfH29Z1h7LslIyPwZcYft+lmVtsixrq2VZY+3+AVTBEBcnqXz+voAK\nBK5UzkWL8r+PadMkVSg62r5x+VunTlC9ulblVIFr+nQ4csR3vfXyMn68pLw/95wzx1fuS0iQ2ZkO\nHfx7XG3BENwWL5b1uU6ncAJUrgwVK2qwVxC4M7OXBvzVGNMEaA88ZllWk8zvvWmMicr8uCxL3LKs\nUOAdoD/QBLg927ZKAbBhA2zcWPhSOF1uuMG7VM6TJ2WNxq23BmcKp0tICAwdKilIR486PRqlLjdp\nEtSuDT17OnP8WrXgscfgk0/kdVMFJmMk2OrZU1J+/UlbMAS3+PjAKVSnFTkLjjwvDY0x+4wxyZn/\nPgFsAK5xc/9tga3GmG3GmPPAVODm/A5WFUxxcfJ50CBnx+GUkiVl7V5+UzlnzZL0smCswpnT8OHS\nW2jGDKdHotSltm+Xwgn33+/sTZVx4+Q14x//cG4M6uo2bpRUW3+ncIK2YAhmxkiw16tX4BSqi4yE\ndetkbCp4efSWZVlWHaAl4Crs+xfLslZbljXZsqzyuWxyDbAr29e7cT9QVIVEXJykulSv7vRInDNk\niKyxWLzY821jYqBKFUmDDHZt20KdOoFflXP+fJld0SplhcfkyXIhfd99zo6jUiUpDvPDD9K0WwUe\nVwpl377OHH/AAMmOWLrUmeOr/ElJkUJ1gZDC6RIZKT18d+92eiTKG0XcfaJlWaWAWGCUMea4ZVn/\nA14ATObn14H78zsQy7IeAh4CqFKlComJifndlQpQJ0+evOzvun9/GMnJ7Xn44d9ITNyV+4aFQJky\noRQr1pGJE/eRkbHV7e3Ong3hu++i6dNnPz//vMWHI/SfDh2u5ZtvanDnnecC9nVg+PD2HDoUhmUZ\nGjc+TocOqXTsmErduqf8Vo5fXSq31xe7pKfD+++35/rrT/Hbb2v47TefHMZtYWHVOHWqId988wtV\nq55zdjBBypfny5dfNqdWreLs2LGcHTt8coirCgsrQkhINO+9t5O0tO3+H0AB5MvzxWXy5DqEhNQm\nImIJiYkXfHosd124UBZoyZdfrqZduz+cHk5Q8cc54zZjTJ4fQFFgNjDmCt+vA6zN5fEOwOxsX48D\nxuV1vNatWxtV8CxYsOCyx954wxgwZutW/48n0Nx6qzHVqhmTnu7+NtOmye9v/nzfjcvfkpPlZxoz\nZqPTQ8nVgQMyvsceM+a554xp3Vq+BmPq1DHm8ceNmT3bmLNnnR5p4ZLb64tdZs2Sv++0aT47hEcW\nLpTxzJrl9EiCl6/Ol1OnjCle3JhRo3yye7d17mxMy5bOjqEg8eXri0tkpDFdu/r8MB754w95rfn3\nv50eSfCx65wBVhg3YrWrfbhTjdMCPgI2GGPeyPZ49jahtwC5LeFcDtS3LKuuZVnFgOHAtx7Go6oA\ni4uTBeXXXef0SJw3ZAjs2+dZKmdMjKR1de7su3H5W1QU1K0Lv/4amKVZk5Pl8+DBsm5qxQrYswfe\nfx+aNZM+bH37yt9l8GCYMkWqq6ngNWmSVKW76SanRyKaZJY5W7fO2XGoy/30k7TBcWK9XnbagiG4\nbNkihVACKYUToHx5WWKjRVqCmztr9qKBu4AeOdos/NuyrDWWZa0GugOjASzLqm5Z1iwAY0wa8Dgy\nK7gB+MYYo29PCshao1ZYq3DmdOONEBbmflXOM2ekCfmtt0pT8oLCsqB9e9i8uZTTQ8lVUpJ8btky\n67Hq1eGhh+Dbb+HwYekVePvt8MsvcO+9sqYyOhomTNDF7sHm4EH5u959t/8rK15JRIScU+vXOz0S\nlVNCgryOd+ni7Di0BUNwiY+Xz4FYqC4yUoO9YOdONc5FxhjLGNPcZGuzYIy5yxjTLPPxm4wx+zKf\nv9cYMyDb9rOMMQ2MMdcZY17y5Q+jgsuMGXLRq8GeKFVK3qBjY90rvJCQAKdOBXcj9Stp0wYOHQrj\nwAGnR3K55GSoVw/Kls39+yVKSOD+/vuwa5fM/P3jH1Ixddw4eeOsVw9GjZLqjufP+3f8yjOffipV\nDZ3qrXclTZvqzF4gmj1besaGhzs7Dm3BEFzi46FVK2ntEmgiI6XVS36qhavAEMRdufznt9+kWuTc\nuU6PpGCJi4P69eWiRYkhQ2DvXliyJO/nTpsmd/i7dfP5sPyuTRv57JpFCyRJSdC6tXvPDQmR544f\nL9vt3g3vvQeNG0sw2KuXpHsOHaqV8wKRMZKW26FDVupkoGjSRGb2dJY4cGzfDps2OZ/CCZIh0b+/\ntmAIBnv3yut/oKVwukRGSibRdq31E7Q02HNDlSqwfLnk4it7HDki5etvvRWtXpjNjTdKQ9W8UjnP\nnpVUwVtuKVgpnC4tW4JlmYAL9lJTpTR2q1b52/6aa2DECEm/PXxYZreHDpXZgLFj7R2r8t4vv0jP\ntECb1QMJ9k6e1JLogWT2bPkcCMEeSLCnLRgCn6uvbCAHe6CpnMFMgz03lColKRH56YGmcvf993K3\n8bbbnB5JYCldWt6gY2Kunso5e7Zc6BWERuq5KV0aatY8zYoVTo/kUq7iLO7O7F1NyZJS8OPDD+Xv\nuGGD9/tU9po0SV7/hw1zeiSXc2VEaCpn4EhIkD6hDRo4PRLRqxeEhmoqZ6CLj5csp0DLHnBxjUuD\nveClwZ6boqPh11/hQmC0Pgl6cXFQo0ZWup7K4k4qZ0wMVKgA3bv7b1z+1qDByYAL9nIrzmKHRo2k\nEMgf2sYoYJw4Ad98I4FeqQCsFeS6ANMiLYHh/HlZg9uvX+Bkq5QrBx07arAXyI4cgQULZFYvUM6b\nnEqWlArZGuwFLw323BQdDadPw6pVTo8k+J06JXdANYUzdwMHXj2V89w5qQ44aBAULerfsflTw4Yn\n2LtX2lEEiuRkedOrUMHe/TZuLJ83brR3vyr/vv5aXqsCMYUTpBVEpUoa7AWKJUsk2yJQUjhdtAVD\nYJs5U7KcAr1QXWSkZhEEMw323BQdLZ81ldN7CQmy5izQX9ycklcq548/wvHjBTeF06VhwxNAYBVp\nSUrK/3q9q3EFe5rKGTgmTZLZs/btnR7JlWlFzsCRkCDrp3v0cHokl9IWDIEtPl5a91x/vdMjubrI\nSLkZqdWjg5MGe26qUQNq1dJgzw5xcXJHulMnp0cSuFypnL/8cvn3YmIkPSfQLirsVq/eSSwrcIK9\nI0dg2zZ71uvlVLu2zOZqsBcY1q6VtP0HHgjs7AOtyBk4EhLkPa10aadHcilXC4ZZs5weicrp9Gk5\nbwYNksrNgSwyUmYgt2xxeiQqPwL89Aos0dES7Okba/6dOyfFWW6+WRaOB4zzJwLqD3ulVM5z56Ry\n16BBgdPg+SJjbK3xHR6eTuPGBMy6vZUr5bMvgr3QUGjYUNM4A8XkyZIifdddTo/k6po2lVn+vXud\nHknhtnevLPEItBRO0BYMgezHHyXgC9QqnNlpRc7gpsGeB6Kj5UV9506nRxK85s2TixPHUjjPHoZv\n/wvf/wNWPAEzu0P5ULinDGx6y6FBXa50ablwyJnKOW8eHDsWYI3UP/0UXn1VKlmMHm3rrlu3Dpxg\nz1WJ0xdpnCCpnDqz57yMDCnM0r+/ZCAEMleRFk3ldFagtVzIqWdPed/QC/XAEh8P5ctD165OjyRv\nDRvKTUk9h4KTBnse0HV73ouLgzJl/JCCeHQXHPgJtvwP7m4FTzSAuKoQWwnu+Au8+QJsmwyhp6BX\nA6gXAft+lJzBvn0DIqIfMgT27Lm0R9K0aVC2rJTUDhhz50rFmLp1Jd/ZxhnSNm2ksEAgzFwkJUkq\nd8WKvtl/o0bStPbMGd/sX7nn11/l/10wrInVipyBYcYM6aHZvLnTI8ldx47y+WoVnpV/XbggvXJv\nvDE4Cq0VLy7tITTYC04FsB2z7zRrJiW4Fy+GP/3J6dEEn/R0ixkzshqHe80YOHcIFsTA7rUQBRxb\nDyOXQLEL8LfM560MhWsrQvUboGwT+DIEGneC61qDFQL9gF//DLvi4Mjv0jW7XDnZNiPDsWT67Kmc\nHTvKwujp06U3my2/P7t8+qmULSxZ0vZdu1pzrFghP7eTkpN9N6sHMrNnjKyJCNSLxsIgNlYuvm68\n0emR5K1yZbn5oDN7zjlxQtZdjRgRuOs7a9eGqlVlDfijjzo9GgWwcKGsAw+GFE6XyEitSB+sNNjz\nQGioVGbTmb38WbOmLIcP25DCmZEB90bCzQfhXCq8CewFJpaVYO6mdlCmBnS7F8o1hduvufRduHEu\n+4xoB79NghubwaD1EuAZI7N8nTvDP/7h5aA9V6aMHD4mBl5/HebPh6NHA2zG4fx5WTyYPdBbvFga\nO734ote7j4qSP0VSkrPB3vHjsHmzb9dwZa/IqcGeM4yRYK9376z7PYHOVaRFOWPmTFlLHVCp9TlY\nltww1Jm9wBEfD+Hh8h4fLCIj5fXxzBkZuwoemsbpoehoWLNGLv6UZxYurEhYmA3rGpLmwFcb4EA9\naDUR3v4EFibB4CPQZwm88jOM+wqq94USNdy73Voxs7566tKsmbxz5+Daa6WUGciV4KlTXg7eM0OG\nwO7dkloWEyNr+fr08esQrmz7dvnd5OzY++OP8MUXctvSSyVKyMWs0+v2XMVZfDmzV7++nKpapMU5\nycmwYwfcdpvTI3GfVuR0VmyszJq5UiUDVceOUk34wAGnR6IyMiRLp29feY8LFpGR8jqja8uDjwZ7\nHoqOlv+o2ddRqbxlZMDPP1eiXz8bsv1KH4aRQPfnoNFI6HAPXNfKuxyaMo2hSCk4/GvWY2Fh8P77\n8Oc/y9fTp8N11/n1anzgQJk4++oruRMYUCmc6enybtWs2aWPP/MMrF4tK89t4CrS4uTFrKs4iy8q\ncbqEh8uyR30jdU5srGRw3Hyz0yNxX9OmMuO/b5/TIyl8Tp2Slga33hpg1aVz0aGDfM6tnY/yr+XL\nZV1wsPUabtpUPuu6veCjwZ6H2reXiR9N5fTMihVw+HBxe17czq2H1kWggY0lrEJCIaItpP565efU\nqiUl+urVk6937/Z5BFK2rMRT//sf/PFHgKVw1qsHX34pRVmyK1pUpiDT0+Xq2cvfUZs2cPCgvDk6\nJSlJGt9WqeLb42hFTucYI7Pn3btDRITTo3GfFmlxTkKClM4P5BROl1at5MahpnI67+23pf7DwIFO\nj8Qz9erJOaTBXvDRYM9DpUvLehoN9jwTFwehoRneFz0wBr74Fs7Xh9AwW8Z2UUQ7OJICaVcoh9i6\nNXz8MRQpImvVunWD++6zdwy5GDJE+iOVKhVAKZwrV+YdfU2dKldBc+d6dajsRVqckpzs21k9l0aN\nZG1gerrvj6UutXatFMcJphRO0GDPSTEx0p6jc2enR5K3sDB5DdNgz1nbtslb44gRwbMu2KVIEbkh\nqcFe8NFgLx+ioyWNUxuUuscYCfZatjzqfWbfxo3w6lrYVMGWsV2iYjswaXBkZd7PDQ2FZ5+Fu++W\nr8+eldRFH3Clbg4cGECLoh9+WGY5r2b4cKkt7WWfiBYt5NedlOTVbvLt5Ek57Xy5Xs+lcWM5lX7/\n3ffHUpeKiZFM8GCqjgcy21yhglbk9LczZ+D772HQILkItlX6OTi+xeadyrq9FSvkXqVyxmuvyfky\nZozTI8mfyEh9rQlGGuzlQ3S05OqvWeP0SILDzz/LHfNu3Q56v7NapeE1YJAPSjNGtJPPh91YkBka\nCvfck9Uw8P33pXSkD14Fy5aFxER44w3bd51/X34puaVXExoq9estCw4fvrQ7vAfCw2WtgFMze6tW\nyQ0Lf83sgaZyOiE2VmZofJ2qazfL0oqcTvjxR7kR5JMUzqd7w/+awvmjtu62QwepO7bSjfuZyn4H\nDsDkyXKPuHp1p0eTP5GR0oZYixQGFw328kGbq3vmzTdlDUyvXjYEe0dWQjWgbifv95VTeFUoWfvq\n6/au5O67JeBzrWCeP9/WV8P27aXiW8C47rqs/wh52bFDpqz++998H87JIi2uGUV/zeyBVuT0t40b\n5T5NsKVwujRtKuPXipz+ExsrNai6d7d5x3tT4N2fYd4FOPiTrbt2FWnRVE5nTJwos6pPPeX0SPIv\nMlI+6+xecNFgLx9q1ZKaFBrs5e2332DGDMlPL148fzM7FxkDz78K24DyLWwZ32Ui2uUv2CtfPqtq\n59mzcMMNtvSZCziHD8vPuW2b+9vUrg333utVOmebNnLoXbvyvYt8S06W2R5/3ImtUEEaZevMnn/F\nxsrnYKuO59KkiXQ60bL6/nHuHHz7raRwFi1q8863/wveDIdWxSB2kq27rl4d6tTRipxOOHYM3n1X\nZoLr17d558bAyR027zR3WpEzOGmwl08dO2qw547//Efy0x97zIad7dkDnyyGA1WhiLf9G66gYns4\n9Tuc2Z//fYSFydXjI4/YN65AkZQkfSDOXKGITW4sC159NauSRD44WaQlKUlmFr3p7OGJRo002PO3\n2FiZPc9ZWDZYaJEW/5o3Ty7ebU/h3PML/D4VWo+C+aXhjTk2H0Bm9xYv1llgf3vvPUn2GTvWBzvf\n/Dk8XxdSZvlg55eqXVvaZ2mwF1w02Mun6GiZZXBipiFYHDsGH30Ew4bZNCtSowZ8Xg1usbHlQk6u\ndXv5md3LbsAAaZpW0PTtC/v3Z93e80R6OoweDS+/7PGmzZvLTQN/F2k5fVouoP2Rwuniar+gF2P+\nsW2brGEKhvL5V+L676ipVf4REyNrqXv2tHnHg2+G/xaFJn+D5x6AZ87B6b22HqJjR9i7V69d/OnM\nGVnO0qePj95L1nwArwKT3/TBzi8VEpKVNq6ChwZ7+aTr9vL20UeygH3UKJt2ePYQpO2BatfbtMNc\nlG8JIUXdK9JyNceOyRVBQep07CrhVqpU/rYPDZU8s0OHPN40LEzWCvh7Zm/1aqkr44/iLC6NG0tK\nXj5+TSofgj2FE2Q9b7lyOrPnDxcuwPTpWVWSbXPgZ6h3CHoOgGLlod1wKAkcmG/jQSTYA03l9Kcp\nU+Stzyezept+gVmL4Cmgz2kfHOBykZE6sxdsNNjLpxYtZCpbg73cpaVJ49DOnW26UDYG7hgOyUAF\nH06zFAmHci28n9nbvVsa5HnZYy6gDBoka++88dlncoszH5wo0uLP4iwuroqcWqTFP2Jj5dwK5ol4\nrcjpPwsWyM0YW4v5GAOrn4ZbqsI/v5THyreA5aXgf+/YeCDJkihRQou0+EtaGvz739CunbTmtd1n\nL8PHQJsb4cQyuOD7MpmRkfm+b6scosFePhUpIv95NdjL3YwZ0its9GibdvjHH7ByDRxBZt98KaId\npC6HDC86WzdqBMuWScBXEBgj09muxXP5FRoqnzdvluqlHmjTRk4Df/agS06GihWhZk3/HdNVkVPX\n7fnerl3w66/BW4UzO63I6R+xsZLc0KePjTud/hr8+DM0fQaKlJDHrBBYVQ7ik2z9oxYpAm3barDn\nL9OmwfbtMqtn+7pvY+D69fBJO2j9KPyQBrPyaIlkA00bDz55BnuWZdW0LGuBZVnrLctaZ1nWyMzH\nX7Usa6NlWasty4q3LKvcFbbfYVnWGsuyUizLcqhTlm9ER0sPrhMnnB5J4HnzTbj2Wkl1sUVEBHza\nDW6qC8VyPdXsU7E9pJ2E417cJg8Nheuvl/zDgsCy4Omn4fHH7dnff/4DzzwDR93vI+VEkZakJJnV\n81dxFpClqSVLarDnD3Fx8rkgBHtNmkBqqt5t96W0NIiPl/ah4eE27dRkwGsvwWdFoPa9l37v9Sdh\n/AU4YW+D9Y4dISVF1iQr3zEGJkyQG3i2XQtld2gxnNoGnR+Fap3hG+C7GB8c6FKu9guayhk83JnZ\nSwP+aoxpArQHHrMsqwkwB4g0xjQHNgPjrrKP7saYKGOMl9MCgSU6Wtbz/Oplxl9Bs3y5zHg+8UTW\nRI4t/kiGCn5YPHWxubqXf9gNG+SVPt2LGcJAcPq05C7ZOWXwyityp6Sc+4F7s2ZS5txfRVrOnpU7\nl/5crweyAL5hQ03j9IeYGDmvGjRweiTe04qcvvfzzxJM21rMZ+c0GHEMvngZwnJUma53A1jAfnuX\nA3ToIIGrE9WNC5MffpB133//u7yu2+6pxyG+KNS8FcJLwdd94KYjPjjQpapVk25TGuwFjzxPP2PM\nPmNMcua/TwAbgGuMMT8aY9Iyn7YUCNKi1fnXvr3c8dd0iEtNnAilS8N999m0Q2Ogc0eY8Ztv1+u5\nlK4HxSpAqpdFWlasgHHjYOtWe8bllKlToUcPe+9qlCqVVaJ1+XK3NileXC7M/XWBsmaNXBD5c72e\ni6sip/Kd/fvlplRBmNUDTa3yh5gYWe/Wv79NOzx/BlY+AxFNofuYy79f6jqYXQ6ee8umA4r27eWz\nXrv41iuvyBKAP/3JBztPOw3b1sH5OlA0s2haoxvh5G9w4jcfHDCLZcnsnr7WBA+P7jVYllUHaAnk\nvOq7H/jhCpsZYK5lWUmWZT3k6QADWdmycvGp6/ay7NkD33wDDz4IZcrYtNNTp6CkBcWA8n648rYs\nmd3zdmbvlltkkVnDhvaMyym33y5/1Hbt7N/355/LApKFC916euvWMrPnj3VJyclZx/S3Ro1g506p\nZqt8Iz5ezqNgbrmQXfXq8pqrM3u+kZ4uab8DBkjAZ4vXRsBjW6HakxCSSxqMZcGJKrB1u3dryHOo\nWFHeljTY851Fi+TjySclI8V2u+Lh4TR4/4Osx8p3g8nAJM/bG3nKVZFT1wgHhyLuPtGyrFJALDDK\nGHM82+NPI6meX1xh007GmD2WZVUG5liWtdEYc9mVXWYg+BBAlSpVSExMdP+ncFCdOvWZO7cK8+Yt\nsjdlMUh9+GFdMjJqcf31v5KYePaS7508eTLff9caj7eg3vElLN5wmgub87cPT9Q+WYU6JxJYNH8W\n6SF2vbMHsUqV4KefbN9tSOXKVB01in0XLmBynBu5nS+lS1fjyJGGfPXVUqpXv/T8stt33zWgdOlK\n7Nix2K9FYQDS0ysCkXz55QoaNNCIzx2evr58+GELatYszsGDywrMOreaNVuyZEkGiYmrnB5KwPP0\nfFm9uiz797ekceP1JCYe9Pr4IeYc7Y/PgHplWXK0FlxhLJUfuI0mx15mxdxJnCxm343DunUbsnBh\nBAsWLPHrmuRg5en5Mm5cM8qWLU2DBktJTMywfTzNd71GeLFq/LohAzZmjisjgy5binAmaR7LfXwN\nXaxYdY4ebUBMzBIqVTrv02MFK2+ueW1njMnzAygKzAbG5Hj8XuAXoISb+xkPPJnX81q3bm2CxWef\nGd+VaUAAACAASURBVAPGpKQ4PRLnnTxpTPnyxtx6a+7fX7BgQf52nJZmzOI7jImvke+xeWxPgjFf\nYMy+ed7tJz7emBdftGdMTnjhBWOmTfPPsc6cueTL3M6XpCT5//b1174fTuvWxvTs6fvj5GbdOvk5\nP//cmeMHI09eXw4eNCY01Jj/+z/fjccJDzxgTKVKTo8iOHj6fjRypDHFixtz/LhNA1j/urzH7J9/\n9eed3ifPWzfBpgOLDz6Q15jNm23dbYHlyfmyapX8bp9/3keD2bjUmKIYM+G2y7+39M/GfF3amPTz\nPjq4SEyUnzEhwaeHCWr5vubNAVhh3IixrvbhTjVOC/gI2GCMeSPb4/2QNo43GWNyrelkWVZJy7JK\nu/4N9AEK1JJOba6e5bPPpP+Qbe0WQHIEGjaEtxP8k8LpUrGtfPa2315iIkyaFJy5Dmlpkr7588++\nP9bvv0tO9NdfX/VpkZFQrJjvi7ScPy9r9pxYrwdQr54UN9J1e74xY4ak5RWUFE6XJk2kgEhBmakM\nFBkZ0nKhXz9Zj+61P/bAW/+AiJ5QpfvVnxteFb6uAI+8cfXnecjVXF1TOe33r3/JsnS7ildfZk8c\n9AQG5HKA6v0g7QQc9rLmQB5ca4S1SEtwcGfNXjRwF9Ajs31CimVZA4D/AqWR1MwUy7LeA7Asq7pl\nWbMyt60CLLIsaxWwDJhpjEmw/8dwTp06UpmosAd7GRlSmKVNm6wA2BYXLsAtN0G1VP8UZ3EpVh7K\nNPT+BfNf/4Jt2/xbu98uRYpIxcxXXvH9sa65RiKratWu+rRixaQpsK+LtKxbJwGfE+v1QH7O667T\nipy+EhsrTdSjopweib1cF2C6bs9ey5bB7t023hx4+xF4/xQUvde959dqBCVTId2+1PXGjaXuwC+/\n2LZLhbzdT50KI0ZIxUrbGQOn4uGvXaFZt8u/X6kbTACee8YHB89SsSJUrapFWoJFnmv2jDGLkOK/\nOc3K5TGMMXuBAZn/3ga08GaAgc6yJLgp7MFeQgJs2gRffGFzXFOsGDw1GOa86ftm6jlFtIN9s+XF\nNb8/VPHi9o7JX1ztIkJDbaxGcBVFiuQ5q+fSurW8mXrzZ8mLa+bQqWAPtCKnrxw5AvPmwahRwXkP\n5mqyt1/o2tXZsRQkMTFSZGPgQBt2dvYQNJ4PH/aAHne6t83fx8LCm+DwL3nPBLopJESqcurMnr1e\ne03ezsbkUlzVFsunwbotMPT/cv9+WAWoVhnY4aMBZHEVaVGBzxedPwqd6GjJQtuzx+mROOfNN6Ua\nnO1pUfv3S3898O/MHkiwd/YAnN7p3X6efhresrd0ts99+y1cey1ssbeZb54yMuD11+Hjj6/4lDZt\n4Ngx+M2H1aWTk6Wy4bXX+u4YeWnUSH79aWl5P1e577vvJGGgoLRcyK5GDUkz1Jk9+xgjwV6fPjIT\n5rVVL0HGGRjyX/e3qdIVrFDYbm9iVMeOcrF+7Jituy20DhyAyZPh7ruzOgvZbsJ4qX5Rod+Vn/Pa\nYxC9C84e9tEghKv9Qob99WeUzTTYs0FhX7e3di3MnSv56cWK2bzz9u1hzFtQvBKEX2PzzvNQMbMZ\nkbepnMnJwTdFU7GiXAnUrev/Y8+cCfPnX/HbbdrIZ1+mciYlSVapTxrhuqlxYwlKtm1zbgwFUWys\nBEXXX+/0SOxnWTK7p6lV9klKkpu5ttzI3PQrDHgLDvSEso3d365oGXi/HNz3rg2DyNKhgwSzy5bZ\nuttCa+JESf9/6ikfHSDtDNywCyb0gnJVr/y8an3kD7t9po8GIpo2hdOnYccOnx5G2UCDPRtERUmm\nW2FNh5g4EcLDJUfdVhkZ8I9/QEcjs3r+zrkq1wxCw7zvt/fDD/Dee/aMyV86d4avvpJ8FHdsfgf2\nz/X+uCEhMqv46afy9apVcPToJU9p2lSyY31VpOXCBTmsU8VZXBpnXgsG232CQHbiBMyeLbN6Tgby\nvtSkic7s2Sk2Vl4Gb7rJhp2tfg1qWdD3n55v26cTtD0F5+2bhmvXTt5WC+u1i52OHYN335WbAvXr\n++ggu6dD+Em4/QopnC7l28D/hcDfXvLRQERkpHzWVM7AV0Df7vyraFHpC10YZ/YOHpS+2PfcAxUq\n2LzzkBC45w649nf/VuK8ePyiUKENpPq2qlXA+flnuSp217H1sOJxWDgIjtlQUaRUKbkCMQaGDYOb\nb77k20WLQosWvpvZ27ABzp1zdr0eSBFa0CItdpo5U/62BTGF06VJE0knS011eiTBz5XC2aOHDe9v\nxzdBWhx8+BdonI8qZg+Mgr4GDtrX77RMGSmCrMGe9/73Pzh+HMaO9eFBnnsWtlaGynksyA0tAjdF\nQuPDPq0E7lojrMFe4NNgzybR0bByJZw65fRI/Ov99+XiaeRIH+w8ORm2LwKT5v/1ei4R7WTNYLoX\nTUP/+AMGDJCrhkB3+jTceKNnf9ANr0FoOISWgEVDIC3XTiyesywp2jJhgnx97hy8+CIcPkzr1nJ6\n+GKtgGvG0OmZvbJlZd2HzuzZJyZGKsi5ys4XRFqR0z6rV8PWrTalcL52D5wJg8in87d9xQ6QHgbL\nYm0YTJaOHWHpUl135Y0zZyTDqU8fH75vHNoC03+DPQ3BcuPSfeRfoOURuRnrI2XKQO3amjYeDDTY\ns0nHjlLAsDDlvp87B++8A/37SzEJ2w0eDI/+Tf7tVLBXsR1knIOjq/O/j3LlJOA7a1/ZbJ8pUQLm\nzHF/0cHpPbDjc7juAej4ORxbJ7N8dmnRQhaWACxYIGm9q1bRpo3cRd261b5DuSQny+Rigwb279tT\njRppsGeX06clo/qWW6TIbEGVvSKn8k5MjCSYDBrk5Y5Wz4QXf4VlrSGscv72EVoc3ikFo92rWuyu\nDh3ktVTPl/ybMkVm08eN8+FBDsbCW8B4Nwv7VOsDp4ClX/hwUFqRM1hosGcT1/VoYUrlnDpVXuBs\nbaLuYox0ab+9DhQtCyUdKBQCEGFDkZaQELl1eqebZbad1rat+9H7prfApEOjMfLm0vRp2PYxbJti\n/7j69ZPylD160KYNjGQiaWOesv2WdFKSrMMNhDVdjRtLGqcPM3EKjYQECfgKWiP1nGrVkpsVerfd\nO8bAtGnQrRtUquTlzlL/C6+WgRc+924/D90Cw87B6X1eDiiLNlf3Tloa/Pvfsv7RZ+1OMjJg+ydQ\nrTNUa+7eNiVrwUvF4R8f+GhQIjJS3qMuXPDpYZSXAuBypmAoX17SZwpLsGeMtFto2hR69fLBAVwN\nDKvvcaY4i0uJGhBeDVK9LNISDJYulVuTOQqiXNH5Y7D1fag1FEplBuPNxkPlbrD8ETjqg6vN664D\ny6JJE6gfup2MjZuyojIbZk7T0yElxfn1ei6NG8td9332XdsVWjExEBEBXbo4PRLfsiw5b3Smxjvr\n10vvWK9vDuxPhH0J0P8ZqFTLu30NGgFRwIErVyv21HXXSTCrzdXzZ9o02L5d3jp9dpny42R4fBOk\n9fZsu9F94cYTUsXTR5o2lQqkvsiyUfbRYM9G0dHyglkYct9/+kkqFvqsMfGsWbD8VziyypniLC6W\nJbN73rZfWLRISnSt9iId1NeWLIEPP3S/f8bWD+DCcWj8t6zHQkIh+kspFb5oCFw46ZOhFikCn1//\nFn+pHicPHD4MNWvKbLAXNm6U9RdOr9dzcU2waiqnd86ehe+/l3Q8dwvMBjOtyOm92Fh5+b/lFi92\nkpEB/W6G70tDAxvS28tFQWoZmOnd61x2liWZSTqz5zljZEl5kyYwcKAPD7T9WwgLgbb3ebbd4BFQ\n7zwc+tk340IrcgYLDfZsFB0t5XcLQ/rMm29KK7Y77vDRAUaOhPHjZL2cU+v1XCq2g5Nb4ZwX5e2q\nVYPmbqZfOGXMGGmYU6JE3s9NPw+bJkKVnpf/fcKrQccv4fhGWP6oz3IQW7eGFStD5eZKWpq827qa\n8B04ALt3e7zP5OSsfQcCV/sFrcjpnTlzpMBsQU/hdGnaVGaDjxxxeiTBKyYGOnWSgj75ti0eKhyH\npjdDkXDvBxUSCrGl4fm5tt5V7tgRNm+We2bKfT/8IPdv//53H6b9p5+Figth8p8gooZn21bpCr8X\ngRkf+mZsyA3JkJDCcd0bzDTYs1Fhaa6+dSt89x088oj01/OJxYvhr/3k3+Vb+uggbopoJ59Tvai+\nc911cqs4UAM+V8J9qVLuPf/3L+HMXmhyhUIuVXtISueOz2DbZFuGmFObNnDypFykULUqTJ6cFR09\n/3xWDqQHkpLknHa1PXBatWpS8Uxn9rwTGyvVTXv0cGgAp/fCye1+O5wWafHOpk2wZo2XNwdMBmx+\nHkZdB+NsfA38+/0wLh1O2Jc351q3p6mcnnnlFUkouf12Hx4k6WM4cwyuvdfzbYuU5P/ZO/O4qMou\njn8vmwgICiiKu4ICai5oKrRoi72pbVZWmqmlZeWSldVrm9mqaaWV2qKZWVq5lGWbmuaSO665izvi\nArggyDbP+8dhXtBYZrkzdwbn+/nwGaC5557kzr3Pec45v8P3VWDMT7q7ZaZyZYiK8mT2XB1PsKcj\njRpBRETFD/YmTJBSqMcfd+BJatSA4GMi51/FYFnE0LYidWxvKSdIBsrVyMuT6Gb8eMver0yw812o\n2hJqltFD0OxFqHmTqHNm6F++ak7ilThv77nnZJB9cLD8PG2aRRFTUpKIs7hKqZ+myc6pJ7NnO7m5\n8OOPMq7R0gpl3cjLhc8ehZ+iYVEimJzz+fcEe/Yxt3C6QY8edhiZ8xrs2gpXjZaZrXqR+CBEACf1\n69tr21bueZ5gz3JWrpSvZ5+V2a8OY8grMNoXIjrbdvyo/vBcjmw4OQiPIqfr4wn2dMSsKVKRa9/P\nnIEvvpCdrFq1HHSS6dOl6zljE1RrJaUrRuIbBCHN7RdpGT9eFCJcLeDLyoJu3YqK78sj5ReZ3RM7\nouyGTS9vSPga/KoV9u9ZMajdAmJiZFfRPBfvEurXL6oxTkuDwYPhu+/k57w8URw1/1xQAGvWYDpz\njk2bXKeE00xsrCezZw9Ll8p9y+mD1DO2wLBoePQzOFwL5h6HHyzcULGT+vWlGttTWmUbc+ZIH1sd\nK6vm/k9BLjz/DkyqDPXu09U3qkTDvuow9XPdTFauDK1bV+y1i9688460sgwY4MCTZKVApzR4tLtl\ns/VK4tqHIARI/UNX14rTrJkIZbvDdKkrFU+wpzOJiZCcDKmpRnviGD7/XAbHO2TcgpkpU+CrGRLs\nGd2vZya8A5xeK1ktW2nTBp54QhRAXImQEPjwQ7jlFsvev/NdCKgH9XuW/17/GpAwS3oe1z2ma/+e\nj48sUErM7BUnLEzk0h59VH4+e1bkc81pnkOHoGNHTnz0PZmZkNjoOPTpUxRF5ubKMQYRGwspKYa6\n4NbMnSvVyV26OOmE59Phlyfht7ZwXTZ88gwM3gw/aTBvulNc8PLyKHLaSnIybNpkZwln8hcwIhcm\nj9F/s1LTYF0IfL4JTAW6mU1IkDnBHgn98tm6FRYuhKFDLWtxt5mDM6GlgkFjbLdRtQVsqwqTP9bP\nr8to3lxaSD0VKK6LJ9jTmYrct5efLzFBp05S6uYw/v4bPnwF8jONVeIsTlh7yDsD5/fabqNzZyny\nr1JFP7/s5fBh61aEp9fAyeUyV8/S0qSI6+Gq1+HQLFHw1JH4eFmYFZS35omIKEpFh4fD778XTUqu\nUQMWLGBtiEQD8ZHHRW7W3O+3ejVUrQpLlsjPu3fDyEchaZGu/y+lYVbk3L3bKaerUOTnw/z50L07\n+Ps74YQnlkFiHXhiEtR/EO7aBY+Og8pBMHcAdE+GXOeopjRr5gn2bMFcwmlzJjgvC7aPhsYJcJMO\nCpwlMWo4jDfBmS26mezYUfYhXVkw2lUYM0Y2kAY76M8LSPQ0+X3waQfB0bbb0bxgcyh8k6Tr5kBx\nPIqcro8n2NOZ1q1lUVERg7158yQ2eOopB5/IywvUfvneVTJ7ZpGW03aWcppMriV59u67EjFZmjba\n+a6UZTZ+xLrzxL0Atf4DG4dB+ibr/SyFtm0l02xXIBQUBLfdxoqDdalUCRr0aCMXeufCHon69eXp\n3rw5pG+EGT1g2mewYKBTpp2bNWc8pZzWs2KFfNwcXsJ56hCsGQBLOkOPEBg7BhK+gEqhRe9pMwhM\nuXDoOwc7I8TFwbFjlo/N9CDMmSP3lfr1bTQwsie8mQIxoxw3eC3uTvAFUpfoZtIzXN0ykpNh9mx4\n7DEpEHEYK2fDR6mwL9Z+W+88D2NMUi3lAKKjpW/RUzbuuniCPZ3x84N27SpmsPfBByIq2b27A08y\nfjy89RZkJIGXH4TEOfBkVhASK7Pj0uwUaeneXfrjXIVXX5X+yJCQ8t97bi8cmQ/RT0gfozVoXtDx\nK6gULv17ufrUJJYp0mIlSUnQsmUJzfYNGsDjd8PeYVKa1+YkzLobmhzSdbhxaTRqJD55gj3rmTtX\n+pFuvdVBJ1AKVk6CJo0K1WBHwEv7oWcJKrXVWsPiCOjzXwc5cylmkRbPdWM5hw5JKaPNJZy5ZyFt\nKQTXhHpWDsC2hoBI2BAJb03RzWTdutKj6An2ymbcOGkhePppB5+o8koYXwkee9N+W03vlNW+g/r2\nfH2lAsWT2XNdPMGeA0hMlIVjVpbRnujH2rVSzTZsGHg7Ui9l0yZZuacnQdWr9FUxswfNC0Lb2Z/Z\nGzAAhgzRxyc9CA+3PHrfNV4C8CY2+u8fDtd8CxcOwtoBumTFmjaFwMBSRFqswGSSz+y/hqlfPAkb\nhsDPMXDsJ2j+Mty+H66bCVp1+H20fSe2AB8f2Tn19ENYh8kk1Qi33irXiO6cPwwresDBJyExFB74\nClqPBZ9Smng0DWp1AK8MOO34VVGzZvLq2W23nHnz5NXmTPDOcXBTFsz5RTefSiWlOqw5APn6qWJ0\n7OhR5CyLEydkT+ehhyAy0oEnKrgIh2ZD+7utn61XEv41YG1dGOm4vr1mzTzBnivjCfYcQGKi9Iqs\nX2+0J/rx/vuiYt+vn4NPNHOm1NFkJLlOCaeZ8A7SI5FvRxTfowc8+KB+PtmKUtJwsMbCTGX2CUie\nDo36QuUI289bPRFavgVH5sAe+x883t4WirSUQ3KytOj9X4kzLxO2jYYFjWHvZGg8AG7fJzLqvsHg\n7Q8fBsOLy+GM41MnHkVO61m9WgaL617CqUzw0aMQ2xD2/wptxsKC49C5d/nHvjQZhnjBsdk6O/Vv\n6teXrKanb89y5syRfvSoKBsOPrQd5o2Dej0h1AmzYce8AqOV/SrRxUhIkOzmsWO6maxQfPCB6HU9\nV8p4Wd2YMQo+y4Dq9+pn80Id2JMCOY6p627eHA4ehPP6im570AlPsOcAzLXvFaWU88gReQgOHOgk\nbZGswyJi4CriLGbC2oMqkKyjPRw/Lv+oRnL4sIwesDRdtOdD6TeKecb+c8c+C5HdYNPTkGZ//WV8\nPGzebN9EC3NmsE3rPAnufoqCba9CrVug2z9w9WSofNmskVHvQn9f2P2B7Se2kJgY2L9fFhoeLGPO\nHCmr17Xs/OwuWHw9ZHwGNatCwhKIGwFeFg5mrFwLanaBpOlQ4NgRLN7ect14gj3LOHZMShhtLuF8\nrT+8cRFCHTmAthi1bpSKkxP69+15snv/JjPTm0mT5PqItkMvxSLW/gjbfKBhV/1svvEGvAycWq6f\nzWKYRVo89xvXxBPsOYDQUNmJryjB3kcfSSLI4dWH//2vpA4zCoOpak7YHbWG8EKRFnt2Uk0mWYG9\n/bY+PtlK/foScPbqVf578zJh7ySoexcE6zDgXvOCjl+Cf01Y2RNy7dtpbNtWSqbtKXNMSlLcn/A9\nrY/FwfonILgpdFkN186R70uiy13QvS8cnAEXHSu6ExsriqP79jn0NBUGpaQkr0sXqUiwm/wcGHIj\nDGwOZ/+B3l/A+tPQNNF6WyfawoBj8MNEHRwrm2bNPGWcljJ/vrzaFOxdOASdtsB7t0BcJz3dKh2/\nEFhUG4ZM0s1kq1YiMOcJ9v7NggW1OXcOXnjBwSfKPg7X74GfngVfP/3sVk8En0A4/rt+NothDvY8\n9xvXxBPsOYjERLlhmuwYy+YKZGbCp59KKZTN6mSW4usrW/HpSaB5y3wYV8K/BgQ2lPEDtuLlJcMK\nBw7Uzy9rMffKVapUNGuuLPZPlUxr7Aj9fKgUBtd8B1lHYM3DdvXv2S3ScmIZ/SPbM+vJnmje/nD9\nz3DjMinbLY/A++Dni7Bnso0ntwyPIqd1bNggyWu7ZqWZOb0Gfo+HzX/CuTrQdQc06me70mK34XCn\nH2iOV8KIi4OjR4umiHgonTlzZMHatJS9nTLZ/SH4AQM+09utsgmOgtx03QSv/PzkfuoRabmU7GyY\nM6cOXbqU0NetN3tnSKl4dD997XpXgqV1oc9Ufe0W0rChlI17+vZcE0+w5yASEyEjwzmiCpmZ0LOn\niKf89pu+M7u//FKkux0+bgFg9GiJLNOTRIXTp7ITTmol4e3t75G4915pNDOK2bPliZWSUv57TXmw\n6z2ofq1lwY81hHeAVmPg6HzYPcFmM02ayPQEq0VaMrbC0q6wpDNVfI7zxa4v4NbNULub5Qv5zakw\nE1g8AQpyrHXdYswLUI9Ii2XMmSPCNrffboeR9BTo3Qa+7gh5Z2HGPPjtIATUtM+5KqHw3EOQ/5tk\nzR2IR5HTMk6cgOXLbezvNJng6SlwoCUE1tXdtzJ54SUYouDUCt1MJiTIvfSifrovbs3hw3Ifycjw\n47+OFtI1meDOUfBDZOkVJfbQoC3UyIHT+t8QvLw8Ii2ujCfYcxDOHK7+0kuyuPn0U1GeCw2V14kT\nYc8e25MmJhNMmABXXy0qXQ6l+FTsjCTX69czE9ZeslFZFgRKpXHxoqwsUlP188sagoJESqymBYvW\nw99LD2WcgzrSY4ZDnTtg0wiblU69vCR2tTizd+EQrO4Lv7aC06tJrzeWqOF7yInsB15WSs3edRes\nmwE10+DQt9a6bjGBgVCvnmfRbglKyciFG2+0Yw7WqVUwpx3M2wQnOknfZsO79HOywUOw+wIsGKuf\nzRLwKHJaxvz5ct3YlAnevxz2XAClwzw0a6meIGJRxxbpZrJjR8jLE3XiKxmlRHmzRQup0nrmmd10\n6uTgk55YCy0uQoKOvXrFefxlGARk/OUQ855gz3XxBHsOIioKqld3fLC3erUEdU88Aenpktl77DE4\ncEAyfU2bii+DB8PPP8sAakv55RfYuxeGD3fcbNj/M2yYPGWyUuDiCddT4jRjzm7Zk907ehSuv17+\nIEZw221ybq9yPv5KwY6xEBwLkQ56+GgadPgCAupI/15Ouk1mLBJpyUmDpGfgpyYSmMWOgDuS+TN1\nBBfzKhcpcVpDYCC0fRBCmkkG1IFD1j2KnJaxZYuI2diswqkUrHsMInwh6Wd4a6kosOpJ9USY5APv\nOE4KHaS0yt/fI5pQHnPmyLPSHBxbxcW/YJwGw8bo7le5ePvD9zWg16e6mTRv7F7JpZwpKSLs9Mgj\nUoSzbRt0737c8Sc++jX08ocn3nWM/SrREFgf9jtm7dG8uejPpaU5xLwHOyg32NM0ra6maUs1Tduh\nado/mqYNK/x9qKZpizRN21v4WuIeqqZp/9E0bbemafs0TXN0a6vLoGlSDuHIYC8nR8a21akjeh+V\nK8Mtt4g88K5dsuD5+GN5gH3xhazxQ0Ph5pvhvfdk4VjW2vT998W27tLlJREfDzfdBBmb5GdXzexV\nayWz/+wJ9ho3hoULZQyDs0lPl21bS0hdJKMmYkeIqIqj8Ksm/XsXj8OafjYFTG3bSsL0kkVtQa6U\nah78BpKelTEKu96HBr3htr3Qegz4VSMpSUr+WtjaIpqWBtOrwaotcHKZjUbKJyZGPtfu3gfsaObO\nlX2MO++00cCK2fDJP1BzMMR209W3/+PlBRMegcfS4YLjlHk9ipzlc/o0LFsmWT2bNjWPzJcMW2Ct\n8t/rCNp2gOYX4YI+wUhEhDyirsRgTyn4+msJWpYulcqmP/+UTROHk3UOFn8Fte8Av6qOOYemwcJw\nuHsh5Og/CNoj0uK6WLKCyweeUUrFAR2AJzVNiwNeAJYopaKBJYU/X4Kmad7Ax8CtQBzwQOGxVwSJ\niaKed+KEY+y//bY8xKdMKXkkQqNGkvFbsEDW+IsWiaJmSgo884z0czRsCIMGwY8/XjofZcsWuckN\nHiy6KQ6nf394/fXCsQYaVGvphJPagLe/qITaI9KiadC1q0TezuaFFyTVa0nEsPNdqBwJDSxQ7LSX\nsHbQepwMLt813rpjlYkOzfdze/yP5G16A1beDwubwXeB8GtL+Lu39ATWuA66boUO0y7prdm4UTZE\n/P1t9D04GNYfhrQg2PmejUbKJzZWVEePHnXYKSoEc+dK4rx6dRsN/PYpLAIaOHiXq+vzEAAc/Mqh\np4mL8yy+yuLHH6WLwKYSzu3L4aEtcLS57n5ZzMAR0BM4tUw3k+bh6g4sVHA5TpyQje0HH5QNks2b\nYejQ8gtgdGPG6zDyHKQ6eKP75jugO5CyUnfTnmDPdSn3MlZKHVdKJRV+fx7YCdQG7gC+LHzbl0BJ\n+6hXA/uUUslKqVxgduFxVwTmvj1H7JBt3w5vvQW9e0vcUB6VKknibNw4+SAePAiffCIlCl9/Lbvg\nYWFwww0wdiy89hoEBMCjj+rv+79ITy8aIJaRJPL+vs4Y6GcjYe0hfQOYCsp/b2kcOSKqnM4enHbP\nPfD88+U/wdKTIHUxNB0mKl7OoMkQqHs3bH4BTpXwoVEKslPh+CLJ0K15GH67Gr6rQqPtUfz49J3E\n+74MaesgqLH0GSbMgq7boOcFuH4BVG3+L5NJSdhWwmnGzw+SD8ATz0DKz3Butx3GSsejyFk+O3bI\nv4/N1QgFOdBmK/x4F0Q4eDs/qCGkXAVPj3FoujYuTkQmPMOOS2bOHMlktbRlf/HAQqgPtOmpWz+m\nxAAAIABJREFUt1uWU601eIfA9gW6mUxIkJbygwd1M+nSfP+9BCq//CLrnxUrRPjLqdTZBk9WhTsc\nrIZ3xxC40xvO6SfqY6Z2bQgJ8fTtuSJW7VlomtYAaA2sBSKUUua6gVQgooRDagPFa1SOFv7uiiA+\nXoIsvUs5CwqkfDMkREo2baF+fQnk5s+XKrSlS6U37/RpiQXmz5eRdzYLHFjDq69KvajJJEGGq5Zw\nmglrD/kXZN6Wrfz9t4xfcPYWWJcuku4tj53vgk8ViHrM8T6Z0TRoP1V6ClbdR9WcJNj3KWwYAos7\nw7zqML8WLO0CSU9DykLppYoaCFd/xhM/rKHzpHNwR7IEdi3fhAb3S4DnXfKIiSNH5Jq3W07bywui\nHwflZ5eyaFnExMirR5GzdObOlcvoLlu1VI79LFL2zZ00GiWvDWw8BzsWOuwU5j40zybBv8nIgMWL\n7SjhrLwWXm8BLW/Q3TeL8fKGL4Lh4e912zQwD1ev6KWcaWlw//2iZt6ggWz8jRgh5c9OJfsEnF8M\nAwbpO1uvJPyqQkg7WDlfd9Oa5hFpcVV8LH2jpmlBwFzgKaXUOa3YnVEppTRNsyvhr2nao8CjABER\nESxbtswecy5DdHRrfv0VunffpJvNOXPqsHZtFC++uIPt20/qZvfWW+Xr1Ck/tm0L4eqrM1i2rCzF\nC+vIzMws8e9atVEjAnr14tTSn0jMOsz+jK4cceG/v3++1DPv/vtLjgfeZpMN78BA/GbOJDsjQxpG\nnEDwP/9wsUYNcsupb/PPP077k99xJPBekv/W77q1lCD/52lzejCtsp6BNMjXKnPBpyEXfDtwIbgh\nF3wbcsGnIXnehTsR5+UrTYtk1bpAFi/+Cx8fy25HK1eGA83RtCSWLbNvGFnc6NFUya6G39BprD5/\nC/leIXbZuxylIDg4kSVLTtGy5R5dbVcEMjMzmT49k2bNCtizZxN7bPgn6vjiY3hX8WdVLT/UnmW6\n+3g5PvF30nHCN6Tu+5y9px1TzXD+fGWgPfPm7SIryyAFYBckMzOTMWN2kp8fS4MGG1m2zLrUZ6XM\nVDqkLudQjQc5aPDzKuraeOrUP8KaRV9zsZL94x8KCqBy5Wv4/vsT1K69VwcPXY9Vq8IYP74p58/7\n8PDDh+jV6zAnTypOlrKkKm39ogdNF42h1rkC1oXFkOWEa6nlNzlU/eYf1kZ+xcVq+o4LCQ1twvLl\n1Vm6dJXjhf1cHEdeM1ajlCr3C/AFfgeeLva73UCtwu9rAbtLOK4j8Huxn/8L/Le888XHx6uKwnPP\nKeXrq1RWlj72kpOVCghQqls3pUwmfWw6i6VLl5b9hpQ/lPoapY4vcYo/NmMyKTUnXKnVDxvtieWY\nTEpFRSnVpUv5710/RKlZvkpdOOp4v0rj1Gq19dc3lTp/QClTgUWHzJqlFCi1aZPlp3npJaW8vJS6\ncME2Ny9h3DilXn1KruHtb+lg8N8kJCh13XUOMe32fPXVGgVKvfeejQayUpXqqin15LW6+lUuK3sp\n9V1VpXJ1ekhcRl6eUn5+So0Y4RDzbsvSpUtV9+5K1atn47P0nf5KeaPUhh91981qzu6S+87eT3Qz\neeONSrVurZs5lyEjQ6mHHpJnRcuWSm3ebNlx5a5fbMVkUqpNkFIxgY6xXxJrvlNqGErtmqG76YkT\n5d82JUV3026HXtcMsEFZEKuV9WWJGqcGTAV2KqWKqw8sAPoWft8X+LGEw9cD0ZqmNdQ0zQ+4v/C4\nK4bERBE/tHrgcwkoJWMVvL1h8mQnjENwBgcPimyoUtKvB6J46cpompRyptkh0gLw118i9+UsFiyQ\nRs+yyEmD/VNFsTLAwIrr8A6k+SdAUAOLlUDNfXcWz9tDynbi4qQ/1W6eeQZGvQ81b4Y9H4oaqM7E\nxnrKOEtj+XLJWNvcr3doFvRW8MZk/ZyyBL//wLAz8PlLDjHv4yMlwB7RhEu5cMGbP/6wo4SzRjLc\nEwytu+vum9VUaQJ5NWGxfrM+ExJEqC0zUzeThvPbb9Kb9/XX8PLLsG6djb2aepKxCYZnwmeO+fyX\nSLu7ILEqZCzT3bRZpMVTyulaWLKKSgT6ADdomra58Ksr8A5ws6Zpe4GbCn9G07RITdN+AVBK5QOD\nkazgTuA7pdQV9cgx177r0bf35ZeiqPnOO1BX38y7cbz/vmje5+VJv15gA6hkgEqltYS1h7M7Ic+O\n0r+FC+Gll8oZDqcTmiaRQnlKJHsmQUEWxDzreJ90pnFj6WO1dGNFKXmv3f16xTGZwHQXZB+Hw9/p\naFiIjYWTJ0XTyMOlLF8eTrt2MnzeJlZ8AqFtoaotw9bsoNV9UM8fMh03pycuzjN+4XJWrw4jN9dG\nFc68TKi8Bp7t50S5xjLQNPg2AJ5bCgX6PE8SEuR2tm6dLuYM5fx50Si49VZ5RqxZA6NHi7aW4SRP\nB59KcLUT++O9fMAnEWbN110cyqPI6ZpYosa5UimlKaWuUkq1Kvz6RSmVppS6USkVrZS6SSmVXvj+\nFKVU12LH/6KUaqKUaqyUetOR/zOuSHi4DGu1N9g7cQKefhquuUZGJVQYhgyBWbPkrpue5LrD1C8n\nvD2gIG297TZGjhR1EB+LW2dtZ/x4SWOVRX427JkIkd2dv+DVAS8viWUtzewdPy6fK7uUOC/n3Xeh\ny5NQEO2QIesekZaSOXQIdu8Otm3hDrD8W3h0F+wwQELf1w8mD4U6G0SowQHExUkRxYULDjHvlixf\nXp3ataF9exsO/utzOJgDdWwd5ugABj8Mzyg4s1UXc+Z/l9WrdTFnGH/+KfvJU6fCc8/JBl/btkZ7\nVUh2JvSZDAfbyrxZZ7IjFCZmwAZ9xaGqV4caNTyZPVfDBbakKj4JCaJqZc+6b8gQeVB/9plrbCTq\nRlQU3HEH5J6FzH2ur8RpJuxqebVn3l7VqiLX6mjS0yWD+McfZb/vwHTIOQ1xIxzvk4OIj4etWy2b\naGHOAOqa2evZU2qEWg+T8pyTf+lo3DN+oTTmzpVXm0s485dBHy+4f6ReLllHo76QUwDLHFPW7VHk\nvJTMTFi7NpS777bxeTpuIrylQWhH3X2zmU79IRo4uVQXc9WqySaBuypyXrggc4JvvFH2kleuhDFj\n7Jin6gi2fAMqH+p0cf65H3oWxgBV9Bfg8Shyuh4VKWxwWRITReJ3t43jt378UebAvPpq0c5+hWD7\ndhlsk5cHGZvld+6S2fOrCsExkLbWPjsTJ8KUKfr4VBqhoXDqFDz+eOnvMRXAzvFSnlr9Wsf640Da\ntpVAz5IHTVKSVD+10rNFtGFDeOABiHkYKoXLPEAdqV9fFiueRXsRa9bA669D06bnaNzYBgOmPMiY\nCwPvgsho3f2ziJA4+KAKDLZxlk45xMXJq6eUU4KACRMgN9fbtkywKQ/uToN3uoCfC0UOAZFwugF8\nPUM3kwkJktlz4BhIh7B7t/TiffwxDBsmA9I7ulBc/n9yFsLbtaCXAZtMda+CuBhILWcT2AaaN5cy\nTne7bioynmDPCZiHq9tSynnmjIxFa9lS5r9UKKZNk614k0myIOA+mT2A8A6S2bMnZfvzz+Vn3PQg\nKEiaFUrj6HzI3C+DyN1Y+cdckmlJ397GjVJiHRSksxOZmfDVbAh8AI79BOf02zn19pZhv54yTmHJ\nErjpJggLg1GjbIxkFk6ANaeg3oP6Omctwx6C+7MhfYvupqOiwNf3yg32zp+H2bPlcVO9uhQ6REef\n/39PvVWc/AsCz8HtZWyeGcXKKjB+K+Rk6WKuY0eZRWjLGBOjyM2V/bazZ2Wq0Qcf6CTApTdpB+Dg\nQmjQR3rojOBiW3h/MVw4o6vZ5s3lMXj4sK5mPdiBJ9hzAk2bymLElmDvuecgNRU+/1we1hWKN96Q\nf5RKlaRfr3IkVI4w2ivLCWsPOafgwkHbbfzyC8ybp5tL/yIjA267rewue6Vgx1gIioLadzjOFyfQ\nqJFUx1rSt7dxo879embOnIFHHoFt4eDlq/uQ9dhYT2YPpOKha1dJpq5YATVrXrTN0EcT4TMNahpQ\nSlWcB0ZBC184qF9mxoyPjzyHriTRhHPnpKL6rrukh+iBByRL9cgjEgRMnrzRtuHZU96GzX7GXy8l\nMWIYfACc1UH+G/ccrv7667Bpk6yZrr/eaG/K4N3h8EQBBHUzzoesKPilAJbP1NWsR6TF9fAEe05A\n0+SmaW2wt2yZ9Og9/bQLNRTrSUBAUcNURpJ7ZfVAgj2A03aUcjpanOXAgfLrKU4uh/T1EPsseNmy\n+nEdNE0+K+UFe6mpkJLioGCvTh2pI332ZWjQC5K/gBz95DNjY+XPmp2tm0m3Y+ZMydK0aiUTTGrV\nstHQxdPQ5zh8+iBUMnj73z8c/G6ACZ9Cro2BaxlcCYqcZ87AjBlw++2SwXvwQVi/XpQYV6yAo0fh\nww8lCLAp0FMmmL4c1oSBT2Xd/beblndDsBekLtHFXJMm0gXgLsHe2rXw9tvQr59IAbg0Nf6BOyIh\n+jrjfLhnKHzuCzUP6WrW3CPs6dtzHTzBnpNITJRSiFOnLHt/djYMHChy8q+95ljfDGHdOhm7kJkJ\n+Vlwbqf79OuZqdoCvCvbN2/vwgXo27dIYUJv2rSROYZlSc7tHAv+NaDhQ47xwcnEx8O2bZCTU/p7\nzMKkuoqzFCcuTiLPpsNllMW+T3UzHRMjydi9+vfVuwWTJkGfPnDddbB4sSxGbebQLPDKh5tdZNRI\nemuYkQm/fKS76WbNZJMgS58KP5chPR2mT4du3SSD17ev9Gg9+aQEKYcPS4/eNdfoIG6WtgFez4f3\nXtTDdf3xqwp7G8P4abqY8/KCDh3cQ5EzKwseeghq15bSTZcmYzPU3AejDL6OAqpB3Wvh+O+6mg0J\nKdrz9OAaeII9J2Hu27P0pjlqFOzbJ5k9l6w3t5c//pDRAz4+IhWtTO4X7Hn5yFwuezJ7AQGy9ZyS\nop9fZkwmiQo0rfQ+vDPbIeUXaDLENXeqbaBtW9H82bat9PeYg73WrR3oyGuvwbRFUPMmXYesX8mK\nnG+/LYv4226TCugqVew0OPg12N4Aql2lh3v20/tF+LgqhNsx0qUU4uLkdlAR+j3T0kRK/9ZbISIC\n+veXrOWwYSLYc+gQvPee9Jzpql599Afw8YYWD+hoVGcOhsPPRyArQxdzCQnyb5uhjzmH8cILsqH+\nxRdlt6e7BLPfgAwfqH+/0Z5ARnN4eRvs36Sr2ebNPcGeK+EJ9pxE27Yi/2tJKefGjTBuHAwYAJ07\nO943Q3jpJThyRKQF0wtX3u5Wxgki0pKRBAVlpJHKQtPkSTpkiL5+Afz6q9ThlNVdv3MceAdA9BP6\nn98gLBFp2bgRoqMhONiBjmzaJE+7psMhOwUOf6+L2ehouWyupGBPKVnMjRwJvXtLItxuCfXDa2Bf\nGuDIiN9KKgfB1X3g6I+Qq+/q2t0VOU+dgk8/hS5dJMAbMEBubc88I/tlycky5rJ9ewdqTA39CP6J\nhUr2pJMdzEsvwkQgQ590nLlvb40dBSyOZvFiKc8dNgxuuMFob8ohLwdGzofvarjGdVQjAY4Am/XV\nDmjeXJ5R+fm6mvVgI55gz0n4+8sitLxgLy9PGsgjIuTBVaEJD5fXjCSoFAYBdYz1xxbC2oMpFzL0\nV9Czm8BAqd2qX7/k/551FA5+DVEDXeOhoxMNGkhpX1l9e0lJDurXK87cubLNHPkfGdOh05D1ypVF\nlKQiZGgswWQSReIxY2DQIOnJ0kWsKm0uvOUDL07SwZiO1OwJn+TAhKd1NRsdLYUU7iiaMHEi1KwJ\njz0mw+Gff14+w/v2wTvvyGaqw0WEj2yAY+fBK9bBJ7KTejeAdyVIXayLuXbtJDvqqqWcZ85IZjcm\nRjL/Ls/JJfCqCUb912hPhGvugSkRUHO/rmabN5dWiv36mvVgI55gz4kkJsoCtKxeonHjYMsWmQ9T\ntarzfHMqf/0lqzZzA2N6oTiLO0r+hxf2wtkzb2/zZtk+3aRvGQWdOsEPP5Q+uH3XB4CCmOH6ntdg\nyhNpOX1a+ngc1q9nxqwAUWCCpk/JpsapFbqYvlIUOfPypD9vyhTJ7E2apFNZXl4O7J0BtbtBYE0d\nDOpIZAKk+MPOP3U16+sriX53y+wlJ0twd9NN8mzcvRvefFNKsJ36yDi/BF4HnhvnxJPagE9l2NAI\nnv1CF3NBQTL6yVVFWoYOhePHZROosjt0IhyYAXXD4LpHjfZE8PIWZdnURdJOoxMeRU7XwhPsOZGE\nBAn0Sisv271b2nzuuUfkoiss+/bJuIEqVaT88ex29+vXMxNQByrXlnl7tlKtmmy56ymvePq0iL+U\nRu4Z2PcJ1LsPAkvJ/Lkx8fFSQXmxBFFDc7+ewzN7ILVFjRpB7fske63TkPWYGLlfFBToYs4lyc6G\nHj3gm29kx/7tt3Vc3M96BwaehFwXrJP38oJ5o+Cmw7rOaARJ9LtbsDdsmOybTJsGV11l4J7goXnS\nox1UzyAHrKEBHD8D547qYi0hQZQuXa0kb948+Oor6Qpp185obyzg1GEY9T2o/4C3n9HeFHEqCoac\nhqXf6GYyNlY+q56+PdfAE+w5EXPte0mlnCaTqG8GBMj6sELzyCOife/vD2f/AVOee/brmQlvb19m\nr359WL4c2yb8lsI774gsWWlp5H2fQH4mxI3Q75wuRNu2sjDZuvXf/8282eJQcRYzMTGiJnLRBFGP\nSy/W+X12m42NlT/tIX0Vs12G8+dFXXHhQsnmvfCCzifIWQGtK0HiIzob1omGfUDzgh1TdTUbFydl\nVe4ytmPBAvj5ZxEsq13bQEf2bYT71sH+GAOdsIIXRsErQIYNw31LoGNHEc52pYX7iRNS1hsfDy+6\nqDjqv1j0Eaw0QchNRntyKS17QHXgpD7XC8hatlEj17pmrmQ8wZ4TiYiAqKiSg71PPpE5QOPHS29C\nhcdci2UWZ3HXzB5AWAfI3A8XLZyr4QzuvhveeqvkEk6lYM9HUPNmqNbK+b45gbJEWpKS5CFUrZoT\nHLn5ZqnJDg2FJk+IgqsOQ9YrsiJnWhrceKPsf3z1FTz+uM4nyM2AKitg3GMiiOKKBETCT/XhznFQ\noF86xazIuXu3biYdRlaWlOg1aybZPUM5tBCuAtr1NNgRCwmNB98QOL5IF3PmfUhX6dtTSjbHz5/X\nsYfXGdRYCzOj4UYXG3PUoDm80wrC9X2gOEqRMz9f5lBX9OoWPfEEe04mMVFq34vrNBw9WtST0K+f\nYa45h0WLpJfs4EH5OSMJfIMhqJGRXtnH//v21tluY9o0iIwsue7QFjp2FFWLkjj7j4izuILss4Oo\nV0/0f0rq29u40Qn9epezZw+oEKhfOGTdTqXFmMIEQ0UTaTl+XG4PW7dKiVbv3g44yR/j4UwuNOrr\nAOM6cvM9cE0BHNFnQDYUDTt2hz6at9+WzPXHH7vAYl5bBc83gXbdDXbEQry8YUlt6P2VLuYaNJBN\naFfp25s+HX76Sa4Rs8qsy3M+GU4uh9h+Os8D0Ylat8CxlXAhTTeTzZvLo68snQpr2bZNljedO8tz\nMChINnf795dxK4sWSeGYDlpoFQoXvOIqNomJoktiHoislOxcFxRIds8dNUqsIidHaohq1JCf0zdB\ntdZSsuSuhMaD5m1/Keett0qtjL38/XfZW/ephbu9NW+2/1wuSmkiLenpMljaKf16Ztavh6ZNpSYt\nZjjkX4B9n9llMjRUPkIVKbN34IAMvj5wQGbo3X67g0404gMY5y/3HVemzyjoWQVSZ+tmMjpa+t9c\nvW9v714YO1aC/euvN9iZjGOwZwnUudO9HtBR7aBeLqTZvyOkabLAdoVg7+BByfRef70LZHytYeSj\n8DYQ6aLZ4bTGMLAAvtNvIn3z5rK2LWv6k6Xk5oqmRXy8bAJNnSpi1088AWFh8NtvMoalSxeoVUue\njzfcINfI559Lz2lZMgYVHR+jHbjSMA9X//tvUUb79lvpSXjvPSktq/B07y5fAKZ8OLMFogYZ65O9\n+ARC1Rb2ibTceKN86cHQobIVXlrNzfFFENwUAuvqcz4XJT5edvmys4tU2syCp07N7MXHw0cfyeqk\nWi2IuEGGrMcMBy/bUxYVSZFz506peM3KgiVLZFaaQzi7C/pdgFqPu/7C3ScA6t4Lv82C2DEQUsNu\nk35+EvC5crCnFAweLC3d41xB+PLLt+DpAljawmhPrGPg8xD5JWSshDD7ew0TEmD+fOmVi4jQwT8b\nMJmKqp+mT3fNBFmJKAUFW6F2TagWZbQ3JdPxPujyBFS2v6fcjFmRc/t2aGHHx2fDBnj4Ycnq9eoF\nEyYUTe4qzunT8p5t26Q6ZNs2CfSysuS/a5qss1u0uPQrKko08ioy7vJRqTDExEiv0KpVcmEOGQJX\nXy3r8wqPyXRpbv3cbijIdu9+PTNh7aWM017pYj3qHebNk9qnkijIgZN/Veisnpm2bWVXcUuxEYjm\nHj6nZva8vODJJ2W7ESTIyzoKh+fYZTYmRoIkdy9X2bgRrr1W+jD++suBgR7AgS+hnjfc+4oDT6Ij\n6fHwWjZM1c/fZs1cu4xz3jz44w94/XUX6V+vmQy9g+AaNyt7D46BypGw/1ddzLlC397EiXKP+OAD\nKS11G06vgWtOwSQXHgQYEAzP3gIBZQyotZImTSSIsrVvLztbxLnat5de7gUL4OuvSw70QH7fubOs\np83ZvPPnRQB+/nzJDLZpI8/NN9+Enj1l09RcCrpype3/r66OJ9hzMl5ectNctQqGD5eBoJ9/XjSS\nqyITumGDSKqZP/kZFUCcxUx4B8g7KwGsrfTtCx062O9LvXqlp65Or4GCrCsi2CtJpCUpSSpmw8Kc\n7IxS8OuvsHQpRHaFKk3sHrIeGwsZGUXjKt2RFSuk1CYoSB609uz+lkteLrzxEeRdA5VdIYqwgFsf\ngxE1oIl+iipmRU692oP1JDMTnnpK5rqV1nLsVAougtcKGNIbvN1s61/TYG416DlfF5GfNm2kYMSo\nUs4dO2Thf9tt0p/lVqz6GLz8oe7dRntSNjVvhr374HCSLub8/CTgsyXYW7UKWrWCMWPk7/3PP/K3\ntxYvL2jcGO68E15+Gb77TnrdMzNlbTB9ulQShIdDSIj19t0FT7BnAImJsrMwcyaMHOngBY4LkRcU\nJNsuDRvKL9KTwLsyVGlqrGN6EKbDcPVbb5UJ0vYwbpxsfZZG6iLpL4zoZN953IA6daRuv3jfniHi\nLGaeekrkdjUvye6lb4BTtktdu7si56+/Sn9FZKQEelGOrm5aNhV+zIQLzkzr2omXNzz4OJz7Cy4c\n0cVkXJwUWejRR6M3b7whgmWTJrlIWdVfU2HHBYi8w2hPbOOWrtBNwclNdpvy95cNNCMye3l58NBD\nMpr3s89cvwL7ErLOwYPfwNx64FvFaG/KJjsGngWm61c/ba0i54UL0md37bVS6PTHH5IQqVpVN5cA\nae1o00b22MeNg99/r9hrcU+wZwDmcojYWAn2rhTOx8VJDj4wUH6RkQRVW8qCxt0JbipS16ftCPbu\nvx+eftr24y9elPl6v/9e+ntSF0lg6hts+3nchMtFWs6elXIOp5ZwFnfmp59g7lz5ueFD4Bcq2T0b\ncWdFzvnz4Y475B64fLkE5g4nYAV8GgL9RjnhZDrSsA+sUDBJn5mYrqrIuXOn7IX076/vyFG7mDAR\nxgJh1xrtiW3cPRS6A2dW6GIuIUH0pnJzdTFnMW++KRt1U6YY1y9oMym/Qh8F/Z402pPyaXkzDAmD\nOPvUoovTvDkkJ1smjrJkiQRcEydK58P27dLL7cF+PMGeAXToILtUM2eWPAatQpKbi09xpUllEiXO\nilDCCZKtCbsa0uwQaQEJ2E6ftu1Yf39ISZE5HiWRmyHZpCughNNMfLyU/2RlGSTOUpwmTYo+8D4B\nED0Ijv4A5/fbZK5OHdk3cbfMXmqqLOhbt5aq1urVnXDS3LNwdD407w2VXXx3/XKqNIaVwfDtz7o0\naDZp4nqKnErJ4i4oSParXAJTAdydBu/d5LrzGMsjoA5Ujoa/5+tiLiFBsi2b7E8UWsz69ZLx7dNH\nxse6HSmzoEstuNUV6pLLwcsLevcA9TeY8nQxaRZpKes5dfYsPPqojB/z8ZENwA8/lPuBB33wBHsG\nUKkSfPmlgYtOI1i2jMTbby+aKH9+P+SfrzjBHkjG7Mw2kda3lUaNSg/WLMHPr/TC89Q/JciudeUE\ne23bSsna5s3SrwcGf+7mzpVpwADRTxYOWZ9okykvL5no4G7B3vDhsqcxc6YTeyTeGwZjL0L1e5x0\nQp2ZOgqGX7BvlmchlSpJyawrBXvffiuB/1tvFU3lMZzTf4NPGtw60GhP7GOWPzy5EnKy7DbVsaO8\nOquUMztbNsZr1ZJsj9txdDfM+glq3Cv3encg9AZYew7W2ScgZsZcSVBaKefChfKeqVNhxAgRVLvW\nTRPprown2PPgHBo14mDfvtJ5D0XiLNUqULAX3gFUAaRvLP+9pfHmm7ZNks7OlnqHJWUMYE5dBD5V\nJAN5hVBcpGXjRtEHMrQM6NAhWLdOJMICIqHe/ZA8DXLP2GQuNta9yjj/+ANmz5by9ehoJ574+Aoo\nCIB6Rg9ts5FWD4OPPxyYoYu5uDjXKeM8d06q1+PjZXffZZj0Jqz2hshbjfbEPh7pB08CaevtNhUZ\nKQJXzhJpGTlS7m9ffKF/z5ZT+HI0TDGBus5oTywnuD28B3w5RRdzjRvLBtPlwV5ammRru3eXv+3q\n1TJb0zwmyYO+eII9Szm/D/LOG+2F+xIVxaG+fYvy8ulJMmMspJmxfumJOYiyp2+vf3+RJ7SWo0fh\n5ElJY5VG6mKI6GzXbDd3IzJS5Ns3bJDMniH9esUZNky2LqsUlhLGDIf8TNj/uU3mYmPh8GFRFnN1\nsrNFYbFJE/uS11Zzbi+0T4ZvXnWjwVyX4RcC+9rC/Z+I4IOdxMVJ/6oek17sZdQoKe1rBiSRAAAg\nAElEQVSdNMmFVKmVgjl/wfpQ1xfVKI/O/SFeg7RluphLSHBOsLd0qYxYGDxYyvvckjZ74IMmcI0b\n1Z9GNIQJLaCr/ZlgkM90XNylwd6cOfK72bPhlVdkI/bqK2cP2hDc9MnnZE5vgv9Gw7dvGO2Je7J5\ns0yOL05GEoS0AG8/Y3xyBP7VIaiRfYqcJpPU5Vmrpx8dLUFEaU/FzAOQuR9quutT0zbMIi1//QW7\nd7tA6bR5NVtQIAvK0NZQo5OUcpqsl0c3i7S4orLi5bz1lkj+T5ni5F7ljZMADRo86MSTOoDGXSGg\nALbOsttUs2ZyCRp93WzbJuV5Awe62GLvzDb470X4+EWjPbEfv2qQ1xzmfquLuY4d4dgxOKKPOGyJ\nnD0rw9ObNBHpfbfk7C7I2ABdHjPaE+vpdDec3wg5abqYMytynjgB99wD994rPecbNsjsuytGu8JA\nyg32NE2bpmnaSU3Tthf73beapm0u/DqoadrmUo49qGnatsL36Tep0dmEtoTvfeDLmUZ74p68+ioM\nGYJmlvBSSjJ7Falfz0xYB5llZytHj8qW1/ffW36MyVSU0StNkzp1kbxeQeIsZuLjpXpSKRfI7IHU\nq9SpU9REGPM0ZB2BI3OtNuUu4xd27pRFW58+Mn3FaRTkw30fwuw6UjbrzvR8DkbXgpyFdpuKi5NX\nI/v2zKIsVavKRoBLcXS+3Evj3GyQemksC4A3dkL6MbtNmZVSHZnde+opeRTOmAEBAY47j0N5ayjM\n16D+A0Z7Yj3hN8NCBTP1SXA0by4bBLGxIkr99tsy8Nzc1ePB8ViS2ZsO/Kf4L5RS9ymlWimlWgFz\ngXllHN+58L1tbXfTYLy84IsB0O+EKBp6sI5vvoE//kD5FWbxso5AbnrFDPbC20P2Mcg6atvxdevC\nV19JIbulrFx5afBQEscXiTJbcAWYaWglbYvdeQzP7IGoqiQmFmX5aneDKtGwe4LVpqKixIwrB3tK\nweOPSwX3OP3GN1nGscXQvQB62jm/0hXw8pbs5P5f4PQBu0w1bSqPNSODva++ghUrRH0zLMw4P0rk\nifdhQ2Oo7G46/6Uw9AUYA5xZbLepq66SAMxRwd4PP8ig65EjoX17x5zD4SgTbFgFR8MhoJbR3lhP\nRAdY7AMLy1raW475GRwTI4VeL7zgInM0ryDKDfaUUsuB9JL+m6ZpGtATsL+uxNXpMAC0Ajiij4Rx\nhScvD95/XwbyBAZeqsaQXgHFWcyYh6vb2renafDgg1CvnuXHBAaKfFVpihemAjixRLJ6bjWNVh/M\n2byaNaWHz3BCQ6VpoVUr+VnzgujH4fRqyNhilSk/P2mAd2WRlhkzpIx2zBgDlBaPfgO3hsD9Lzn5\nxA6icld4ogDee8YuM/7+ct0YJdJy5owo73XoAA8/bIwPpXJ6F2ScBf8K1E/e6g6IagiH7F+q+fpC\nu3aOCfZOnhSRntat4eWX9bfvNE7+BY9kwVfvG+2JbXh5wZzBcO8xuGjjKKhidO4se9ErVhRVo3hw\nLvb27F0LnFBK7S3lvytgsaZpGzVNcyWdLeup1gZWhsMzo4z2xD34/XeRWCtpwHdGEmjeUPUq5/vl\naKq1Ai8/+/r20tNhwQLRp7eE+HjRLq9SipBARpJkpK/AEk4Q2e46dS7N8LkEaWlS2wLQsC94+8Ne\n6xXQYmNdN7OXlgbPPiulX4884uSTp6fA999CrXvAp4JIvMV0gvsiIdL+lFxcnHGZvZdflnGiH3/s\ngpo5p3+DkcB/xxvtiX5oGnjdAm/9AYft/6MnJIioRosWcNttIqIybpzsYa1fLy3n1o6EVAoGDZJ+\nvRkzZCPLbdk7HXyDoUEPoz2xneZ9RV38qP3ZPU2TAN5lBJiuQOxNpD5A2Vm9a5RSxzRNqwEs0jRt\nV2Gm8F8UBoOPAkRERLBs2TI7XdOf1qciCN73D6sWzSPfN9Rod1yboCCCPvuMzCpVoPBvmZmZybJl\ny2iRtgh/77qsX2lHQOTCtPFujGn/72w+29Wm48NXrKD5K6+Q9PHHnDM315SCz9mzoGnkBweX+p56\n57+mEbBqvz95B5fZ5JMRmK8XPXjllSCCgvJZtszCANrBaPn5JPTowanrr2fPM5KlifG7nvD9X7I6\n8zYKvCxvVAkMbMiePXVZsmQF3t72D93Wk7Fjm5KREcEjj2xk+XI75k9awOXXS5OfxxH5QS6764Vw\nPH9Zqce5G7XvvZvocx+yftE0Lvg2stmO+bpZtGgFvr7Ou2727Ali0qR4br89hXPn9mLUo760+0ur\n1Gn4+DRiw8YjgANVSJxMeEpNmi9XHJ3+IvuuG2aXrdatK9GjR12OH/dnxw5/li7158KFS5eT/v4F\nRERcJCLiIjVrXiQiIoeaNc3fX6RatdxLAv3ff49g/vxYBg3az+nTRwy7LkrD0ueRT9YZEnvN4Gzv\nFmyu7sZrHKVI+CkEPnqev99qYrQ3bomeaxi7UUqV+wU0ALZf9jsf4ARQx0Ibo4BnLXlvfHy8cknS\nkpT6GqX2fmq0J67LDz8o9c8/Jf6npUuXyjfzaim1qo/zfHI2G4YpNTtAqYI8247PyFBq1SqlsrPL\nf+9rrynl5yfHlMbizkotbGmbLwby/+ulovL110pt3lz086nVcn/ZM9kqM9OnKwVK7d6ts392sny5\n+PXcc84537+ul9+uVer12koVFDjHAWeRfVKp97yV+vBuu8zMnCl/n+3bdfLLAgoKlOrQQakaNcq+\nZTmDEu8vh3coVRml3unhdH8cjsmk1JxYpf641iHmMzLkdvbDD0pNmKDU8OFK9eihVJs2SoWFybVW\n/KtSJaWio5W6+WalBgxQKjhYqWuvVSo/3yHu2Y3Fz6P1k5S6AaUWfOhQf5zCsBuVSkCp80eM9sQt\n0WsNA2xQFsROZX3Zk9m7CdillCpRiULTtEDASyl1vvD7LsBoO85nPNVaiZDCgdkQNdBob1yPnByR\n0WrRQsoQSyL7uHxVRHEWM2HtRWzj7Ha5ZqylatUiybPyuOsuqF699Imz+VlwahU0HWq9Hx4cS69e\nl/4c1l6ul71TIOoxi/sriytyNnGRDdjcXCnJql9f5ig5ncxkSFsBd73pgnWCduJfHeZVhx3zYVAe\n+Ng2N7O4ImczJ7WnffEFrFkDX37pokOyDy2EBCDxPqM90R9Ng6a9YOvLcOEIBNbV1XzVqvJVmsJi\nZqaoIh88+O/Xn36SPtLp0ytAqd/5H2Fofej+hNGe2M9rE2FhMzg2H5oOMdobD3ZgyeiFWcBqoKmm\naUc1TTN3XtzPZSWcmqZFapr2S+GPEcBKTdO2AOuAhUqp3/Rz3QA0DXZfBXf9qUvde4WjUiUp2fzi\ni9Lfk75JXitysBduFmmxYwTDli1l/zuaadFCpA5L4+RyMOVesf16Ls+2bfD11/K9pkHUIDizxaqe\nz6aFAquu1Lc3frwEER9/LPpBTmfsMFgI1Hfz2XqlMfoFeM0Ep5bZbCImRi45Z/XtpaXB88/DNdfI\nCA6XpGA5DKkPifca7YljiLhLVDnfGOz0UwcFyaZCt27wxBMwdqy0mq9dC6mpMoOtke1Vya7B0X9g\nyx/QoI8Ib7k7IXFQtQX887XRnniwE0vUOB9QStVSSvkqpeoopaYW/r6fUmrKZe9NUUp1Lfw+WSnV\nsvCrmVLqTcf8LziZxN7QFkj+wWhPXIfTp2HqVPm+fv2ydbQzzEqcNmS83IXAhhBQF479XP57S2PO\nHJk0nJ1d+ntWr5appGWRugi8KkH1a233xYPjmDRJho2ZZ1A26AU+VWDvZItNhISIyqirKHImJ8Po\n0dCjhyzsnI4ywbKlsCsUgqxQtXUnrnsMIoLhoO2LsMqVZXHtLEXOF18UFc5Jk1xUFPjsSdj8O9S5\n00Ud1IHqzSCkGmRuMtqTismE52G4Ap8bjfZEP7Y0hZ5rYbcb9x96sFuN88qjw53wbAwU2D+vpsLw\n0UeyYE1OLv+96UlSCutbuqCI26Np0KA3HP8Nsk/YZmPIEDh+XFZkpfHSS9CvX9l2UhdD9Wsqjhph\nReOll2D//iLpOd8q0PBBOPQt5JQ48aZEXEWRUylR5vPxgQnWjw3Uh5MrYNAFmPmuQQ44AW9/yLkB\nXvwGzlt+nVyOsxQ516+HTz+FoUOlGMEl+WYsPJULJ1ykFtpRfPoydDwC5/YY7UnFQilovg+ebgTN\nOhntjX50exy6Ayd+KfetHlwXT7BnLZoG9XrCjmVwfLfR3lzK6tUy387ZvPyynNuSGoyMpIo5X+9y\nGhbKFtu6816jhvTilcW338pk4tLIPgFntkLNm2zzwYPjqV3735nwqEFgyoHk6RabiYmRzJ61cud6\nM2cO/PorvPGGjLswhP3TJTsaXQH7rorjfw2szYNln9hsolkz2LPHsY+NggKpNK9ZE0aNctx57CYy\nGfoHwE2uNvhPZ+r1lKFYGz8z2pOKxZkt4LsbBj1rtCf6ctUNMCgesjzBnjvjCfZswdQRhiqYYoTy\nQCmkpsINN8ALLzjvnJ9/LjPhvL1liEo5+JjOwoVDFbtfz0xIDIRdDQe+tN3GrFnSsV4a4eFl/7un\nFmafa3n69VyarVvhnnukqQmg2lUQngD7plgcvcXGwrlzkgw2irNnYdgwuSSffNIgJ86cgDu+hJ1t\nwMeIZkEncudQ+KIWBNveGxwXJ4Hevn06+nUZn30mM9nGj4cyJsQYiykPcv+ER3uCn7/R3jiWgNow\nrQb0mQgmk9HeVBwmvwg7vCWYrmjUuw/WbYC9q432xIONeII9W2j1H3iiFjQ5aLQnshj85hsICIDv\nvxc1TJBV36FDjjtvcrKs6D76yOJDquQVriiuhGAPJLt3ZitkbLbt+JkzZaVUEuPGiYRZWaQugkph\nUK38QNyDgZhMsGoV7C5WKRA9CM7vhRN/WmSiuCKnUbz8suw5ffKJlHEaws7Z0ERB2wq44LocH1+I\n6gUpv8DF0zaZKK7I6QhOnYKRI6FzZ7j/fsecQxdWfAkbzkLN2432xDncfy90yy0STPNgHwV5MOl3\nWF5DnrkVDe9r4VXgo1eN9sSDjXiCPVsZNAi09ZCVYqwfW7dC796i6Ne9O9QtlFN+4QVo0wYuOGiQ\ncaNGIqM1cqTFhwTlFvYIXCnBR/37wcsPkm3M7n3zDaxc+e/fFxTA5Mnwxx+lH6uUBHsRN1YMVbCK\nTMuWcPTopeM26t0LfqEyhsECYmLk1SiRlg0bilp327UzxgcAshbA842gaxkKtRWJarfDq/nwrm1D\nsmNjpTPBUSItzz8P58/LteHSmieTJsIEIOx6oz1xDg+9Bp184Mh3RntSMTixGN4ogPGvG+2JY2ja\nAV6LgUSD17sebMazCrSVuvfCPwq+f9tYP1q2lKDrwcskxl9/XbbYzbrnmzbp09CzZQssWiTft2pl\n1RZ+UP4+CKxfMXe+SqJSKNS+Tfr2TDY0xYSElLxC8vaGvXvhrbdKP/bcTshO8YxccAc0rWi4VEGB\nvHr7Q6P+cPQHmUtZDrVqSYmcEZm9/Hx47DHpyXrjDeef30xg+l7Y+adk1F06stCR+tdA9SqQvdGm\nwwMCoEEDx2T2/v5bpsc8/XRRBtElUSbokQYfdoYqoUZ74xwqhUHVG2DWNDAVGO2N+3NgBgSFQhtX\nnSmiA/c9Dvn/wFkXUALzYDWeYM9WqsbCLH/4oAyBDGdx9dVQpcqlv6tXT/qAQKTQ2rSBadPsP9fI\nkfDoo0VS8VZQJXfvlZPVM9OwL+ScghQbRkzm5sKIEfBDCWM+vLz+/Tcvjqdfz73IypLNk3eLKUhG\nPQYqH/ZPLfdwTTNOkXPSJEhKgg8+kP0Jo4iaNwWGAUFdjXPC2Xh5wacvQPPd0g9tA45Q5MzPl1lq\ndepIea9Lk7YBClLgpv5Ge+JcdjWCd0/Db+XfXzyUwanD8Nh3kHE9ePsZ7Y3jqHevTNye4uofaA8l\n4Qn27OGDx2DoWcg6asz5p0yR4Ku8JutWraSO5r5CdbqDByEz07ZzzpoFv/1WJBVvKXnnCCg4cmUo\ncRYn8j9QqbptQi2+vtKHublYz19+vgjxzJ9f9rHHF0FQlGRSPbg+AQHQseOlirbB0aKkuu9Ti3bf\nzYqczuTYMZke8Z//wL1GzqFWCv92x2BYE2jY1kBHDKB+LzABqyfZdHizZtIump9vnxtKyRSRzz+H\nu+6SIpD335dh2i7NR6/BYg1qdzfaE+fS92V41Q/CtxrtiXuzbhqcMUG9Cn79VK4FK6rCN78aL/vs\nwWo8wZ49dHoSKgGH5xhz/h07JGvnVc6f0ddXmmmCguRD2qsXXH+95R/Y/HzZvs/Pl1qxpk2t9zVj\ni7xeKeIsZrx8ZebesZ+smpsGSLomOflSvfKTJ8vXSTflwcllnqyeuzF5MvS8TFgk+nHIOiIiHOUQ\nGwspKaKK6SyGDZPL8eOPDa6cPP03laufgMGW9xBXGIIawKRQeGSCTeqKcXFSRLB/v/WnPnRIBIP7\n9oX69SEqCgYOlMfSiBFw993W23Q6vy2HzVXBr5rRnjiX0Ei48XY4+j2Y7Iz0r2QqLYNJ0XBzP6M9\ncTyfvAgjskR4zoNb4Qn27CE4GnY2hBfGGHP+iRNlqJU1aJpoYL/2mnyvlAxaKouff5Zg8fffbfc1\nPUler7RgD6BRXzDlwqHZ1h97eSAfGQkrVsjWeWmcXgP5mZ5+PXckJ+fSmrrat8mO6r7yhVqcLdKy\ncCHMnStlepaM2HQYG+fCwHso2O8Hdd0hunAAA3rBnTk2Kf82ayavlpRypqSIFtiAAdC4sfT79e8v\n18LVV0sByT//iBj02LFu0Dp5dhcMzYRpLxntiTGE3QGzTsLPlqtqeyjGqV2Q+pc848vbdK8ItOwn\n/eWHvjXaEw9WcgVcnQ4mswmsS4WTTq6fOndOXm3ROO/YUZQ7QQZzx8VJN31p3HmniMB062b9ucxk\nJJHjFSoL1yuNaq2g6lVWDcn+P3v3wh13iNyhUpb1SqYuEgXOiM7Wn8+DsTz0ENxyS1GGxssXGg+E\nlF8h80CZh5rHLzgj2MvKkv2fuDh41ogZwnm5sOVL+CMBNtwDK09w6lQn8HX1mkEH0etV6OADh2dZ\nfah5k6AkRc4TJ+QRMWiQFHTUri1aYHPnQosW0qe5ZYsUHMyZU3RNuHyQZ+ZoYT90kytgVEdJ1OsG\nvwC/zjTaE/dk9GDpE65exuZrRcI/HP5pBo984JnR6GZ4gj17eWUCvAukLXTeOffvh4gImDfPfls3\n3wyjR0P79vLzoUNFH+Kff5ZgA2Tb1lZMeZC+kUzfaPt8dWca9oX09dYrWQUGSuY1LU1WVTVqwNKl\nZR+TuhhCrwa/qrb768EYhg2DTz+99HeNB8jqeV8pMxcLadRIKradIdIyerTcKiZPtr591y7ys2Dv\nZGgZAv36QXYqdJoIx86wq8d/neiIi+EfDkE3wtTPJRC2gqAgKcHcsUNuM3PnwuDBkvGrWVPm433z\nDURHy3jPjRvh9GnRjRo2DK66yo2TGo+OhWV1IaCO0Z4YQ1A1+PF+uGEfFOQY7Y17oRSE74Qu9SDC\nleVmdaZqO8jOhmTLZsB6cA2MGn1bcajWFMLaSlo79hnnnLNSJWmM6NDBflthYUWz8rKzpZevUycR\nf3n8cWjZHL6dArlnIe9c4Vdp35fy3wqyATgf1IcrZOjCv2nQGzY/J0Itrd6x/LjIyKLV+65d0KNH\nUd1VSeSehbR1EHcFL3zdmeKz9swE1oXI7pA8FVqMKlXxzcdHFuSOzuxt3y6V4P37w3XXOfZc/+fI\nDpjyLLReC7npcGtDiLoLbhsDXp7HGACHmsLHv0OXKXD7UKsObdZMMnOzChODAQFwzTWSaO7cWcSc\nbSkicWUq5RyH3Ayo1tFoV4wlri8smw3Hf4M6dxjtjfuQthaiU+BBHVTO3Ymh46DODDj3K3CT0d54\nsJAKdvs2iNR4eO4T+HM5NHfC6qdOHenX0xt/f3j1efDeAL82guHHIfAo/Nig9GM0L/AJBt9g8AuR\n10rVoUqUfO8bLP/drxpHj9WlDEsVm8oRUOs/cOAruOpN8PK23kZMTPnjM04sBVXgEWdxZ06elFTK\noEHymQQRajm2AI7Oh/r3lXpobCxs2+Y410wmcSskRHqyHM65PbDrPfhsKnyeD9M6QbfR8MA1blQr\n6CT6vgKZn0GNLVYf2q+f/G0TEyW4a9fOyRlbAwjLXwfDgW7jjHbFWGreKGOkZj8NCzzBnsXMHwNe\n/lDvCusT9qsqa5m930LLMeDtCSPcAc9fSQ/aPQiRn8C++Y4P9lavFkXMsrI7tnB+nyyqAqZLJq5a\nN4i6GnxDioI23+BLf/YLAe8Aixdd+ceX6euzu9GoL6xcCCeWQK0ulh83bx4MGQJ//lm+EmrqIvAJ\nhDAdsr4ejGHTJhg+XD7jNxcG7bW6QGBD2Dul3GDvhx/+1959x0dd5H8cf01C7016bwJSVDoigiAi\nSlOkqDSlKKh4Hiqch+WQE2xnL6go4gmKgiCiElBOQZEmRYr0akJvoabM749ZfgbYkN1kN7vZvJ+P\nxz6S/Zb5TsKw+X6+M/MZN7UzGDfrEyfCokVusewSJQJfPuCGR/0xBwYNhKvjoHlu6NMXBt8GjbPR\nGnr+Klgcrrsddn0Bjd+A6Dw+n3r77SFeOiMEShz9EQrWhEK1Ql2V0IrKCcVrQ9xqSIjPvvNe/XHq\nODw8E1pUhHsKhbo2me/A1dD/K8g3AToNDXVtxAcK9gLhipYwvgnYn4J/rREjXG71NWsC82T7wGJY\n/zzsmuE+9Kv0gVp/h8K1M162nK9cJ8hZBLZO8i/YK1zYpcGrVQt27XI9u6mJi4GSrSN7cddI17at\nG4uZMrA3UVBjCKwc6eZ9pvL/s1YtSEqCzZtdooxA2rcPHn3UDd3s1y+wZQNurtmv78GJybB/MeyN\nhqK3QJf3XM+4pK1ENxj7EZweC33GhLo24WvfDorevQIe7QCd1EPMc6/AvFZuiaDKvUNdm/C3PwZG\nWmg+MtQ1CY02g6HFM3BqEaBgLytQsBcoFXvCor/DrhVQIYjLC8yc6W74MxLo2WT3ob7+Bdi/0K0v\ndMUoqPkA5C0duLrK+aLzQKVebt5ewjHXO+qLtm1dUp6YmEsHeid2wPFNUGNYYOoroZEjh/ce3KoD\nYPVo2PwONHzZ66nnMnKuXx/4YG/ECIiPd9N5AzqCMvGEy1Q78DHYcALeqQpN3oC1fdXL4K/KN8Gv\nUVB2emQHe8lJkHwa9u6B6CTIl8Ml71m2Ai4rCKUKwtkTMOsHqFESqpeA+GPwwTxoXB6KHMLcALTu\nFeqfJDxcdo1LUrPqQwV7vtg+GWqXhtYDQ12T0ChWFp7qBvvnu/+L6ZmWIplKwV6gFGjnHnDcNwpe\nycB6dGkpUSL946eSTrs5YxtehGN/QP7K0PAVqHq3bqoyS9V+bs20ndOg2j1+nFcVhgy59DFx89xX\nzdfL+k6dctFVq1bQ0zNsM09JqNDd9Qw3+DfkyHfRaedixEBn5Jw3DyZPhscf/yugzLCda2HcUGix\nBqIOQ6facMfNcMtYyKme6XTJmRtmDYUdE1zykUhYKDw5AVaOgu0fwz37oJWBnp6M0X2BW4AeQDLQ\nB7gVuA1IwKXF7wF0AU4BrwG9gS75ONr3Cgq36ZPZP014MlGwqAa8PBd2boEy1UJdo/C1ZyO8OAuG\n3JO9k0NV6gkrPodlU6HJnaGuTcbMfgOipsB1n0D+iqGuTVBk45YaYBXqw4BKUGN7cMo/cAAGD3Y5\nz+vW9e/cMwdduvKNr8HpfVCsIVwz1S1AnJ0/rEKheFM3T2TrJP+CPV/ExkDeslBIQ3CzvDx53OS4\nsmXP317jXtgxxWX/rTbgotPy54eKFQOTkXPvXpehcepUWLjQPW94/PGMl0tCPKx+Ar5+Hd5OgIrN\n4J4XoEQLJV0JhJr9YNvrsONzqDEo1LXJmM3LYMyt0H6XS4TRbSfUr+QW+YvOC6OXQZ2q0OgK9/6j\n36F6VaheHaJyw6+xUKY8lCjt3t+RA3Lng6gofluwgNYmq64ZEQRdh8DOH+DP2VBmeKhrE75mvwSz\nk+ERP6ZiRKLibWEkcOMz8GUWDfaObYLPhsCgH2BwEWi0Q8Ge+OBvf4MVD7kMcoVqBrbsdevczZ8/\nN0Px21zSlS0TIekklLkJ6jzi5nTppio0jIGq/WHVPyB+KxSoGphybbJL/FK2o/5tI4ExsGLFxQuY\nXXYtFK7jHt54CfbA9bylt2fv0CGYMcMFeN9/7zI01q0LzzzjllrImzd95f6/uHmweCCc3AntB0C3\nO6Be2wwWKucp1hA+Kwovj4QlWTjYi/sexnaBT+Oh/3/g2ofg2guOefKC930uyIyYbdf6SYcWPeDQ\naDgyC9clKl5V/g0+vgJadg91TUIrf1F4shUUXOV636NyhrpGvovdArMeg0IzoVAe+Hd3GPYWFApW\n1rHQ02OtQKrYHfYA04OQk7xVK9i927csnAeXwsKe8FV1N7+nUg/ouAbazIFSbRQMhFqVPoCBrR8F\nrszDK+HMASitIZwR41ygl5Dw1zZjoPq9cGgpHFru9bTatV3PXnKyb5c5fhw+/hhuucUtoj1wIGzf\n7pbfXLPGvR5//OJORr+cPQK/DoT5N8C/9sPOh6DZ+wr0gsEYqN0Mih+C+J2hro3/khLh+1Hwww3Q\nrTwsiYE2D4W6VpHPGCjf0z3l2fl7qGsTno794daxvdL7g7Zs564RUPToX1NIwl3SWdjwH7i1Noz4\nAsr2gU6bYNS0iA70QMFeYOUrBx8Vgn/9N7DlHj/u0pHnvMSTE5sMe2bDvNbwXROI/Q5qPwKdt0Oz\nD6CIn0M/JXjylXdrG237yP27BUJcjPtaWoucRpQRI6D5BYs+V+njljzZ9I7XU2rVgpMn3bOh1Jw6\n5YZodu8OJUtCnz6wejU89BAsXw4bN8KYMf6PGPdq91fw9RWw9QOo/BBUagcl9W8IzgAAACAASURB\nVHkUVP941c1f2/lpqGvin7NHoHsN6DEOinaBm5ZCfX2mZZrEZvBv4J0Lu0wFgFGD3bzPikrsA7is\n4pvzwxvPhLoml5acDG8/AlMuhxUPw31N4bvpcN3EbJOUUMM4A23sfbBr/CXTo/utZ0+IjoavvvK+\n//QB9xT08ErIVwGufgmqDYScBQNzfQm8Kv3glz4uG2rJAKzNGDcPitTLNh9c2cZVV0Hu3K5379zD\nnlxFXMa87f+Fq553612mkDIjZ8UU0w/OnoW5c90QzZkzXWbNUqVg0CDo1QuaNbt41GiGnD4Ay4fD\njk9c22w1E4o3gmtwD68keApWd/ODF73vhu5nBYdXwk/doeEuqNMN2n8e4AYpaWp8MzxVBa7cFeqa\nhB+bDEdXQYGykL9cqGsTHqJzw5Iybv3np05CzouThoXcgSUweygMXQ53loTx30LZG0Ndq0ynT9JA\nu2E4FDWw87PAldmlC3Tu7H2ftbB4ABxdB80mQectUOtvCvTCXYVukKOAS9SSUYmnYN9PUEpPwCPO\nnXfC2LEX9+pXv9fNw93+8UWn1PKsEb1hAyQmuhU77rnHBXadOsE330Dv3jB/PuzZA6++Ci1aBPC+\n2lrY8Rl8XQd2TYN6T8ONy+DrdRAb647RUPLgW305DP4Dfp4R6pqk7fmB8GBjSDoFQ36EsdMV6IXK\n7UPgxFI35z+rObQHfn4FFtwC00pAnQLwz6awajRsmQRzJsDeLel62FTk7GroeBTefSEIFc/Cxo6B\nFyzsjQl1Tc63fhGMbgFzm0KhXfDfR+D9Hdky0AMFe4GXtwzsawBPvRa4MocMcY/fvfnjZZc966oX\noGrfrDVJNjvLkR8q3u6WYEg8mbGy9i+E5DNaciFSWQvLlp0/Ca94IyjWyCVqueDG5bLLoFgxeP11\nN8+ufXuYNs09L5ozB+LiYMIEuP56N2AgoE7Fwk+3wqKebmmXDiug3hNw8Ajcey+8oBulTNPzUehn\nIOF/oa5J6pJOw+JBMON9WF8Q2i+Dy1qEulbZW4Ue8APw1qhQ18Q3CSfc39Efb4Ua5WHEQ3BkDRTr\nAAXzwuntsO5Z+K4/3DwEHq0OnxeFaVfBDZVh8t2w7WOIXeQSd6SizJ+zIEdBKN8ls36yrKHebW6+\n246poa6Jk3DMLdXyUCt48Reo8HfotBl6Pwe58oS6diGTZrBnjJlojNlnjPk9xbanjDF7jDErPa+O\nqZzbwRjzhzFmszFmZCArHtYOXw5zD8IfGfwje25iTcoEDSkdXAYrH4PyXaHm/Rm7lmS+Kv0g8Tjs\nyuCT97gYiMoVmOGgEn6mT4fGjV023pRq3AtH18L+87cb46b57dkDbdq40/ftg0mT4KabLj31N92s\ndQujz64Dsd+64aXtf/5rrnDJkrBqFYweHYSLi1cVr4B+HeDwjMDNDQ6ktT/B1Caw9T14cQT8/Cfk\nLxPqWknBKrCoAEz/JtQ1SV3SGTcX+O56cFUhWNgDDvwCf+8AT7wLXbZB24/h1/3wwl7ocRJuXQmT\nnoS+o6HynRCfF1bvgTUfuikV77WEstVhZAH4ril80RWGtoFFr8KOHyg57AeYW83r+qbZWlROONAc\nhn0Gx/aHrh5nT8PTveGtyrBuHAy/DX5bDNe+oJFu+DZn70PgdeDC1IH/sdam+pjWGBMNvAHcAOwG\nlhpjZllr16WzrlnH8HFQ/TM4Mx+4Lv3lzJjhhnEtWADXXVDO2aPu6XmeMtD0fQ2LyopKXut6P7ZN\ngioZWKcmLsatUZYjf8CqJmGkQwd4/31o0OD87ZV6wYq/u969ki3P2zV9OiQlBWCpBF+c2AFLhrik\nUJddC03fO3/pmSNHoEgRqFEjEyoj5ynXE2b3h2IfQ6u+oa7NX7bNguu6QpVomDETyqcyTUFC48N/\nwqaRbnpI4Tqhro2TnAjfvwfvvAq3/QnJR90csRo14brXoEwbiEplqEJ0LijdAPqm+AxtDAzABY7x\n22DjYoifAdcUghyxsGIRvH0Ayi+A0mC6A92HBP/nzIpKtIF9X8GSydDu4cy9trXw5zew4CEYvwm6\nVoTXY9wSNPL/0uzZs9b+CBxKR9lNgM3W2q3W2rPAVCB79H8Xqwxl27h5exlJRNCzp8uo0OqCHhtr\n3c3ViR1wzRTIXSxD1ZUQMVFQpa9LrnLyEqkTL+X0PpfYQEM4I1f+/HD33VCo0Pnbc+R37WfX53D6\n/CequXJlQqBnk2HjG/B1Xde72OgNaLfg/EDv4EEX5L38cpArI16V7OAeub75fKhr4iQlwuon4Zeu\nMKwSvPedAr1wVK+/mzO5fUpo65GUCDNfg5h7YEZZ+Po++Ho9JLaC1nPgrSMwYz2Ua5d6oJeW6NxQ\nuBY07g/jZ0KnydB2HozdD8cPw72rof2XrO09Gm5UsOdV1wfgtVKQe1HaxwbSwmnQvxr872YoBMx9\nCz7epkDPi4zM2XvAGLPaM8yzqJf95YCUKZ12e7ZlDydbwEN/wMLP019GdDTccMPFvXZb3nMptes/\no/kNWV2VvoB1cwbSI+5791Xr60W2hASYMsVlPUup+hBIPuuWNchMxza5ZV6W3e96lW/+HWoOdQ8w\nUsqVC/r2hbZaTy8kipSCt2+CrrvdGlOhFLsZmpeGyf9yn3uj10KD60NbJ/EubylYcTn0fdH3BTsD\nJTkZdv/kRi28Vg66PghTJ7s1gh/4DPYfhrtmQdmbgp+jIH8RKFYPyndhf97rNYIqNdE5oFJP2DUb\nTqWnb8hPCcfduq1v9IDp26H8v6Dj79DyXiV2SoWxPvQ8GWMqA7OttXU970sBBwALjAHKWGvvvuCc\n7kAHa+1Az/s+QFNrrdfJZcaYwcBggFKlSjWcOjVMJnumU96DO2jyaH/2DryBDc3/4ff5lSdO5EyJ\nEsRekIUzf8JWrt5/H0dz12d1sfEX31yFsfj4eAoUKBDqaoSdKw88SM7koyy97EO//5hcfuQ5Spxa\nyKLSM8AEOttGaKm9/MUkJtK8e3cOtGzJxhEjztt35YHh5E46wK8lJwf988DYJMqf+JzKxyaSbHKx\npfBQ4vJ2CIubILUX74qd/pX6h0aypugYDuZtmfYJQVDw7Hqu2Pskuf91gLhON/JHp0dD3mbUXi6t\n5lfPUXb+N6x84kWOFLs66NfLn7CVkvHzqPDANKLqJ5J8V04O5WnCyZXl2NW0Jwn5QzuCSe3l0orv\n/ol6Dz3Bnv6d2XTL34J2nRwnDlN/+ygKFt3E7pzd2J3rNs4UDs+5voFqM23atFlurW2UkTLSFez5\nss8Y0xx4ylp7o+f9KABr7bNpXa9Ro0Z22bJladc+3H3f3o0F77TRvz9syckuhV7NmvDmm39tTzwB\n3zaGs4fhppXu6VsWsmDBAlq3bh3qaoSfze/BkkHQ/lco0cT386yFmZWgeBO4NgM9yGFK7eUCW7ZA\nlSoXP7ncPgV+vgPafOcWuQ2WI7/D4rvh0FKXFKrxmy77sDdJSTB0KAwbBvXrB69OKai9pCI5Ee4v\nCicqwKRMnjKfnAyvDoLiH0GhctDiM7jMj8+4IFJ7ScPZwzC9FNR8AK5+MXjXSUqE93pCwenugeXc\nytC4Awx8xq0pGibUXtKQlAjtikDnuvC3xcG5RuIpaF8ZVu2DhR9C7X7BuU6ABKrNGGMyHOyl6zGw\nMSblX/huwO9eDlsK1DDGVDHG5AJ6AbPSc70sq2IPOLoZdvo5jjkqCubNg1deOX/7sgfh2AZo8XGW\nC/TkEireDtF5XKIWfxzfCCd3aQhndlGtmvchKhVuhdyXuUQtwXB6v0tl/e3VcGI7XPMpXDs99UAP\nYNMmlyVmXeTn4wp7UTngYBX4dQOcOZJ51008ARNvhr9NhOU13TIcYRLoiQ9yFYUyN8H6Ke5GPhiS\nE+CRFnDvdEjsAd1i4YPNMPT1sAr0xAfROeCl+6DMCjgThKGcSafhx67QYR88NSzsA71w48vSC1OA\nX4DLjTG7jTH3AM8ZY9YYY1YDbYC/eY4ta4yZA2CtTQTuB74D1gOfWWvXBunnCE8lO8Jw4KlHfD8n\nKQlOetZdS5kjffsnsHUiXPEPKK35LxElV2Eo3w12THGZwXwV61nEVMlZso8JE2Dw4PO3ReeGanfD\nnq/Sn+jHmxM73QOmmZVg3Xio2BNuXgeVeqQ9UqFWLdi82SWZktB7+U14wsKeLzPnekc3wnfNIN93\nMKEfvLhKicSyon1XwYBYmPVG4MtOOg0/dYf6S+Hf3eGuKZDnssBfRzJPpV4QnwAL3w5suSePwbMt\nIW4u3DYRHng9sOVnA75k4+xtrS1jrc1prS1vrX3fWtvHWlvPWlvfWtvZWhvrOfZPa23HFOfOsdbW\ntNZWs9aODeYPEpYKlYXbakClbb5n5fz2WyhfHlav/mvbsU0u++ZlLaHeU0GpqoRYlX5u2Mye2b6f\ns3ceFKjqXpI9/PmnC6IuXHuz+mCXHXPzexm/xtENsHgAzKrmegsr9XJBXovJkKdE2ucvXeo+7woX\nDvm8LPEofQ0UqAZb05kIyh+/xUD9K2DtTmjzLQz60D31l6yn/VBolwOSAjws79CfcOflbgmOFq/D\nqGlKrBEJil4NY3LCYwEc9pucACNawOjlUGg0VBsQuLKzEf3vCraRo6DmXji03Lfjy5eH226D2rXd\n+6QzsKiXWzS7xSduSI5EntLtIG9Z34dyJifC3h/ceZJ9PPkkfP/9xSujF6gKZW6ELe+6tpEeB5fB\nT7fB13Vgx6dQcxh03grNJrrU5L5YtgyaNIF3301fHSQ4jIGdjaH7fNiyInjXid8Kv/aFnMC1k4I7\nh1SCr3BJePx2MDHupjsQzhyEN9vAFzshx0j3OSORwRgY0Q26HHbLQmVUciIsugOarYUJ98Et/8p4\nmdmUgr1gq9AVjueA2T6uM9WggbtROncz99ujcHgFNPsA8lcIXj0ltKKiofJdbnFQXz4kDy6BhGOa\nr5fdnOspO+NluG+Ne+HUn244p6+sdQ8Nvm8P3zWGuPlwxePQZQc0fNn/z5yrroJ33oE77/TvPAm+\n5v2gFrBxWnDK37Ma5rWBEmdhyVJo3jU415HMVakXbD3oEmJkVPxumHcdVN8Bi96Fu9LM1ydZTZ9/\nQjULu77IWDkJZ+G+q2Hj59DkJRj0ZtrnSKoU7AVbrqLwYVF4ZGra69XExEBs7F/vd30JG1+Fyx/S\nwrPZQdV+YBPd/My0xMUABkppnapsJyYGSpaEDRvO3172ZshXHjb5MF/CJsPumTC3Ocy/Ho6shivH\nQ9ed0GBM+ubOWOvWBh082C0EL+GlYQd4qhEwN/Blb1sBDRvCtH1wfQwUuzLw15DQKHE9PG3g2XEZ\nK+f3/0H9arB6K7T5BpoMDEz9JLwUrgvHq8Fb/0l/GclJ8EEXeG8N7O8JtYK3lEN2oWAvM4y6Hx5O\ncr0xqUlIgN69Yfhw9/7ETvj1bijWEK7M4IesZA2F60CxRrD1w7SPjYtxxyrpQfZTrx7ceqsLrFKK\nygHVBrlJ7Mc3ez83ORG2fQxz6rvMZqf3uSUUumyHOo9CzkLpq1NsrLvZv3DRdwkvle+E7Stgy8+B\nK/PkHlh5O1wVBQPehkxYk00yUZ4C8OyN0DXOpb5PjyNrYXEPiEqChm+4BdIlMhkDqyrAq5tgl7dE\n/WmwybBkMBT4FmY8CCOz9prb4ULBXmbo8CBUzgW7LjF8JmdOWLwYxoxxY+MX9XY3Ztd86rLtSfZQ\npR8cWQWHV6V+TMIxOLBYWTizq9Kl4YMPoEaNi/dVG+jWqto84fztiadg45vwVQ34pQ9goPnHbg3Q\nGve5pT8yYv9+17NXwocELhI6hW+CB4HxjwWmvG2r4MtWkLAfJv8P2ikdekS6fQQUPAl/fu3/uSu+\nhJhroXg0LP0N2ivBRsR75Fl4DTgxz7/zkpPhrqtgzkSoOxo6v5L2OeITBXuZIVcRONsCxr936fVq\nqleHyy+H1U/CgZ+hyQQoWC3z6imhV7k3ROWErZdI1LL3f2CTlJwlu9uxA3buPH9bvrJQvotbpiXp\njHswsG48zKoCy4ZBntLQaiZ0XAVV7gxcwqf69WHFCu8BqISPMpfDg7WhwXbfM0Sn5kQstGsKY7ZD\n6zlQolkgaijhqGRr2FgEnh/j33k/fwTXdINvDNywEIrWC0r1JMxUaQaVr3SJvnxlLSy4FxashrgW\nUO/p4NUvG1Kwl1kO1oIvjsHizy7et3w5DBoEcXFu7bR149wT+sq9Mr+eElq5i0PZW2DHf1PPfhYX\nA9H5oESLzK2bhI9Tp6BOHRjnZYh3jftcxruFPeDLirByJBSpD21/gPY/u/m/JkAf/QkJLiHL2bNa\nZiGrGDYSiu52owPS6/R+WNAOegLjXoaSLQNWPQlDUdGwoRJ8shpOHvTtnN0zYccguO0yeDxGSwRl\nN8nXw9OLYe1CH45NhhV/h7h3YcowePsn/T0JMAV7meWep+CdXJDHy7y9NWtg1iwwx+GXu9zcrYbq\nvs62qvZzc6liv/O+Py4GSrbS8N7sLG9emDwZHnnk4n2lroeCNV1WztLt4MalcP1cKNU68H9AZ8+G\ne+91y0FI1lChG+zIDe+n88n5n5vg5WZumYUH50CXBwJbPwlPY8bDf4A4H4ZyvjUcZt8KRRvA++uh\nmuZxZjsVO8PvwKIPLn1ccjIMvAYe/w/UGAbXvqY1F4NAv9HMUqgU1OgIO6e5Cagp9e8PO3fA2qGQ\ncNzN08uRLyTVlDBQ5ibIXcL7UM6Tu+HYBi25IC5JS5UqF283UXD9POi0Ca79HIo3Cl4dunVzSVk6\ndAjeNSSwchaEeSXh+RhI9LKEx6WcOQT9msKYrdDgEyitbMDZRvX2ULgy7Jhy6eN+eRYefhW+KQNt\n57vRKpL91LsOpjaGCr9d+rg1T0HcYshXG65+RT16QaJgLzNFtYGxf8KcFKnRT5xwXze/BHHzoNFr\nUOSK0NRPwkN0Lqh0B+yZ5W6uUoqNcV+VnEUAfv7Z+wLm+SsEf77vsWPuazPN1cpyxj4J45Jhrx8J\nFM4egR/aQ894mPwc1OoWvPpJ+DEGDl0DQ76FXesv3m8t/D4Wtv0DXr4WPvndPViQ7Ktqbzj8GxzZ\n4H3/kidg7RgYPQCmrb44w7QEjIK9zHRFD9hjYK1n0eOkJLeI+oN3werRUKk3VL07tHWU8FC1HySf\nhZ0XTHCOmwd5Srm1bEQ+/hj++U83dy4zzZsHFSvCkkssJyPhq3EfKFoMtv/Xt+MP7oYh9eDQKrhp\nOtzqZfiwRL7qnSEBWDH5/O3JyTCgKbz/T6h8FwyaD/mLhKSKEkZKdYYngNHDLt73WCfoMgYK3w5N\n34XoACULE68U7GWmoqXh025QfaVbNPLsWbijOxT6FvJXgSZvqwtbnKJXuYAu5VBO63kSX7qd2ok4\nTz8NW7e6pVsyU8WKbhhp/fqZe10JjOhcEN8aHv4MDsVe+tiE4/Dv1jB5NxT6N5S7JTNqKOGo5e3w\nWm3Iv+ivbclJ8NPd8PNS2NcAmk9yGaVFilaDWqUh1x/nb9/wH8gzGxpXhRsmuwRAElQK9jJbpZ5w\nOg52fw958kDb9VD3GLT8NP0LGkvkMcb17h38FY55PiiPrHGJWzRfT8657DLInz/zr1uzJkyc6D7D\nJGsq0Q42JcGi91M/JvEELLgZGm2HuS/DLerRy9aMcSOQYn+Eg5vg1HFY0AP2TIKPHoPJKwKX6Vci\nw0v/hIZ74IhngfX5T8OKh6HdbfDlBsilRHOZQf8rM1vZjvBEFNw1CCY/CLtnwZXPQ7GGoa6ZhJvK\nd7o/nOd69+I88/W0vp6ktGwZtGvnlm4Jtp074YEH4NChtI+V8NZxCLxfCQqkkhr92AHoWM2lTm/x\nX7h+eObWT8JToZvgfmDs/dC6KvxzuruHaTZOWRTlYhW6Q7KBRW/DW0PhxqdgXwto8Yl6gDOR/mdm\ntpwFoHVtWLcTBrwOedvD5Q+GulYSjvKWgdI3wvbJbqhMbIxbliNfuVDXTMJJvnwuCNuxI/jX+uEH\nmDQJjh8P/rUkuKKioOqdEDsXju85f1/iKfiiM/y6F3IOcyNSRAAqNYIbS0G+uVD9IHTqB3VGhLpW\nEq7yloJ3SsCgtyD3W3BHdRj8jRtKLplGwV4oPPE0PG/hiRJw8yeafyWpq9rPLbcQ+y3s/1FDOOVi\nderAH39A06bBv1a/fi6orFQp+NeS4CvaGR6xMC7FWnmJp+CnWyH3YvjxLRj2WujqJ+Fp7NNQNxc8\n+ymM+jDUtZFwN7gP9EqGiu3ggzWQT1OWMpvS34RC2Y5Qszd0Gq41aOTSyneBnIVh+XBIOq0hnOKd\nMS4j3pkzbsH1QDt9GrZsgSuugKJFA1++hEbFplC3GOTwzKc5eQxuqAG198Go96DaPaGtn4Sn6oOh\nyl2QIwTzhSXruXMcXNvI3c9Ea553KKhnLxRy5IVrPoESmfAkXrK26DxuCFX8FjA5oOR1oa6RhKNT\np6BGDRg3Ljjlz5gBV17pAj6JLC//Ay7fBEfXwaI74eQ+t86nAj1JjTEK9MR3UTmhcm/IkS/UNcm2\nFOyJhLsq/dzXEs21SK14lzcv9OoFjRoFrsyVK13yF4BbboHhw6FakBdql8xXqRecBiZcA/tnwyev\nwGgf198TEZGwp2GcIuGuRHOX0apCt1DXRMLZ2LGBKys5Gbp2hdq14ZtvoGBBeOGFwJUv4SNfOXih\nEKw/Amueh9pKGCYiEkkU7ImEO2Pg2mmhroVkBceOwYYN0KSJ/+dOnuwybc6d6zI1TpsG1asHvo4S\nfqbMgFU/QV1lVRQRiTQaxikiEimGDHFDLhMSfDt++XI33w8gOtoFeYcPu/eNGysZS3bR4Hro+2So\nayEiIkGgYE9EJFI89hjMnAk5fBi0sWSJm+M3dap7f8cdrlevuDIEi4iIRAoN4xQRiRRXXpn6Pmvh\nmWdcMDd0qOu5mzgRbr018+onIiIimUo9eyIikWTXLnjySYiPd+937nRfjYGff3ZDN8+9HzAAChcO\nTT1FREQk6NLs2TPGTARuAfZZa+t6tj0PdALOAluAAdbaI17O3Q4cB5KARGttAPOCi4jIRbZtcz14\nLVvCL7/A+PGwZw8UKeKGeObKFeoaioiISCbxZRjnh8DrwEcptsUAo6y1icaY8cAo4LFUzm9jrT2Q\noVqKiIhvWraE3buhTBn3Kl78rwBPgZ6IiEi2kmawZ6390RhT+YJtc1O8XQx0D2y1REQkXaKiXJAH\nULeue4mIiEi2FIg5e3cD36SyzwLzjDHLjTGDA3AtERERERER8YGx1qZ9kOvZm31uzl6K7Y8DjYBb\nrZeCjDHlrLV7jDElcUM/H7DW/pjKNQYDgwFKlSrVcOq5dOASMeLj4ylQoECoqyFZhNqL+EPtRfyh\n9iL+UHsRfwWqzbRp02Z5RnOepHvpBWNMf1zilrbeAj0Aa+0ez9d9xpgZQBPAa7BnrZ0ATABo1KiR\nbd26dXqrJmFqwYIF6N9VfKX2Iv5QexF/qL2IP9RexF/h1GbSNYzTGNMBeBTobK09mcox+Y0xBc99\nD7QHfk9vRUVERERERMR3aQZ7xpgpwC/A5caY3caYe3DZOQsCMcaYlcaYtz3HljXGzPGcWgpYaIxZ\nBSwBvrbWfhuUn0JERERERETO40s2zt5eNr+fyrF/Ah09328FGmSodiIiIiIiIpIugcjGKSIiIiIi\nImFGwZ6IiIiIiEgEUrAnIiIiIiISgRTsiYiIiIiIRCAFeyIiIiIiIhFIwZ6IiIiIiEgEUrAnIiIi\nIiISgRTsiYiIiIiIRCAFeyIiIiIiIhFIwZ6IiIiIiEgEUrAnIiIiIiISgRTsiYiIiIiIRCAFeyIi\nIiIiIhFIwZ6IiIiIiEgEUrAnIiIiIiISgRTsiYiIiIiIRCAFeyIiIiIiIhFIwZ6IiIiIiEgEUrAn\nIiIiIiISgRTsiYiIiIiIRCAFeyIiIiIiIhFIwZ6IiIiIiEgEUrAnIiIiIiISgRTsiYiIiIiIRCAF\neyIiIiIiIhEozWDPGDPRGLPPGPN7im3FjDExxphNnq9FUzm3gzHmD2PMZmPMyEBWXERERERERFLn\nS8/eh0CHC7aNBOZba2sA8z3vz2OMiQbeAG4C6gC9jTF1MlRbERERERER8UmawZ619kfg0AWbuwCT\nPN9PArp6ObUJsNlau9VaexaY6jlPREREREREgiy9c/ZKWWtjPd/HAaW8HFMO2JXi/W7PNhERERER\nEQmyHBktwFprjTE2o+UYYwYDgz1v440xf2S0TAk7JYADoa6EZBlqL+IPtRfxh9qL+EPtRfwVqDZT\nKaMFpDfY22uMKWOtjTXGlAH2eTlmD1Ahxfvynm1eWWsnABPSWR/JAowxy6y1jUJdD8ka1F7EH2ov\n4g+1F/GH2ov4K5zaTHqHcc4C+nm+7wfM9HLMUqCGMaaKMSYX0MtznoiIiIiIiASZL0svTAF+AS43\nxuw2xtwDjANuMMZsAtp53mOMKWuMmQNgrU0E7ge+A9YDn1lr1wbnxxAREREREZGU0hzGaa3tncqu\ntl6O/RPomOL9HGBOumsnkUbDdMUfai/iD7UX8Yfai/hD7UX8FTZtxlib4dwqIiIiIiIiEmbSO2dP\nREREREREwpiCvWzKGFPBGPODMWadMWatMWa4Z3sxY0yMMWaT52tRz/binuPjjTGvpyinoDFmZYrX\nAWPMy6lcs6ExZo0xZrMx5lVjjLlg/23GGGuM8Zq9yBjzsKe+q40x840xlTzb21xQh9PGmK6B+l2J\nE05txhjT3xizP0UZA1M5P7cx5lPP+b8aYypfsL+QZy7y697Ol/TLou2llTFmhTEm0RjT/YJ93xpj\njhhjZgfqdyR/icD28pzn51hvvPy9k4zJou3F6z2MZ19SivOVzDDAwqm9ePb1SFGXT1I5P9X7F7//\nHllr9cqGL6AMcLXn+4LARqAO8Bww0rN9JDDe831+oCVwL/D6JcpdDrRKcKXSDgAABTVJREFUZd8S\noBlggG+Am1LsKwj8CCwGGqVyfhsgn+f7+4BPvRxTDDh07ji9IrPNAP0vVWaK84cCb3u+73VhmwFe\nAT7xpSy9skV7qQzUBz4Cul+wry3QCZgd6t9tJL4iqb0ALYBFQLTn9QvQOtS/40h6ZdH2kuo9DBAf\n6t9pJL/CrL3UAH4Dinrel0zl/FTvX/z9e6SevWzKWhtrrV3h+f44LmNqOaALMMlz2CSgq+eYE9ba\nhcDp1Mo0xtQESgI/edlXBihkrV1sXUv96FzZHmOA8Zcq31r7g7X2pOftYtzajRfqDnyT4jgJkDBs\nM75IWbfPgbYpnsY2BEoBc/0sU3yQFduLtXa7tXY1kOxl33zguD/lie8irL1YIA+QC8gN5AT2+lO2\nXFoWbS++3MNIEIRZexkEvGGtPey5lre1yuES9y/+/j1SsCd4uoavAn4FSllrYz274nA3w7469+TB\nW9afcsDuFO93e7ZhjLkaqGCt/dqPa92De1LirQ5T/ChH0iHUbcbjNs8Qic+NMRVSKb8csAv+fzmY\no0BxY0wU8CIwwo+6SjplofYiYSCrtxdr7S/AD0Cs5/WdtXa9P2WI77Joe7nwHiaPZ0jwYqNpKEEV\nBu2lJlDTGLPI8+/dIZXyvd6/+FG//6dgL5szxhQAvgAestYeS7nP04D9Sdfqd6Dluel+Cfi7H+fc\nBTQCnr9gexmgHm5tRwmSULcZj6+AytbaekAMfz398tVQYI61dneaR0qGREh7kUwSCe3FGFMdqI3r\nuSkHXG+MuTYd9ZA0ZMX2kso9TCVr7dXAHcDLxphq6aiHpCFM2ksO3FDO1kBv4F1jTJF0lOMzBXvZ\nmDEmJ67R/9daO92zea8naDoXPKXWvXxhWQ2AHNba5Z730SkmsP4L2MP5QxbKe7YVBOoCC4wx23Hj\nm2cZYxoZY8aeKyPFddoBjwOdrbVnLqhGD2CGtTbBj1+D+CFM2gzW2oMp/v3fAxp6yriwzewBKnj2\n5QAKAweB5sD9njb3AtDXGDPOv9+GpCULthcJoQhqL92AxdbaeGttPK4Hp7kv9RbfZcX2kto9jLX2\nXFlbgQW4nicJoHBpL7hevlnW2gRr7Tbc/MEafty/+E3BXjblGff7PrDeWvtSil2zgH6e7/sBM30s\nsjcpnnBYa5OstVd6Xk94usmPGWOaea7dF5hprT1qrS1hra1sra2MG8fe2Vq7zFr7+LkyPHW+CnjH\ns9/bf8jz6iCBFS5txlOXMinK6Ywbf8+FbeaCunUHvrfOndbaip42NwL4yFo70sd6iw+yaHuREImw\n9rITuM4Yk8Nzg3nduTIkMLJie0ntHsYYU9QYk9vzfQngGmCdj/UWH4RTewG+xPXqnfv3rgls9fX+\nxfefOgUbBlly9Mr8Fy7LkAVWAys9r4648cDzgU3APKBYinO24zJdxuOeTNRJsW8rUCuNazYCfge2\nAK8DxssxC0g9G+c83CT3c/WdlWJfZdxTkKhQ/24j9RVObQZ4FlgLrMLNjfFaDi5JwjRgMy4zVlUv\nx/RH2TjVXtxxjT3XPYF7gro2xb6fgP3AKc8xN4b6dxxJr0hqL7gMnO/gbvrXAS+F+vcbaa8s2l68\n3sPgsreu8Zy/Brgn1L/fSHuFWXsxuOlL6zz/3r1SOT/V+xf8/Ht07sIiIiIiIiISQTSMU0RERERE\nJAIp2BMREREREYlACvZEREREREQikII9ERERERGRCKRgT0REREREJAIp2BMREREREYlACvZERERE\nREQikII9ERERERGRCPR/4rKBzqkiRDMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x292b23ada58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2016 Act vs 2017 Pred\n",
    "pred = pd.DataFrame(sub.groupby(['visit_date'])['pred','visitors'].mean().reset_index()).rename(columns={'visitors':'pred_adj'})\n",
    "\n",
    "train = pd.read_csv('C:/Users/Kohei/Documents/Kaggle/Recruit/00_input/air_visit_data.csv')[['visit_date','visitors']]\n",
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "train['visit_date'] = train['visit_date']+timedelta(days=365-1)\n",
    "df = train[train.visit_date>=date(2017,4,23)]\n",
    "df = df[df.visit_date<=date(2017,5,31)]\n",
    "df = pd.DataFrame(df.groupby(['visit_date'])['visitors'].mean().reset_index())\n",
    "\n",
    "df = df.merge(pred, on='visit_date', how='left')\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(df.visit_date, df.visitors, \"blue\", label = \"2016\")\n",
    "plt.plot(df.visit_date, df.pred, \"orange\", label = \"Test-original\")\n",
    "plt.plot(df.visit_date, df.pred_adj, \"red\", label = \"Test-adjusted\", linestyle='dotted')\n",
    "plt.ylim(10,30)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
